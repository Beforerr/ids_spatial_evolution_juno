[
  {
    "objectID": "notebooks/30_data.html",
    "href": "notebooks/30_data.html",
    "title": "Different time windows",
    "section": "",
    "text": "Code\n%load_ext autoreload\n%autoreload 2\n\n\n::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=2}\n\nCode\nfrom utils.config import JunoConfig, WindConfig\n\n:::\n\n\nCode\nwind_conf = WindConfig()\n\n\n\n\nCode\nconfs = [wind_conf]\nfor conf in confs:\n    conf.load()\n    if conf.events.is_empty():\n        print('No events found')\n        conf.get_and_process_data().export()\n\n\n2024-05-15 18:08:15.819 | INFO     | discontinuitypy.config:load:69 - Loading data from data/events.Wind.fit.ts_0.09s_tau_60s.arrow\n\n\n\n\nCode\ntaus = range(60, 10, -10)\nfor tau in taus:\n    JunoConfig(tau=tau).get_and_process_data().export()\n\n\n\n\nCode\nJunoConfig().get_and_process_data().export()\n\n\n\n\nCode\nJunoConfig(ts=0.125, method='derivative').get_and_process_data().export()\n\n\n\n\nCode\nJunoConfig(ts=0.125, method='derivative').path\n\n\n\n\nCode\nJunoConfig(ts=0.125).get_and_process_data().export()\nJunoConfig(method='derivative').get_and_process_data().export()\nJunoConfig(ts=0.125, method='derivative').get_and_process_data().export()\n\n\n\n\nCode\nWindConfig().get_and_process_data()",
    "crumbs": [
      "Home",
      "Notebooks",
      "Different time windows"
    ]
  },
  {
    "objectID": "notebooks/10_examples.html",
    "href": "notebooks/10_examples.html",
    "title": "Discontinuities examples",
    "section": "",
    "text": "Large derivative current density but small when using fitting\n\n\nCode\nj_config = JunoConfig(ts=0.125)\nj_fit_config = JunoConfig(ts=0.125).load()\nj_der_config = JunoConfig(ts=0.125, method='derivative').load()\n\nprint(\n    len(j_fit_config.events),\n    len(j_der_config.events)\n)\n\nj_config.events = j_fit_config.events.join(j_der_config.events, on='time').with_row_index()\nprint(len(j_config.events))\n\nj_config.events = j_config.events.filter(\n    pl.col(\"radial_distance\") &gt; 5,\n    pl.col(\"j0_k_right\") &gt; 3\n)\nprint(len(j_config.events))\n\n# j_fit_config.events.join(j_der_config.events, on='time')\n\n\n72656 94095\n156532\n1168\n\n\n\nCode\nindices = [153101, 156356, 156046]\noffset = timedelta(seconds=0)\nfigs = j_config.plot_candidates(\n    indices=indices,\n    offset=offset,\n    add_plasma_params=True,\n    plot_current_density=True,\n    plot_fit_data=True,\n)\n\n\n04-Mar-24 15:57:37: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 15:57:37: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 15:57:38: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\noffset = timedelta(seconds=0)\nfigs = j_config.plot_candidates(\n    num=3,\n    add_plasma_params=True,\n    offset=offset,\n    plot_fit_data=True,\n    plot_current_density=True,\n    start_col = \"tstart\",\n    end_col = \"tstop\"\n)\n\n\n2024-03-04 15:57:07.447 | INFO     | discontinuitypy.datasets:plot_candidates:245 - Candidates indices: [156049 156046 154208]\n04-Mar-24 15:57:07: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 15:57:08: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 15:57:08: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nj_config = JunoConfig(ts=0.125).load()\nj_config.events = j_config.events.with_row_index().filter(\n    pl.col(\"radial_distance\") &gt; 5,\n    # pl.col(\"j0_k\") &gt; 2\n)\n\n\n\nCode\nindices = [72643, 72280, 69537]\noffset = timedelta(seconds=2)\nfigs = j_config.plot_candidates(\n    indices=indices,\n    offset=offset,\n    add_plasma_params=True,\n    plot_current_density=True,\n    plot_fit_data=True,\n)\n\n\n04-Mar-24 12:01:19: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n\n04-Mar-24 12:01:24: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 12:01:25: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 12:01:25: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\noffset = timedelta(seconds=2)\nfigs = j_config.plot_candidates(\n    num=3,\n    add_plasma_params=True,\n    offset=offset,\n    plot_fit_data=True,\n    plot_current_density=True,\n)\n\n\n2024-03-04 14:41:02.337 | INFO     | discontinuitypy.datasets:plot_candidates:243 - Candidates indices: [71263 71875 71222]\n04-Mar-24 14:41:02: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 14:41:03: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 14:41:03: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nj_config = JunoConfig(ts=0.125, method='derivative').load()\nj_config.events = j_config.events.with_row_index().filter(\n    pl.col(\"radial_distance\") &gt; 5,\n    pl.col(\"j0_k\") &gt; 4\n)\n\n\n02-Mar-24 16:24:50: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n\n\n\n\n\nCode\noffset  = timedelta(seconds=2)\n\n\n\nCode\nindices=[91384, 93825, 93618, 92389]\nfigs = j_config.plot_candidates(indices=indices, add_plasma_params=True, offset=offset, plot_current_density=True)\n\n\n02-Mar-24 16:52:58: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:52:58: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:52:58: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:52:59: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfigs = j_config.plot_candidates(num=3, add_plasma_params=True, offset=offset, plot_current_density=True)\n\n\n2024-03-02 16:53:51.076 | INFO     | discontinuitypy.datasets:plot_candidates:243 - Candidates indices: [90260 92835 92303]\n02-Mar-24 16:53:51: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:53:51: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:53:52: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\noffset  = timedelta(seconds=30)\nfigs = j_config.plot_candidates(num=15, add_plasma_params=True, offset=offset)\n\n\n2024-03-02 16:53:24.688 | INFO     | discontinuitypy.datasets:plot_candidates:243 - Candidates indices: [92436 90663 92303 90640 92616 93261 93834 94014 94032 93294 92471 92343\n 92972 93732 92478]\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[35], line 4\n      1 #| layout-ncol: 3\n      2 #| column: screen\n      3 offset  = timedelta(seconds=30)\n----&gt; 4 figs = j_config.plot_candidates(num=15, add_plasma_params=True, offset=offset)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:245, in IDsDataset.plot_candidates(self, indices, num, random, predicate, **kwargs)\n    242         indices = indices.head(num).to_numpy()\n    243     logger.info(f\"Candidates indices: {indices}\")\n--&gt; 245 return [self.plot_candidate(index=i, **kwargs) for i in indices]\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:245, in &lt;listcomp&gt;(.0)\n    242         indices = indices.head(num).to_numpy()\n    243     logger.info(f\"Candidates indices: {indices}\")\n--&gt; 245 return [self.plot_candidate(index=i, **kwargs) for i in indices]\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:224, in IDsDataset.plot_candidate(self, event, index, **kwargs)\n    222 if event is None:\n    223     event = self.get_event(index)\n--&gt; 224 data = self.get_event_data(event, **kwargs)\n    226 return _plot_candidate(event, data, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:100, in IdsEvents.get_event_data(self, event, offset, **kwargs)\n     97 end = event[\"t.d_end\"] + offset\n     99 _data = self.data.filter(pl.col(\"time\").is_between(start, end))\n--&gt; 100 return df2ts(_data, self.bcols)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/utils/basic.py:292, in df2ts(df, cols, time_col, attrs, name)\n    289         cols = df.columns.tolist()\n    291 if isinstance(df, pl.LazyFrame):\n--&gt; 292     df = df.collect()\n    294 # Prepare data\n    295 data = df[cols].to_numpy()\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/polars/lazyframe/frame.py:1940, in LazyFrame.collect(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, no_optimization, streaming, background, _eager)\n   1937 if background:\n   1938     return InProcessQuery(ldf.collect_concurrently())\n-&gt; 1940 return wrap_df(ldf.collect())\n\nKeyboardInterrupt:",
    "crumbs": [
      "Discontinuities examples"
    ]
  },
  {
    "objectID": "notebooks/10_examples.html#strong-current-density-in-5au-from-derivative-method",
    "href": "notebooks/10_examples.html#strong-current-density-in-5au-from-derivative-method",
    "title": "Discontinuities examples",
    "section": "",
    "text": "Large derivative current density but small when using fitting\n\n\nCode\nj_config = JunoConfig(ts=0.125)\nj_fit_config = JunoConfig(ts=0.125).load()\nj_der_config = JunoConfig(ts=0.125, method='derivative').load()\n\nprint(\n    len(j_fit_config.events),\n    len(j_der_config.events)\n)\n\nj_config.events = j_fit_config.events.join(j_der_config.events, on='time').with_row_index()\nprint(len(j_config.events))\n\nj_config.events = j_config.events.filter(\n    pl.col(\"radial_distance\") &gt; 5,\n    pl.col(\"j0_k_right\") &gt; 3\n)\nprint(len(j_config.events))\n\n# j_fit_config.events.join(j_der_config.events, on='time')\n\n\n72656 94095\n156532\n1168\n\n\n\nCode\nindices = [153101, 156356, 156046]\noffset = timedelta(seconds=0)\nfigs = j_config.plot_candidates(\n    indices=indices,\n    offset=offset,\n    add_plasma_params=True,\n    plot_current_density=True,\n    plot_fit_data=True,\n)\n\n\n04-Mar-24 15:57:37: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 15:57:37: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 15:57:38: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\noffset = timedelta(seconds=0)\nfigs = j_config.plot_candidates(\n    num=3,\n    add_plasma_params=True,\n    offset=offset,\n    plot_fit_data=True,\n    plot_current_density=True,\n    start_col = \"tstart\",\n    end_col = \"tstop\"\n)\n\n\n2024-03-04 15:57:07.447 | INFO     | discontinuitypy.datasets:plot_candidates:245 - Candidates indices: [156049 156046 154208]\n04-Mar-24 15:57:07: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 15:57:08: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 15:57:08: deriv_data was applied to: fgm_rot_tot_0-der",
    "crumbs": [
      "Discontinuities examples"
    ]
  },
  {
    "objectID": "notebooks/10_examples.html#fitting",
    "href": "notebooks/10_examples.html#fitting",
    "title": "Discontinuities examples",
    "section": "",
    "text": "Code\nj_config = JunoConfig(ts=0.125).load()\nj_config.events = j_config.events.with_row_index().filter(\n    pl.col(\"radial_distance\") &gt; 5,\n    # pl.col(\"j0_k\") &gt; 2\n)\n\n\n\nCode\nindices = [72643, 72280, 69537]\noffset = timedelta(seconds=2)\nfigs = j_config.plot_candidates(\n    indices=indices,\n    offset=offset,\n    add_plasma_params=True,\n    plot_current_density=True,\n    plot_fit_data=True,\n)\n\n\n04-Mar-24 12:01:19: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n\n04-Mar-24 12:01:24: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 12:01:25: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 12:01:25: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\noffset = timedelta(seconds=2)\nfigs = j_config.plot_candidates(\n    num=3,\n    add_plasma_params=True,\n    offset=offset,\n    plot_fit_data=True,\n    plot_current_density=True,\n)\n\n\n2024-03-04 14:41:02.337 | INFO     | discontinuitypy.datasets:plot_candidates:243 - Candidates indices: [71263 71875 71222]\n04-Mar-24 14:41:02: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 14:41:03: deriv_data was applied to: fgm_rot_tot_0-der\n04-Mar-24 14:41:03: deriv_data was applied to: fgm_rot_tot_0-der",
    "crumbs": [
      "Discontinuities examples"
    ]
  },
  {
    "objectID": "notebooks/10_examples.html#derivative",
    "href": "notebooks/10_examples.html#derivative",
    "title": "Discontinuities examples",
    "section": "",
    "text": "Code\nj_config = JunoConfig(ts=0.125, method='derivative').load()\nj_config.events = j_config.events.with_row_index().filter(\n    pl.col(\"radial_distance\") &gt; 5,\n    pl.col(\"j0_k\") &gt; 4\n)\n\n\n02-Mar-24 16:24:50: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.6/migration/\n\n\n\n\n\nCode\noffset  = timedelta(seconds=2)\n\n\n\nCode\nindices=[91384, 93825, 93618, 92389]\nfigs = j_config.plot_candidates(indices=indices, add_plasma_params=True, offset=offset, plot_current_density=True)\n\n\n02-Mar-24 16:52:58: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:52:58: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:52:58: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:52:59: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfigs = j_config.plot_candidates(num=3, add_plasma_params=True, offset=offset, plot_current_density=True)\n\n\n2024-03-02 16:53:51.076 | INFO     | discontinuitypy.datasets:plot_candidates:243 - Candidates indices: [90260 92835 92303]\n02-Mar-24 16:53:51: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:53:51: deriv_data was applied to: fgm_rot_tot_0-der\n02-Mar-24 16:53:52: deriv_data was applied to: fgm_rot_tot_0-der\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\noffset  = timedelta(seconds=30)\nfigs = j_config.plot_candidates(num=15, add_plasma_params=True, offset=offset)\n\n\n2024-03-02 16:53:24.688 | INFO     | discontinuitypy.datasets:plot_candidates:243 - Candidates indices: [92436 90663 92303 90640 92616 93261 93834 94014 94032 93294 92471 92343\n 92972 93732 92478]\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[35], line 4\n      1 #| layout-ncol: 3\n      2 #| column: screen\n      3 offset  = timedelta(seconds=30)\n----&gt; 4 figs = j_config.plot_candidates(num=15, add_plasma_params=True, offset=offset)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:245, in IDsDataset.plot_candidates(self, indices, num, random, predicate, **kwargs)\n    242         indices = indices.head(num).to_numpy()\n    243     logger.info(f\"Candidates indices: {indices}\")\n--&gt; 245 return [self.plot_candidate(index=i, **kwargs) for i in indices]\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:245, in &lt;listcomp&gt;(.0)\n    242         indices = indices.head(num).to_numpy()\n    243     logger.info(f\"Candidates indices: {indices}\")\n--&gt; 245 return [self.plot_candidate(index=i, **kwargs) for i in indices]\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:224, in IDsDataset.plot_candidate(self, event, index, **kwargs)\n    222 if event is None:\n    223     event = self.get_event(index)\n--&gt; 224 data = self.get_event_data(event, **kwargs)\n    226 return _plot_candidate(event, data, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:100, in IdsEvents.get_event_data(self, event, offset, **kwargs)\n     97 end = event[\"t.d_end\"] + offset\n     99 _data = self.data.filter(pl.col(\"time\").is_between(start, end))\n--&gt; 100 return df2ts(_data, self.bcols)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/utils/basic.py:292, in df2ts(df, cols, time_col, attrs, name)\n    289         cols = df.columns.tolist()\n    291 if isinstance(df, pl.LazyFrame):\n--&gt; 292     df = df.collect()\n    294 # Prepare data\n    295 data = df[cols].to_numpy()\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/polars/lazyframe/frame.py:1940, in LazyFrame.collect(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, no_optimization, streaming, background, _eager)\n   1937 if background:\n   1938     return InProcessQuery(ldf.collect_concurrently())\n-&gt; 1940 return wrap_df(ldf.collect())\n\nKeyboardInterrupt:",
    "crumbs": [
      "Discontinuities examples"
    ]
  },
  {
    "objectID": "notebooks/12_tau_analysis.html",
    "href": "notebooks/12_tau_analysis.html",
    "title": "tau effect",
    "section": "",
    "text": "Code\ninclude(\"utils/plot.jl\")\ninclude(\"utils/io.jl\")\n\n\nload (generic function with 2 methods)\n\n\n\n\nCode\ndf = reduce(vcat, [\n    load(tau = 20),    \n    load(tau = 60),\n    load(tau = 180),\n])\n\ndf.tau = CategoricalArray(df.tau);\n\n\n\n\nCode\n# datalimits_f = x -&gt; (0, 100)\ndatalimits_f = x -&gt; quantile(x, [0.03, 0.97])\n\nplt = data(df) * mapping(:duration, color=:tau, layout=:r) * (visual(Lines) + visual(Scatter))\nspecs = plt * histogram(; datalimits=datalimits_f, normalization=:density) \naxis = (; yscale=log10)\nfacet = (; linkxaxes=:none, linkyaxes=:none)\ndraw(specs, axis=axis, facet=facet)\neasy_save(\"tau_effect\"; dir=\"../figures/method\")\n\n\n┌ Info: Saved /Users/zijin/projects/ids_spatial_evolution_juno/figures/method/tau_effect.png\n└ @ beforerr /Users/zijin/.julia/dev/beforerr.jl/src/utils/makie.jl:34\n\n\n\n\n\n\n\nCode\nspecs = plt * histogram(; datalimits=datalimits_f)\ndraw(specs, axis=axis, facet=facet)\n\n\n\n\n\nAnd large duration probabilities remain the same from 1-5au\nThis is sufficient to justify using one tau in different radial distances\n\n\nCode\ndatalimits_f = x -&gt; quantile(x, [0.05, 0.95])\nspecs = data(df) * mapping(:duration, color=:r, layout=:tau) * density(;datalimits = datalimits_f)\ndraw(specs, facet = (; linkxaxes = :none, linkyaxes = :none))",
    "crumbs": [
      "Home",
      "Notebooks",
      "tau effect"
    ]
  },
  {
    "objectID": "notebooks/12_tau_analysis.html#effect-on-the-distribution-of-event-duration",
    "href": "notebooks/12_tau_analysis.html#effect-on-the-distribution-of-event-duration",
    "title": "tau effect",
    "section": "",
    "text": "Code\ninclude(\"utils/plot.jl\")\ninclude(\"utils/io.jl\")\n\n\nload (generic function with 2 methods)\n\n\n\n\nCode\ndf = reduce(vcat, [\n    load(tau = 20),    \n    load(tau = 60),\n    load(tau = 180),\n])\n\ndf.tau = CategoricalArray(df.tau);\n\n\n\n\nCode\n# datalimits_f = x -&gt; (0, 100)\ndatalimits_f = x -&gt; quantile(x, [0.03, 0.97])\n\nplt = data(df) * mapping(:duration, color=:tau, layout=:r) * (visual(Lines) + visual(Scatter))\nspecs = plt * histogram(; datalimits=datalimits_f, normalization=:density) \naxis = (; yscale=log10)\nfacet = (; linkxaxes=:none, linkyaxes=:none)\ndraw(specs, axis=axis, facet=facet)\neasy_save(\"tau_effect\"; dir=\"../figures/method\")\n\n\n┌ Info: Saved /Users/zijin/projects/ids_spatial_evolution_juno/figures/method/tau_effect.png\n└ @ beforerr /Users/zijin/.julia/dev/beforerr.jl/src/utils/makie.jl:34\n\n\n\n\n\n\n\nCode\nspecs = plt * histogram(; datalimits=datalimits_f)\ndraw(specs, axis=axis, facet=facet)\n\n\n\n\n\nAnd large duration probabilities remain the same from 1-5au\nThis is sufficient to justify using one tau in different radial distances\n\n\nCode\ndatalimits_f = x -&gt; quantile(x, [0.05, 0.95])\nspecs = data(df) * mapping(:duration, color=:r, layout=:tau) * density(;datalimits = datalimits_f)\ndraw(specs, facet = (; linkxaxes = :none, linkyaxes = :none))",
    "crumbs": [
      "Home",
      "Notebooks",
      "tau effect"
    ]
  },
  {
    "objectID": "notebooks/12_tau_analysis.html#effect-on-the-number-of-observations",
    "href": "notebooks/12_tau_analysis.html#effect-on-the-number-of-observations",
    "title": "tau effect",
    "section": "Effect on the number of observations",
    "text": "Effect on the number of observations\n\n\nCode\n# plot the number of events with respect to tau in different radial distances\nj_events_taus = 60:-10:20 .|&gt; load_tau |&gt; x -&gt; reduce(vcat, x);\nspecs = data(j_events_taus) * mapping(:tau, row=:r) * histogram()\ndraw(specs, facet=(; linkyaxes=:none))\n\n\n\n\n\n\n\nCode\n# get unique events defined by the starting time and ending time\ncols = [:\"t.d_start\", :\"t.d_end\"]\n\nprintln(\"Number of events: \", size(j_events_taus, 1))\nj_events_taus_u = unique(j_events_taus, cols)\nprintln(\"Number of unique events: \", size(j_events_taus_u, 1))\n\n\nNumber of events: 129460\nNumber of unique events: 88679",
    "crumbs": [
      "Home",
      "Notebooks",
      "tau effect"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/index.html",
    "href": "notebooks/missions/juno/index.html",
    "title": "IDs from Juno",
    "section": "",
    "text": "See following notebooks for details:",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/index.html#setup",
    "href": "notebooks/missions/juno/index.html#setup",
    "title": "IDs from Juno",
    "section": "Setup",
    "text": "Setup\n::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’}\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n:::\n\n\nCode\nfrom discontinuitypy.datasets import IDsDataset\nimport polars as pl\nfrom fastcore.utils import walk\n\nfrom loguru import logger\n\nfrom datetime import timedelta\n\n\n\n\nCode\nmission = \"JNO\"\nts = timedelta(seconds=1)\ntau = timedelta(seconds=60)\n\n\ndata_dir = '../../../data'\ndir_path = f'{data_dir}/03_primary/JNO_MAG_ts_{ts.seconds}s'\njuno_state_path = f'{data_dir}/03_primary/JNO_STATE_ts_3600s.parquet'\nvec_cols = ['v_x', 'v_y', 'v_z']\n\nformat = 'arrow'\nfname = f'events.{mission}.ts_{ts.total_seconds():.2f}s_tau_{tau.seconds}s.{format}'\noutput_path = f'{data_dir}/05_reporting/{fname}'\n\n\n\n\nCode\nplasma_data = pl.scan_parquet(juno_state_path).sort('time')\nlogger.info(plasma_data.columns)",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/index.html#standard-process",
    "href": "notebooks/missions/juno/index.html#standard-process",
    "title": "IDs from Juno",
    "section": "Standard Process",
    "text": "Standard Process\n\n\nCode\njuno_events = []\nfor mag_path in walk(dir_path):\n    mag_data = pl.scan_parquet(mag_path).drop('X', 'Y', 'Z').sort('time')\n\n    _juno_events = (\n        IDsDataset(\n            mag_data=mag_data,\n            plasma_data=plasma_data,\n            tau=tau,\n            ts=ts,\n            vec_cols=vec_cols,\n            density_col=\"plasma_density\",\n            speed_col=\"plasma_speed\",\n            temperature_col=\"plasma_temperature\",\n        )\n        .find_events(return_best_fit=False)\n        .update_candidates_with_plasma_data()\n        .events\n    )\n    \n    juno_events.append(_juno_events)\n    \njuno_ids_dataset = IDsDataset(\n    events=pl.concat(juno_events),\n    mag_data= pl.scan_parquet(list(walk(dir_path))).drop('X', 'Y', 'Z').sort('time')\n)\n\njuno_ids_dataset.export(output_path)",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/index.html#check-the-discontinuity-in-juno-cruise-phase",
    "href": "notebooks/missions/juno/index.html#check-the-discontinuity-in-juno-cruise-phase",
    "title": "IDs from Juno",
    "section": "Check the discontinuity in Juno cruise phase",
    "text": "Check the discontinuity in Juno cruise phase\n\nFull time resolution data\n0.03 s - 0.125 s time resolution\n\n\nCode\nfrom space_analysis.missions.juno.fgm import download_data\nfrom discontinuitypy.utils.basic import resample\nfrom toolz import curry\nfrom pipe import select\nfrom fastcore.utils import mkdir\nimport os\n\n\n\n\nCode\ndef preprocess(\n    fp,\n    every = timedelta(seconds = 0.125),\n    dir_path = \"../../../data/02_intermediate/JNO_MAG_8hz\",\n    update = False\n):\n    fname = fp.split('/')[-1]\n    \n    output_path = f\"{dir_path}/{fname}\"\n    \n    if not os.path.exists(output_path) or update:\n        mkdir(dir_path, parents=True, exist_ok = True)\n        df = pl.scan_ipc(fp).sort('time').pipe(resample, every = every)\n        df.collect().write_ipc(output_path)\n    return output_path\n\n@curry\ndef process(fp, ids_dataset: IDsDataset, sparse_num = 10, **kwargs):\n    df = pl.scan_ipc(fp).sort('time').unique('time')\n\n    ids_dataset.data = df\n    \n    return ids_dataset.find_events(return_best_fit=False, sparse_num = sparse_num, **kwargs).update_candidates_with_plasma_data().events\n\n\n\n\nCode\nmag_paths = list(download_data(datatype=\"FULL\") | select(preprocess))\n\n\n\n\nCode\nmag_paths\n\n\n\n\nCode\nts = timedelta(seconds=0.125)\ntau = timedelta(seconds=20)\nmethod = \"derivative\"\n# method = \"fit\"\n\nfname = f'events.{mission}.{method}.ts_{ts.total_seconds():.2f}s_tau_{tau.seconds}s.{format}'\noutput_path = f'{data_dir}/05_reporting/{fname}'\nlogger.info(output_path)\n\n\n\n\nCode\nids_ds = IDsDataset(\n    plasma_data=plasma_data,\n    tau=tau,\n    ts=ts,\n    vec_cols=vec_cols,\n    density_col=\"plasma_density\",\n    speed_col=\"plasma_speed\",\n    temperature_col=\"plasma_temperature\",\n)\n\n\nReasonably splitting the data files may accelerate the processing.\n\n\nCode\nfps = split_list(mag_paths, n=100)\n\nfunc = process(ids_dataset = ids_ds, sparse_num = 10, method = method)\n\nids_ds.data = pl.scan_ipc(mag_paths)\nids_ds.events = pl.concat(fps | select(func)) \nids_ds.export(output_path)",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/index.html#superposed-epoch-analysis",
    "href": "notebooks/missions/juno/index.html#superposed-epoch-analysis",
    "title": "IDs from Juno",
    "section": "Superposed epoch analysis",
    "text": "Superposed epoch analysis\n\n\nCode\nfrom discontinuitypy.utils.basic import df2ts\nfrom discontinuitypy.core.pipeline import compress_data_by_interval\nfrom xarray_einstats import linalg\n\nfrom sea_norm import sean\nimport pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import Figure, Axes\n\nfrom discontinuitypy.integration import J_FACTOR\n\nfrom tqdm.auto import tqdm\n\ndef keep_good_fit(df: pl.DataFrame, rsquared = 0.95):\n    return df.filter(pl.col('fit.stat.rsquared') &gt; rsquared)\n\n\n\n\nCode\nfrom discontinuitypy.propeties.mva import minvar\nimport numpy as np\n\n\ndef get_dBdt_data(data: pl.LazyFrame):\n    \"\"\"\n    Calculate the time derivative of the magnetic field\n    \"\"\"\n    # TODO: compress data first by events, however, this will decrease the reliability of the derivative at the edges of the events\n    ts = df2ts(data)\n\n    vec_diff = ts.differentiate(\"time\", datetime_unit=\"s\")\n    vec_diff_mag: xr.DataArray = linalg.norm(vec_diff, dims=\"v_dim\")\n    return vec_diff_mag.to_dataframe(name=\"dBdt\")\n\n\ndef get_mva_data(\n    data: pl.LazyFrame,\n    starts: list,\n    ends: list,\n    columns=[\"B_l\", \"B_m\", \"B_n\"],\n    normalize=True,\n):\n    for start, end in tqdm(zip(starts, ends)):\n        event_data = data.filter(\n            pl.col(\"time\") &gt;= start, pl.col(\"time\") &lt;= end\n        ).collect()\n\n        event_numpy = event_data.drop(\"time\").to_numpy()  # Assuming this is efficient for your use case\n        time = event_data.get_column(\"time\").to_numpy()\n        \n        vrot, _, _ = minvar(event_numpy)\n        \n        if True:\n            vl = vrot[:, 0]\n            vl = vl * np.sign(vl[-1] - vl[0])\n            vl_ts = xr.DataArray(vl, dims=\"time\", coords={\"time\": time})\n            dvl_dt_df = vl_ts.differentiate(\"time\", datetime_unit=\"s\").to_dataframe(name=\"dBl_dt\")\n\n        if normalize:\n            vrot = normalize_mva_data(vrot)\n            vrot_df = pd.DataFrame(vrot, columns=columns, index=time)\n            \n        yield pd.concat([vrot_df, dvl_dt_df], axis=1)\n\n\ndef normalize_mva_data(\n    data: np.ndarray, shift=False  # shift the data in l direction to the origin\n):\n    \"\"\"\n    normalize the MVA data: Bl, Bm, and Bn were respectively normalized to B0 = 0.5ΔBl, Bm, and &lt;B&gt;\n    \"\"\"\n    vl, vm, vn = data.T\n\n    vl_norm_q = (vl[-1] - vl[0]) / 2\n    vm_norm_q = (vm[0] + vm[-1]) / 2\n    vn_norm_q = (vn[0] + vn[-1]) / 2\n\n    return np.array([vl / vl_norm_q, vm / vm_norm_q, vn / vn_norm_q]).T\n\n\n\n\nCode\ndef sea_ids(\n    ds: IDsDataset,\n    event_cols=[\"t.d_start\", \"t.d_time\", \"t.d_end\"],\n    sea_cols=[\n        \"B_l\",\n        \"j_m\",\n        \"j_m_norm\",\n        \"j_k\",\n        \"j_k_norm\",\n        \"B_m\",\n    ],\n    bins=[10, 10],\n    return_data=True,\n):\n    # converting to a list of numpy arrays\n\n    events = keep_good_fit(ds.events)\n\n    sea_events = [col.to_numpy() for col in events[event_cols]]\n\n    mag_data = ds.data\n    mag_data_c = compress_data_by_interval(\n        mag_data.collect(), sea_events[0], sea_events[2]\n    ).lazy()\n\n    # dBdt_data = get_dBdt_data(ds.data)\n    dBdt_data = get_dBdt_data(mag_data_c)\n    mva_data = pd.concat(get_mva_data(mag_data_c, sea_events[0], sea_events[2]))\n\n    b_data = dBdt_data.join(mva_data, on=\"time\")\n\n    p_data = (\n        events[[\"time\", \"v_k\", \"j_Alfven\"]].to_pandas().set_index(\"time\").sort_index()\n    )\n\n    data = pd.merge_asof(\n        b_data,\n        p_data,\n        left_index=True,\n        right_index=True,\n        direction=\"nearest\",\n    )\n\n    data = data.assign(\n        j_m=lambda df: df.dBl_dt / df.v_k * J_FACTOR,\n        j_m_norm =lambda df: df.j_m / df.j_Alfven,\n        j_k=lambda df: df.dBdt / df.v_k * J_FACTOR,\n        j_k_norm=lambda df: df.j_k / df.j_Alfven,\n    )\n\n    return sean(data, sea_events, bins, return_data=return_data, cols=sea_cols)\n\n\ndef plot_SEA(SEAarray: pd.DataFrame, meta) -&gt; tuple[Figure, list[Axes]]:\n    \n    cols = meta[\"sea_cols\"]\n    fig, axes = plt.subplots(nrows=len(cols), sharex=True, squeeze=True)\n\n    if len(cols) == 1:\n        axes = [axes]\n\n    # loop over columns that were analyzed\n    for c, ax in zip(cols, axes):\n        # for each column identify the column titles which\n        # have 'c' in the title and those that don't have\n        # 'cnt' in the title\n        # e.g. for AE columns\n        # AE_mean, AE_median, AE_lowq, AE_upq, AE_cnt\n        # fine columns AE_mean, AE_median, AE_lowq, AE_upq\n        # mask = SEAarray.columns.str.startswith(c) & ~SEAarray.columns.str.endswith(\"cnt\")\n        mask = [c + \"_mean\", c + \"_median\", c + \"_lowq\", c + \"_upq\"]\n\n        # plot the SEA data\n        SEAarray.loc[:, mask].plot(\n            ax=ax,\n            style=[\"r-\", \"b-\", \"b--\", \"b--\"],\n            xlabel=\"Normalized Time\",\n            ylabel=c.replace(\"_\", \" \"),\n            legend=False,\n        )\n\n    return fig, axes\n\ndef plot_ids_sea(SEAarray: pd.DataFrame, meta) -&gt; tuple[Figure, list[Axes]]:\n    import scienceplots\n    \n    SEAarray.index = SEAarray.index.map(lambda x: x / bin/2)\n\n    with plt.style.context(['science', 'nature', 'notebook']):\n\n        fig, axes = plot_SEA(SEAarray, meta)\n\n        axes[0].set_ylabel(r\"$B_x \\ / \\ B_0$\")\n        axes[1].set_ylabel(r\"$J_y$\")\n        axes[2].set_ylabel(r\"$J_y \\ / \\ J_A$\")\n        axes[3].set_ylabel(r\"$J_k$\")\n        axes[4].set_ylabel(r\"$J_k \\ / \\ J_A$\")\n        axes[5].set_ylabel(r\"$B_m \\ / \\ B_g$\")\n\n        fig.tight_layout()\n        fig.subplots_adjust(hspace=0)\n        \n        axes[0].legend(labels=[\"Mean\", \"Median\", \"LowQ\", \"UpQ\"])\n    return fig, axes\n    # fig.savefig(f\"../../../figures/sea/sea_juno_first_year_{freq}.png\", dpi = 300)\n\n\n\nFirst year\n\n\nCode\nmag_path = sorted(list(walk(dir_path)))[0]\ntau = timedelta(seconds=60)\nmag_data = pl.scan_parquet(mag_path).drop('X', 'Y', 'Z').sort('time')\n\nids_ds = (\n    IDsDataset(\n        mag_data=mag_data,\n        plasma_data=plasma_data,\n        tau=tau,\n        ts=ts,\n        vec_cols=vec_cols,\n        density_col=\"plasma_density\",\n        speed_col=\"plasma_speed\",\n        temperature_col=\"plasma_temperature\",\n    )\n    .find_events(return_best_fit=True)\n    .update_candidates_with_plasma_data()\n)\n\n\n\n\nCode\nmag_paths = list(download_data(datatype=\"FULL\") | select(preprocess))\n\n\n\n\n\n\nCode\nfreq = 'high'\n# freq = 'low'\nif freq == 'high': # use high dimensional data\n    ids_ds.data = pl.scan_ipc(mag_paths[0:365]).drop('X', 'Y', 'Z').sort('time')\n    bin = 16\nelse: # use low dimensional data\n    bin = 8\n\nSEAarray, meta, p1data, p2data = sea_ids(ids_ds, bins=[bin, bin])\n\n\n\nFigure 1\n\n\n\n\nCode\nids_ds.plot_candidates(num=20, plot_fit_data=True, predicates=(pl.col('fit.stat.rsquared') &gt; 0.95))\n\n\n\n\n\n\nLast year\n\n\nCode\n# mag_path = sorted(list(walk(dir_path)))[-1]\ntau = timedelta(seconds=300)\ntau = timedelta(seconds=60)\nmag_path = sorted(list(walk(dir_path)))[-1]\nmag_data = pl.scan_parquet(mag_path).drop('X', 'Y', 'Z').sort('time')\n\nids_ds = (\n    IDsDataset(\n        mag_data=mag_data,\n        plasma_data=plasma_data,\n        tau=tau,\n        ts=ts,\n        vec_cols=vec_cols,\n        density_col=\"plasma_density\",\n        speed_col=\"plasma_speed\",\n        temperature_col=\"plasma_temperature\",\n    )\n    .find_events(return_best_fit=True)\n    .update_candidates_with_plasma_data()\n)\n\n\n\n\nCode\nmag_paths = list(download_data(datatype=\"FULL\") | select(preprocess))\n\nfreq = 'high'\n# freq = 'low'\nif freq == 'high': # use high dimensional data\n    ids_ds.data = pl.scan_ipc(mag_paths[-365:]).drop('X', 'Y', 'Z').sort('time')\n    bin = 16\nelse: # use low dimensional data\n    bin = 8\n\nSEAarray, meta, p1data, p2data = sea_ids(ids_ds, bins=[bin, bin])\n\n\n\n\nCode\nfig, axes = plot_ids_sea(SEAarray, meta)\nfig.savefig(f\"../../../figures/sea/sea_juno_last_year_{freq}.png\", dpi = 300)\n\n\n\nSuperposed epoch analysis\n\n\nCode\ngroupby = 'index'\ncolumns = ['dBdt', 'j_k']\n\n# Plot each group\nfig, axes = plt.subplots(nrows=len(columns))\n\nif len(columns) == 1:\n    axes = [axes]\n\np1groups = p1data.groupby(groupby)\np2groups = p2data.groupby(groupby)\n\nfor ax, column in zip(axes, columns):\n    for name, group in p1groups:\n        ax.plot(group['t_norm'] -1 , group[column], color='grey', alpha=0.3)\n        \n    for name, group in p2groups:\n        ax.plot(group['t_norm'], group[column], color='grey', alpha=0.3)\n\n# plt.yscale('log')\nplt.show()\n\n\n\nCode\nids_ds.plot_candidates(num=20, plot_fit_data=True, predicates=(pl.col('fit.stat.rsquared') &gt; 0.95))",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/index.html#processing-the-whole-data",
    "href": "notebooks/missions/juno/index.html#processing-the-whole-data",
    "title": "IDs from Juno",
    "section": "Processing the whole data",
    "text": "Processing the whole data",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/index.html#obsolete",
    "href": "notebooks/missions/juno/index.html#obsolete",
    "title": "IDs from Juno",
    "section": "Obsolete",
    "text": "Obsolete\n\nEstimate\n1 day of data resampled by 1 sec is about 12 MB.\nSo 1 year of data is about 4 GB, and 6 years of JUNO Cruise data is about 24 GB.\nDownloading rate is about 250 KB/s, so it will take about 3 days to download all the data.\n\n\nCode\nnum_of_files = 6*365\njno_file_size = 12e3\nthm_file_size = 40e3\nfiles_size = jno_file_size + thm_file_size\ndownloading_rate = 250\nprocessing_rate = 1/60\n\ntime_to_download = num_of_files * files_size / downloading_rate / 60 / 60\nspace_required = num_of_files * files_size / 1e6\ntime_to_process = num_of_files / processing_rate / 60 / 60\n\nprint(f\"Time to download: {time_to_download:.2f} hours\")\nprint(f\"Disk space required: {space_required:.2f} GB\")\nprint(f\"Time to process: {time_to_process:.2f} hours\")",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "notebooks/missions/wind.html",
    "href": "notebooks/missions/wind.html",
    "title": "IDs from Wind",
    "section": "",
    "text": "See following notebooks for details:\nReferences:\nNotes:\nCode\nfrom space_analysis.utils.speasy import Variables\nfrom discontinuitypy.datasets import IDsDataset\nfrom discontinuitypy.utils.basic import resample\nfrom beforerr.polars import pl_norm\n\nfrom datetime import timedelta\n\nfrom space_analysis.plasma.formulary import df_thermal_spd2temp\nCode\nfrom discontinuitypy.missions import wind_mag_h4_rtn_meta, wind_plasma_k0_swe_meta\nCode\ntimerange = [\"2011-08-25\", \"2016-06-30\"]\nmission = \"Wind\"\nts = timedelta(seconds=1)\ntau = timedelta(seconds=60)\n\nprovider = \"archive/local\"\nmag_dataset = \"WI_H4-RTN_MFI\"\nmag_parameters = [\"BRTN\"]\nplasma_dataset = \"WI_K0_SWE\"\nplasma_parameters = [\"Np\", \"V_GSM\", \"THERMAL_SPD\"]",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Wind"
    ]
  },
  {
    "objectID": "notebooks/missions/wind.html#downloading-data-file-first",
    "href": "notebooks/missions/wind.html#downloading-data-file-first",
    "title": "IDs from Wind",
    "section": "Downloading data file first",
    "text": "Downloading data file first\n\n\nCode\nimport pyspedas\n\n\ndef download(timerange):\n    pyspedas.wind.threedp(timerange, datatype=\"3dp_pm\", downloadonly=True)\n    pyspedas.wind.mfi(timerange, datatype=\"h2\", downloadonly=True)\n    # pyspedas.wind.swe(timerange, datatype='k0', downloadonly=True)",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Wind"
    ]
  },
  {
    "objectID": "notebooks/missions/wind.html#loading",
    "href": "notebooks/missions/wind.html#loading",
    "title": "IDs from Wind",
    "section": "Loading",
    "text": "Loading\n\n\nCode\ndef get_and_process_data(\n    mag_dataset,\n    mag_parameters,\n    plasma_dataset,\n    plasma_parameters,\n    timerange,\n    tau,\n    ts,\n    provider=\"archive/local\",\n):\n    # define variables\n    mag_vars = Variables(\n        provider=provider,\n        dataset=mag_dataset,\n        parameters=mag_parameters,\n        timerange=timerange,\n    )\n\n    plasma_vars = Variables(\n        provider=provider,\n        dataset=plasma_dataset,\n        parameters=plasma_parameters,\n        timerange=timerange,\n    )\n\n    # get column names\n    bcols = mag_vars.data[0].columns\n    density_col = plasma_vars.data[0].columns[0]\n    vec_cols = plasma_vars.data[1].columns\n    temperature_col = plasma_vars.data[2].columns[0]\n\n    # get data\n    mag_data = mag_vars.to_polars()\n    plasma_data = (\n        plasma_vars.to_polars()\n        .with_columns(plasma_speed=pl_norm(vec_cols))\n        .rename({density_col: \"plasma_density\"})\n    )\n    # process temperature data\n    if plasma_vars.data[2].unit == \"km/s\":\n        plasma_data = plasma_data.pipe(df_thermal_spd2temp, temperature_col)\n    else:\n        plasma_data = plasma_data.rename({temperature_col: \"plasma_temperature\"})\n\n    return (\n        IDsDataset(\n            mag_data=mag_data.pipe(resample, every=ts),\n            plasma_data=plasma_data,\n            tau=tau,\n            ts=ts,\n            bcols=bcols,\n            vec_cols=vec_cols,\n            density_col=\"plasma_density\",\n            speed_col=\"plasma_speed\",\n            temperature_col=\"plasma_temperature\",\n        )\n        .find_events(return_best_fit=False)\n        .update_candidates_with_plasma_data()\n    )",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Wind"
    ]
  },
  {
    "objectID": "notebooks/missions/themis/mag.html",
    "href": "notebooks/missions/themis/mag.html",
    "title": "THEMIS Magnetic field data pipeline",
    "section": "",
    "text": "For convenience, we choose magnetic field data in GSE coordinate system\nThe fgs data are in 3-4s resolution",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/themis/mag.html#loading-data",
    "href": "notebooks/missions/themis/mag.html#loading-data",
    "title": "THEMIS Magnetic field data pipeline",
    "section": "Loading data",
    "text": "Loading data\n\n\nCode\nfrom space_analysis.utils.speasy import Variables\nfrom datetime import timedelta\n\n\n\n\nCode\ntimerange = [\"2011-08-25\", \"2016-06-30\"]\n# timerange = [\"2011-08-25\", \"2011-09-01\"]\nmission = \"THB\"\nts = timedelta(seconds=1)\ntau = timedelta(seconds=60)\n\nprovider = 'archive/local'\nmag_dataset = \"THB_L2_FGM\"\nmag_parameters = [\"thb_fgl_gse\"]\n\nmag_vars = Variables(\n    provider = provider,\n    dataset=mag_dataset,\n    parameters=mag_parameters,\n    timerange=timerange,\n).retrieve_data()\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[10], line 14\n      7 mag_dataset = \"THB_L2_FGM\"\n      8 mag_parameters = [\"thb_fgl_gse\"]\n     10 mag_vars = Variables(\n     11     dataset=mag_dataset,\n     12     parameters=mag_parameters,\n     13     timerange=timerange,\n---&gt; 14 ).retrieve_data()\n     16 plasma_dataset = 'THB_L2_MOM'\n     17 plasma_parameters = [\"thb_peim_densityQ\", \"thb_peim_velocity_gseQ\"]\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/space_analysis/utils/speasy.py:84, in Variables.retrieve_data(self)\n     80     self.data = spz.get_data(\n     81         self.products, self.timerange\n     82     )\n     83 else:\n---&gt; 84     self.data = spz.get_data(\n     85         self.products, self.timerange, disable_proxy=self._disable_proxy\n     86     )\n     87 return self\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:316, in get_data(*args, **kwargs)\n    314 product = args[0]\n    315 if is_collection(product) and not isinstance(product, SpeasyIndex):\n--&gt; 316     return list(map(lambda p: get_data(p, *args[1:], **kwargs), progress_bar(leave=True, **kwargs)(product)))\n    318 if len(args) == 1:\n    319     return _get_catalog_or_timetable(*args, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:316, in get_data.&lt;locals&gt;.&lt;lambda&gt;(p)\n    314 product = args[0]\n    315 if is_collection(product) and not isinstance(product, SpeasyIndex):\n--&gt; 316     return list(map(lambda p: get_data(p, *args[1:], **kwargs), progress_bar(leave=True, **kwargs)(product)))\n    318 if len(args) == 1:\n    319     return _get_catalog_or_timetable(*args, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:323, in get_data(*args, **kwargs)\n    321 t_range = args[1]\n    322 if _is_dtrange(t_range):\n--&gt; 323     return _get_timeserie1(*args, **kwargs)\n    324 if is_collection(t_range):\n    325     return list(\n    326         map(lambda r: get_data(product, r, *args[2:], **kwargs),\n    327             progress_bar(leave=False, **kwargs)(t_range)))\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:166, in _get_timeserie1(index, dtrange, **kwargs)\n    165 def _get_timeserie1(index, dtrange, **kwargs):\n--&gt; 166     return _scalar_get_data(index, dtrange[0], dtrange[1], **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:157, in _scalar_get_data(index, *args, **kwargs)\n    155 provider_uid, product_uid = provider_and_product(index)\n    156 if provider_uid in PROVIDERS:\n--&gt; 157     return PROVIDERS[provider_uid].get_data(product_uid, *args, **kwargs)\n    158 raise ValueError(f\"Can't find a provider for {index}\")\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/__init__.py:223, in AllowedKwargs.__call__.&lt;locals&gt;.wrapped(*args, **kwargs)\n    220 unexpected_args = list(\n    221     filter(lambda arg_name: arg_name not in self.allowed_list, kwargs.keys()))\n    222 if not unexpected_args:\n--&gt; 223     return func(*args, **kwargs)\n    224 raise TypeError(\n    225     f\"Unexpected keyword argument {unexpected_args}, allowed keyword arguments are {self.allowed_list}\")\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/dataprovider.py:30, in ParameterRangeCheck.__call__.&lt;locals&gt;.wrapped(wrapped_self, product, start_time, stop_time, **kwargs)\n     28     log.warning(f\"You are requesting {product} outside of its definition range {p_range}\")\n     29     return None\n---&gt; 30 return get_data(wrapped_self, product=product, start_time=start_time, stop_time=stop_time, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/cache/_providers_caches.py:247, in UnversionedProviderCache.__call__.&lt;locals&gt;.wrapped(wrapped_self, product, start_time, stop_time, **kwargs)\n    243 fragment_duration = timedelta(hours=fragment_hours)\n    244 data_chunks, maybe_outdated_fragments, missing_fragments = self.split_fragments(fragments, product,\n    245                                                                                 fragment_duration, **kwargs)\n    246 data_chunks += \\\n--&gt; 247     list(filter(lambda d: d is not None, [\n    248         self._cache.add_to_cache(\n    249             get_data(\n    250                 wrapped_self, product=product, start_time=fragment_group[0],\n    251                 stop_time=fragment_group[-1] + fragment_duration, **kwargs),\n    252             fragments=fragment_group, product=product, fragment_duration_hours=fragment_hours,\n    253             version=datetime.utcnow(), **kwargs)\n    254         for fragment_group\n    255         in progress_bar(leave=False, desc=\"Downloading missing fragments from cache\", **kwargs)(\n    256             missing_fragments)]))\n    258 for group in progress_bar(leave=False, desc=\"Checking if cache fragments are outdated\", **kwargs)(\n    259     maybe_outdated_fragments):\n    260     oldest = max(group, key=lambda item: item[1].version)[1].version\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/cache/_providers_caches.py:249, in &lt;listcomp&gt;(.0)\n    243 fragment_duration = timedelta(hours=fragment_hours)\n    244 data_chunks, maybe_outdated_fragments, missing_fragments = self.split_fragments(fragments, product,\n    245                                                                                 fragment_duration, **kwargs)\n    246 data_chunks += \\\n    247     list(filter(lambda d: d is not None, [\n    248         self._cache.add_to_cache(\n--&gt; 249             get_data(\n    250                 wrapped_self, product=product, start_time=fragment_group[0],\n    251                 stop_time=fragment_group[-1] + fragment_duration, **kwargs),\n    252             fragments=fragment_group, product=product, fragment_duration_hours=fragment_hours,\n    253             version=datetime.utcnow(), **kwargs)\n    254         for fragment_group\n    255         in progress_bar(leave=False, desc=\"Downloading missing fragments from cache\", **kwargs)(\n    256             missing_fragments)]))\n    258 for group in progress_bar(leave=False, desc=\"Checking if cache fragments are outdated\", **kwargs)(\n    259     maybe_outdated_fragments):\n    260     oldest = max(group, key=lambda item: item[1].version)[1].version\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/split_large_requests.py:25, in SplitLargeRequests.__call__.&lt;locals&gt;.wrapped(wrapped_self, product, start_time, stop_time, **kwargs)\n     22 else:\n     23     fragments = range.split(max_range_per_request)\n     24     return var_merge(\n---&gt; 25         [get_data(wrapped_self, product=product, start_time=r.start_time, stop_time=r.stop_time, **kwargs)\n     26          for r in fragments])\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/split_large_requests.py:25, in &lt;listcomp&gt;(.0)\n     22 else:\n     23     fragments = range.split(max_range_per_request)\n     24     return var_merge(\n---&gt; 25         [get_data(wrapped_self, product=product, start_time=r.start_time, stop_time=r.stop_time, **kwargs)\n     26          for r in fragments])\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/proxy/__init__.py:129, in Proxyfiable.__call__.&lt;locals&gt;.wrapped(*args, **kwargs)\n    127     except:  # lgtm [py/catch-base-exception]\n    128         log.error(f\"Can't get data from proxy server {proxy_cfg.url()}\")\n--&gt; 129 return func(*args, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/webservices/cda/__init__.py:173, in CDA_Webservice.get_data(self, product, start_time, stop_time, if_newer_than, extra_http_headers)\n    164 @AllowedKwargs(\n    165     PROXY_ALLOWED_KWARGS + CACHE_ALLOWED_KWARGS + GET_DATA_ALLOWED_KWARGS + ['if_newer_than'])\n    166 @ParameterRangeCheck()\n   (...)\n    170 def get_data(self, product, start_time: datetime, stop_time: datetime, if_newer_than: datetime or None = None,\n    171              extra_http_headers: Dict or None = None):\n    172     dataset, variable = self._to_dataset_and_variable(product)\n--&gt; 173     return self._dl_variable(start_time=start_time, stop_time=stop_time, dataset=dataset,\n    174                              variable=variable, if_newer_than=if_newer_than, extra_http_headers=extra_http_headers)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/webservices/cda/__init__.py:155, in CDA_Webservice._dl_variable(self, dataset, variable, start_time, stop_time, if_newer_than, extra_http_headers)\n    153 log.debug(resp.url)\n    154 if resp.status_code == 200 and 'FileDescription' in resp.json():\n--&gt; 155     return cdf.load_variable(file=resp.json()['FileDescription'][0]['Name'], variable=variable)\n    156 elif not resp.ok:\n    157     if resp.status_code == 404 and \"No data available\" in resp.json().get('Message', [\"\"])[0]:\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/cdf/__init__.py:77, in load_variable(variable, file, cache_remote_files)\n     74     if is_local_file(file):\n     75         return _load_variable(variable=variable, file=urlparse(url=file).path)\n     76     return _load_variable(variable=variable,\n---&gt; 77                           buffer=any_loc_open(file, mode='rb', cache_remote_files=cache_remote_files).read())\n     78 if type(file) is bytes:\n     79     return _load_variable(variable=variable, buffer=bytes)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/any_files.py:102, in any_loc_open(url, timeout, headers, mode, cache_remote_files)\n    100 else:\n    101     if cache_remote_files:\n--&gt; 102         last_modified = http.head(url).getheader('last-modified', str(datetime.now()))\n    103         cache_item: Optional[CacheItem] = get_item(url)\n    104         if cache_item is None or last_modified != cache_item.version:\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/http.py:93, in _HttpVerb.__call__(self, url, headers, params, timeout)\n     91 headers = headers or {}\n     92 headers['User-Agent'] = USER_AGENT\n---&gt; 93 return Response(self._verb(url=url, headers=headers, fields=params, timeout=timeout))\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/urllib3/_request_methods.py:110, in RequestMethods.request(self, method, url, body, fields, headers, json, **urlopen_kw)\n    107     urlopen_kw[\"body\"] = body\n    109 if method in self._encode_url_methods:\n--&gt; 110     return self.request_encode_url(\n    111         method,\n    112         url,\n    113         fields=fields,  # type: ignore[arg-type]\n    114         headers=headers,\n    115         **urlopen_kw,\n    116     )\n    117 else:\n    118     return self.request_encode_body(\n    119         method, url, fields=fields, headers=headers, **urlopen_kw\n    120     )\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/urllib3/_request_methods.py:143, in RequestMethods.request_encode_url(self, method, url, fields, headers, **urlopen_kw)\n    140 if fields:\n    141     url += \"?\" + urlencode(fields)\n--&gt; 143 return self.urlopen(method, url, **extra_kw)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/urllib3/poolmanager.py:444, in PoolManager.urlopen(self, method, url, redirect, **kw)\n    442     response = conn.urlopen(method, url, **kw)\n    443 else:\n--&gt; 444     response = conn.urlopen(method, u.request_uri, **kw)\n    446 redirect_location = redirect and response.get_redirect_location()\n    447 if not redirect_location:\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/urllib3/connectionpool.py:790, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\n    787 response_conn = conn if not release_conn else None\n    789 # Make the request on the HTTPConnection object\n--&gt; 790 response = self._make_request(\n    791     conn,\n    792     method,\n    793     url,\n    794     timeout=timeout_obj,\n    795     body=body,\n    796     headers=headers,\n    797     chunked=chunked,\n    798     retries=retries,\n    799     response_conn=response_conn,\n    800     preload_content=preload_content,\n    801     decode_content=decode_content,\n    802     **response_kw,\n    803 )\n    805 # Everything went great!\n    806 clean_exit = True\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/urllib3/connectionpool.py:536, in HTTPConnectionPool._make_request(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\n    534 # Receive the response from the server\n    535 try:\n--&gt; 536     response = conn.getresponse()\n    537 except (BaseSSLError, OSError) as e:\n    538     self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/urllib3/connection.py:461, in HTTPConnection.getresponse(self)\n    458 from .response import HTTPResponse\n    460 # Get the response from http.client.HTTPConnection\n--&gt; 461 httplib_response = super().getresponse()\n    463 try:\n    464     assert_header_parsing(httplib_response.msg)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/http/client.py:1386, in HTTPConnection.getresponse(self)\n   1384 try:\n   1385     try:\n-&gt; 1386         response.begin()\n   1387     except ConnectionError:\n   1388         self.close()\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/http/client.py:325, in HTTPResponse.begin(self)\n    323 # read until we get a non-100 response\n    324 while True:\n--&gt; 325     version, status, reason = self._read_status()\n    326     if status != CONTINUE:\n    327         break\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/http/client.py:286, in HTTPResponse._read_status(self)\n    285 def _read_status(self):\n--&gt; 286     line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n    287     if len(line) &gt; _MAXLINE:\n    288         raise LineTooLong(\"status line\")\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/socket.py:706, in SocketIO.readinto(self, b)\n    704 while True:\n    705     try:\n--&gt; 706         return self._sock.recv_into(b)\n    707     except timeout:\n    708         self._timeout_occurred = True\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/ssl.py:1315, in SSLSocket.recv_into(self, buffer, nbytes, flags)\n   1311     if flags != 0:\n   1312         raise ValueError(\n   1313           \"non-zero flags not allowed in calls to recv_into() on %s\" %\n   1314           self.__class__)\n-&gt; 1315     return self.read(nbytes, buffer)\n   1316 else:\n   1317     return super().recv_into(buffer, nbytes, flags)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/ssl.py:1167, in SSLSocket.read(self, len, buffer)\n   1165 try:\n   1166     if buffer is not None:\n-&gt; 1167         return self._sslobj.read(len, buffer)\n   1168     else:\n   1169         return self._sslobj.read(len)\n\nKeyboardInterrupt: \n\n\n\n\n\nCode\nmag_vars.to_polars()\n\n\nnaive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n    \n    DF [\"Bx FGL-D\", \"By FGL-D\", \"Bz FGL-D\", \"time\"]; PROJECT */4 COLUMNS; SELECTION: \"None\"\n\n\n::: {#cell-5 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef preprocess_data(\n    raw_data: pl.LazyFrame,\n    datatype: str = None,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Preprocess the raw dataset (only minor transformations)\n\n    - Applying naming conventions for columns\n    - Dropping duplicate time\n    - Changing storing format to `parquet`\n\n    \"\"\"\n\n    datatype = datatype.upper()\n    name_mapping = {\n        f\"Bx {datatype}-D\": \"B_x\",\n        f\"By {datatype}-D\": \"B_y\",\n        f\"Bz {datatype}-D\": \"B_z\",\n    }\n\n    return raw_data.sort(\"time\").unique(\"time\").rename(name_mapping)\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/index.html",
    "href": "notebooks/missions/index.html",
    "title": "Missions",
    "section": "",
    "text": "This module includes helpful functions for working with mission (magnetic field and plasma) data. We currently support the following missions:\n\nParker Solar Probe\nJuno\nWind\nTHEMIS/ARTEMIS\nSTEREO\n\nNote: speasy and pyspedas are not suitable for processing large datasets, although they are useful for downloading and processing small datasets.\n\nspeasy cache file and data are not persistent\npyspedas loads the entire dataset into memory",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions"
    ]
  },
  {
    "objectID": "notebooks/missions/stereo/state.html",
    "href": "notebooks/missions/stereo/state.html",
    "title": "STEREO State data pipeline",
    "section": "",
    "text": "magplasma Data\nFor STEREO’s mission, We use 1-hour averaged merged data from COHOWeb.\nSee STEREO ASCII merged data and one sample file [here](https://spdf.gsfc.nasa.gov/pub/data/stereo/ahead/l2/merged/stereoa2011.asc\nPlasma in RTN (Radial-Tangential-Normal) coordinate system\n- Proton Flow Speed, km/sec\n- Proton Flow Elevation Angle/Latitude, deg.\n- Proton Flow Azimuth Angle/Longitude, deg.\n- Proton Density, n/cc\n- Proton Temperature, K)\nNotes\n- Note1: There is a big gap 2014/12/16 - 2015/07/20 in plasma data\n- Note2: There is a big gap 2015/03/21 - 2015/07/09 and 2015/10/27 - 2015/11/15 in mag data\n- Note that for missing data, fill values consisting of a blank followed by 9’s which together constitute the format are used\nCode\n\nimport polars as pl\nimport pandas\n\nfrom discontinuitypy.pipelines.default.data import create_pipeline_template\nCode\nimport pyspedas\npyspedas.stereo.load",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/stereo/state.html#loading-data",
    "href": "notebooks/missions/stereo/state.html#loading-data",
    "title": "STEREO State data pipeline",
    "section": "Loading data",
    "text": "Loading data\n\n\nCode\n\nimport pooch\nfrom pipe import select\nfrom discontinuitypy.utils.basic import pmap\n\n\n\n\nCode\ndef download_data(\n    start: str,\n    end: str,\n    datatype,\n) -&gt; list[str]:\n    start_time = pandas.Timestamp(start)\n    end_time = pandas.Timestamp(end)\n\n    url = \"https://spdf.gsfc.nasa.gov/pub/data/stereo/ahead/l2/merged/stereoa{year}.asc\"\n\n    files = list(\n        range(start_time.year, end_time.year + 1)\n        | select(lambda x: url.format(year=x))\n        | pmap(pooch.retrieve, known_hash=None)\n    )\n    return files\n\n\n\n\nCode\n\nheaders = \"\"\"Year\nDOY\nHour\nRadial Distance, AU\nHGI Lat. of the S/C\nHGI Long. of the S/C\nIMF BR, nT (RTN)\nIMF BT, nT (RTN)\nIMF BN, nT (RTN)\nIMF B Scalar, nT\nSW Plasma Speed, km/s\nSW Lat. Angle RTN, deg.\nSW Long. Angle RTN, deg.\nSW Plasma Density, N/cm^3\nSW Plasma Temperature, K\n1.8-3.6 MeV H flux,LET\n4.0-6.0 MeV H flux,LET\n6.0-10.0 MeV H flux, LET\n10.0-12.0 MeV H flux,LET\n13.6-15.1 MeV H flux, HET\n14.9-17.1 MeV H flux, HET\n17.0-19.3 MeV H flux, HET\n20.8-23.8 MeV H flux, HET\n23.8-26.4 MeV H flux, HET\n26.3-29.7 MeV H flux, HET\n29.5-33.4 MeV H flux, HET\n33.4-35.8 MeV H flux, HET\n35.5-40.5 MeV H flux, HET\n40.0-60.0 MeV H flux, HET\n60.0-100.0 MeV H flux, HET\n0.320-0.452 MeV H flux, SIT\n0.452-0.64 MeV H flux, SIT\n0.640-0.905 MeV H flux, SIT\n0.905-1.28 MeV H flux, SIT\n1.280-1.81 MeV H flux, SIT\n1.810-2.56 MeV H flux, SIT\n2.560-3.62 MeV H flux, SIT\"\"\"\n\ndef load_data(\n    start: str,\n    end: str,\n    datatype = 'hourly',\n) -&gt; pl.DataFrame:\n    \"\"\"\n    - Downloading data\n    - Reading data into a proper data structure, like dataframe.\n        - Parsing original data (dealing with delimiters, missing values, etc.)\n    \"\"\"\n    files = download_data(start, end, datatype=datatype)\n    \n    labels = headers.split(\"\\n\")\n    missing_values = [\"999.99\", \"9999.9\", \"9999999.\"]\n\n    df = pl.concat(\n        files\n        | pmap(\n            pandas.read_csv,\n            delim_whitespace=True,\n            names=labels,\n            na_values=missing_values,\n        )\n        | select(pl.from_pandas)\n    )\n    \n    return df",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/stereo/state.html#preprocessing-data",
    "href": "notebooks/missions/stereo/state.html#preprocessing-data",
    "title": "STEREO State data pipeline",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n\n\nCode\n\ndef preprocess_data(\n    raw_data: pl.DataFrame,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Preprocess the raw dataset (only minor transformations)\n\n    - Parsing and typing data (like from string to datetime for time columns)\n    - Changing storing format (like from `csv` to `parquet`)\n    \"\"\"\n\n    return raw_data.with_columns(\n        time=pl.datetime(pl.col(\"Year\"), month=1, day=1)\n        + pl.duration(days=pl.col(\"DOY\") - 1, hours=pl.col(\"Hour\"))\n    )",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/stereo/state.html#processs-state-data",
    "href": "notebooks/missions/stereo/state.html#processs-state-data",
    "title": "STEREO State data pipeline",
    "section": "Processs state data",
    "text": "Processs state data\n\n\nCode\n\nSTATE_POSITION_COLS = [\n    \"Radial Distance, AU\",\n    \"HGI Lat. of the S/C\",\n    \"HGI Long. of the S/C\",\n]\n\nSTATE_IMF_COLS = [\n    \"IMF BR, nT (RTN)\",\n    \"IMF BT, nT (RTN)\",\n    \"IMF BN, nT (RTN)\",\n    \"IMF B Scalar, nT\",\n]\n\nSTATE_PLASMA_COLS = [\n    \"SW Plasma Speed, km/s\",\n    \"SW Lat. Angle RTN, deg.\",\n    \"SW Long. Angle RTN, deg.\",\n    \"SW Plasma Density, N/cm^3\",\n    \"SW Plasma Temperature, K\",\n]\n\nDEFAULT_columns_name_mapping = {\n    \"SW Plasma Speed, km/s\": \"plasma_speed\",\n    \"SW Lat. Angle RTN, deg.\": \"sw_elevation\",\n    \"SW Long. Angle RTN, deg.\": \"sw_azimuth\",\n    \"SW Plasma Density, N/cm^3\": \"plasma_density\",\n    \"SW Plasma Temperature, K\": \"plasma_temperature\",\n    \"Radial Distance, AU\": \"radial_distance\",\n}\n\n\n\n\nCode\n\ndef convert_state_to_rtn(df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Convert state data to RTN coordinates\"\"\"\n    plasma_speed = pl.col(\"plasma_speed\")\n    sw_elevation = pl.col(\"sw_elevation\").radians()\n    sw_azimuth = pl.col(\"sw_azimuth\").radians()\n    return df.with_columns(\n        sw_vel_r=plasma_speed * sw_elevation.cos() * sw_azimuth.cos(),\n        sw_vel_t=plasma_speed * sw_elevation.cos() * sw_azimuth.sin(),\n        sw_vel_n=plasma_speed * sw_elevation.sin(),\n    ).drop([\"sw_elevation\", \"sw_azimuth\"])\n\ndef process_data(\n    raw_data: pl.DataFrame,\n    ts=None,  # time resolution\n    columns: list[str] = STATE_POSITION_COLS + STATE_PLASMA_COLS + STATE_IMF_COLS,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Corresponding to primary data layer, where source data models are transformed into domain data models\n\n    - Applying naming conventions for columns\n    - Transforming data to RTN (Radial-Tangential-Normal) coordinate system\n    - Discarding unnecessary columns\n    \"\"\"\n\n    name_mapping = {\n        \"sw_vel_r\": \"v_x\",\n        \"sw_vel_t\": \"v_y\",\n        \"sw_vel_n\": \"v_z\",\n        \"IMF BR, nT (RTN)\": \"B_background_x\",\n        \"IMF BT, nT (RTN)\": \"B_background_y\",\n        \"IMF BN, nT (RTN)\": \"B_background_z\",\n    }\n\n    return (\n        raw_data.select(\"time\", *columns)\n        .rename(DEFAULT_columns_name_mapping)\n        .pipe(convert_state_to_rtn)\n        .rename(name_mapping)\n    )",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/stereo/state.html#pipelines",
    "href": "notebooks/missions/stereo/state.html#pipelines",
    "title": "STEREO State data pipeline",
    "section": "Pipelines",
    "text": "Pipelines\n\n\nCode\n\ndef create_pipeline(sat_id=\"STA\", source=\"STATE\"):\n    return create_pipeline_template(\n        sat_id=sat_id,\n        source=source,\n        load_data_fn=load_data,\n        preprocess_data_fn=preprocess_data,\n        process_data_fn=process_data,\n    )",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/archive/05_sea.html",
    "href": "notebooks/archive/05_sea.html",
    "title": "Superposed epoch analysis",
    "section": "",
    "text": "Try to decouple the large scale and small scale currents in the discontinuity.\n\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nimport polars as pl\n\nfrom datetime import timedelta\nfrom discontinuitypy.utils.basic import df2ts\nfrom discontinuitypy.core.pipeline import compress_dfs_by_intervals\nfrom xarray_einstats import linalg\nfrom sea_norm import sean\nimport pandas as pd\nimport xarray as xr\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import Figure, Axes\n\nfrom discontinuitypy.integration import J_FACTOR\n\nfrom utils import keep_good_fit\n\nfrom loguru import logger\n\n\nTqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\nUserWarning: Traceback (most recent call last):\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/__init__.py\", line 85, in &lt;module&gt;\n    from . import skintegrate\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/skintegrate.py\", line 20, in &lt;module&gt;\n    from sklearn.base import BaseEstimator\nModuleNotFoundError: No module named 'sklearn'\n\nUserWarning: pdpipe: Scikit-learn or skutil import failed. Scikit-learn-dependent pipeline stages will not be loaded.\nUserWarning: Traceback (most recent call last):\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/__init__.py\", line 105, in &lt;module&gt;\n    from . import nltk_stages\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/nltk_stages.py\", line 19, in &lt;module&gt;\n    import nltk\nModuleNotFoundError: No module named 'nltk'\n\nUserWarning: pdpipe: nltk import failed. nltk-dependent  pipeline stages will not be loaded.\n\n\n\n\nCode\nfrom discontinuitypy.propeties.mva import minvar\nimport numpy as np\n\ndef get_dBdt_data(data: pl.LazyFrame):\n    \"\"\"\n    Calculate the time derivative of the magnetic field\n    \"\"\"\n    # TODO: compress data first by events, however, this will decrease the reliability of the derivative at the edges of the events\n    ts = df2ts(data)\n\n    vec_diff = ts.differentiate(\"time\", datetime_unit=\"s\")\n    vec_diff_mag: xr.DataArray = linalg.norm(vec_diff, dims=\"v_dim\")\n    return vec_diff_mag.to_dataframe(name=\"dBdt\")\n\ndef _get_mva_data(\n    data: pl.LazyFrame,\n    start,\n    end,\n    columns=[\"B_l\", \"B_m\", \"B_n\"],\n    normalize=True,\n):\n    event_data = data.filter(pl.col(\"time\") &gt;= start, pl.col(\"time\") &lt;= end).collect()\n\n    event_numpy = event_data.drop(\n        \"time\"\n    ).to_numpy()  # Assuming this is efficient for your use case\n    time = event_data.get_column(\"time\").to_numpy()\n\n    vrot, _, _ = minvar(event_numpy)\n\n    if True:\n        vl = vrot[:, 0]\n        vl = vl * np.sign(vl[-1] - vl[0])\n        vl_ts = xr.DataArray(vl, dims=\"time\", coords={\"time\": time})\n        dvl_dt_df = vl_ts.differentiate(\"time\", datetime_unit=\"s\").to_dataframe(\n            name=\"dBl_dt\"\n        )\n\n    if normalize:\n        vrot = normalize_mva_data(vrot)\n        vrot_df = pd.DataFrame(vrot, columns=columns, index=time)\n\n    return pd.concat([vrot_df, dvl_dt_df], axis=1)[1:-1]\n\n\ndef get_mva_data(\n    data: pl.LazyFrame,\n    starts: list,\n    ends: list,\n    **kwargs,\n):\n    \"\"\"\n    Get the MVA data for the given intervals\n    \"\"\"\n    return pd.concat(\n        _get_mva_data(data, start, end, **kwargs) for start, end in zip(starts, ends)\n    )\n\n\ndef normalize_mva_data(\n    data: np.ndarray, shift=False  # shift the data in l direction to the origin\n):\n    \"\"\"\n    normalize the MVA data: Bl, Bm, and Bn were respectively normalized to B0 = 0.5ΔBl, Bm, and &lt;B&gt;\n    \"\"\"\n    vl, vm, vn = data.T\n\n    vl_norm_q = (vl[-1] - vl[0]) / 2\n    vm_norm_q = (vm[0] + vm[-1]) / 2\n    vn_norm_q = (vn[0] + vn[-1]) / 2\n\n    return np.array([vl / vl_norm_q, vm / vm_norm_q, vn / vn_norm_q]).T\n\n\n\n\nCode\ndef sea_ids(\n    dfs: list[pl.LazyFrame],\n    events: pl.DataFrame,\n    start_col=\"t.d_start\",\n    end_col=\"t.d_end\",\n    event_cols=[\"t.d_start\", \"t.d_time\", \"t.d_end\"],\n    sea_cols=[\n        \"B_l\",\n        \"j_m\",\n        \"j_m_norm\",\n        \"B_m\",\n    ],\n    offset = timedelta(seconds=1),\n    bins=[10, 10],\n    return_data=True,\n):\n\n    if not isinstance(dfs, list):\n        dfs = [dfs]\n\n    events = keep_good_fit(events)\n\n    sea_events = [col.to_numpy() for col in events[event_cols]]\n\n    starts = events.get_column(start_col) - offset\n    ends = events.get_column(end_col) + offset\n\n    logger.info(\"Compressing data by events\")\n    data_c = compress_dfs_by_intervals(dfs, starts, ends).lazy()\n    \n    mva_data = get_mva_data(data_c, starts=starts, ends=ends)\n    b_data = mva_data.sort_index()\n    # dBdt_data = get_dBdt_data(data_c)\n    # b_data = dBdt_data.join(mva_data, on=\"time\")\n\n    p_data = (\n        events[[\"time\", \"v_k\", \"j_Alfven\"]].to_pandas().set_index(\"time\").sort_index()\n    )\n\n    sea_data = pd.merge_asof(\n        b_data,\n        p_data,\n        left_index=True,\n        right_index=True,\n        direction=\"nearest\",\n    )\n\n    sea_data = sea_data.assign(\n        j_m=lambda df: df.dBl_dt / df.v_k * J_FACTOR,\n        j_m_norm=lambda df: df.j_m / df.j_Alfven,\n        # j_k=lambda df: df.dBdt / df.v_k * J_FACTOR,\n        # j_k_norm=lambda df: df.j_k / df.j_Alfven,\n    )\n\n    return sean(sea_data, sea_events, bins, return_data=return_data, cols=sea_cols)\n\n\n\n\nCode\ndef plot_SEA(SEAarray: pd.DataFrame, meta) -&gt; tuple[Figure, list[Axes]]:\n    \n    cols = meta[\"sea_cols\"]\n    fig, axes = plt.subplots(nrows=len(cols), sharex=True, squeeze=True)\n\n    if len(cols) == 1:\n        axes = [axes]\n\n    # loop over columns that were analyzed\n    for c, ax in zip(cols, axes):\n        # for each column identify the column titles which\n        # have 'c' in the title and those that don't have\n        # 'cnt' in the title\n        # e.g. for AE columns\n        # AE_mean, AE_median, AE_lowq, AE_upq, AE_cnt\n        # fine columns AE_mean, AE_median, AE_lowq, AE_upq\n        # mask = SEAarray.columns.str.startswith(c) & ~SEAarray.columns.str.endswith(\"cnt\")\n        mask = [c + \"_mean\", c + \"_median\", c + \"_lowq\", c + \"_upq\"]\n        style = [\"r-\", \"b-\", \"b--\", \"b--\"]\n        \n        # plot the SEA data\n        SEAarray.loc[:, mask].plot(\n            ax=ax,\n            style=style,\n            xlabel=\"Normalized Time\",\n            ylabel=c.replace(\"_\", \" \"),\n            legend=False,\n        )\n\n    return fig, axes\n\ndef plot_ids_sea(SEAarray: pd.DataFrame, meta) -&gt; tuple[Figure, list[Axes]]:\n    import scienceplots\n    \n    SEAarray.index = SEAarray.index.map(lambda x: x / bin/2)\n\n    with plt.style.context(['science', 'nature', 'notebook']):\n\n        fig, axes = plot_SEA(SEAarray, meta)\n\n        axes[0].set_ylabel(r\"$B_x \\ / \\ B_0$\")\n        axes[1].set_ylabel(r\"$J_y$\")\n        axes[2].set_ylabel(r\"$J_y \\ / \\ J_A$\")\n        axes[3].set_ylabel(r\"$J_k$\")\n        axes[4].set_ylabel(r\"$J_k \\ / \\ J_A$\")\n        axes[5].set_ylabel(r\"$B_m \\ / \\ B_g$\")\n\n        fig.tight_layout()\n        fig.subplots_adjust(hspace=0)\n        \n        axes[0].legend(labels=[\"Mean\", \"Median\", \"LowQ\", \"UpQ\"])\n    return fig, axes\n    # fig.savefig(f\"../../../figures/sea/sea_juno_first_year_{freq}.png\", dpi = 300)\n\n\n::: {#cell-7 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=6}\n\nCode\nfrom utils.config import JunoConfig\n\n:::\n\n\nCode\n# config = JunoConfig(timerange=['2011-01-01','2012-01-01'], ts=1/8, split=16)\nconfig = JunoConfig(timerange=['2011-01-01','2011-09-01'], ts=1/8, split=2)\nts_f = config.ts.total_seconds()\n\n\n\n\nCode\nids_ds = config.get_and_process_data()\n\n\n\n\nCode\nbin = 32\n\nevents = ids_ds.events.with_columns(r_bin=pl.col(\"radial_distance\").round())\n\ndfs = config.mag_dfs\n\nSEAarrays = {}\n\nfor [r], r_events in events.group_by([\"r_bin\"]):\n    SEAarray, meta, p1data, p2data = sea_ids(dfs, events=r_events, bins=[bin, bin])\n    SEAarrays[r] = SEAarray\n    \ndef func():\n    for r, SEAarray in SEAarrays.items():\n        yield pl.from_dataframe(SEAarray).with_columns(r_bin=r)\n\nsea_df = pl.concat(func()).sort(\"r_bin\")\nsea_df.write_csv(\"../data/05_reporting/sea_juno_ts_{ts_f}.csv\")\n\n\n2024-02-25 01:00:43.955 | INFO     | __main__:sea_ids:28 - Compressing data by events\n100%|██████████| 2150/2150 [00:01&lt;00:00, 1959.71it/s]\n2024-02-25 01:01:14.763 | INFO     | __main__:sea_ids:28 - Compressing data by events\n100%|██████████| 2460/2460 [00:01&lt;00:00, 2114.96it/s]\n2024-02-25 01:01:47.126 | INFO     | __main__:sea_ids:28 - Compressing data by events\n\n\nThere is no data for event 11627\n\n\n100%|██████████| 20562/20562 [00:10&lt;00:00, 2037.43it/s]\n2024-02-25 01:03:24.733 | INFO     | __main__:sea_ids:28 - Compressing data by events\n100%|██████████| 9430/9430 [00:04&lt;00:00, 2118.91it/s]\n2024-02-25 01:04:19.989 | INFO     | __main__:sea_ids:28 - Compressing data by events\n100%|██████████| 1776/1776 [00:00&lt;00:00, 2127.65it/s]\n\n\n\n\nCode\nfor r, SEAarray in SEAarrays.items():\n    fig, axes = plot_ids_sea(SEAarray, meta)\n    fig.show()\n    fig.savefig(f\"../figures/sea/sea_juno_ts_{ts_f}_{r}.png\", dpi = 300)\n    \n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[89], line 2\n      1 for r, SEAarray in SEAarrays.items():\n----&gt; 2     fig, axes = plot_ids_sea(SEAarray, meta)\n      3     fig.show()\n      4     fig.savefig(f\"../figures/sea/sea_juno_ts_{ts_f}_{r}.png\", dpi = 300)\n\nCell In[67], line 45, in plot_ids_sea(SEAarray, meta)\n     43 axes[2].set_ylabel(r\"$J_y \\ / \\ J_A$\")\n     44 axes[3].set_ylabel(r\"$J_k$\")\n---&gt; 45 axes[4].set_ylabel(r\"$J_k \\ / \\ J_A$\")\n     46 axes[5].set_ylabel(r\"$B_m \\ / \\ B_g$\")\n     48 fig.tight_layout()\n\nIndexError: index 4 is out of bounds for axis 0 with size 4\n\n\n\n\n\n\n\n\n\n\n\nFitting\n\n\nCode\nimport mygrad as mg\nfrom lmfit import Model\n\n\n\n\nCode\ndef logistic(x, A, mu, sigma):\n    alpha = (x - mu) / sigma\n    return A * (1 - 1 / (1 + np.e**alpha))\n\n\ndef dfdx(x, A=1, mu=0, sigma=1):\n    x = mg.tensor(np.array(x))\n    f = logistic(x, A, mu, sigma)\n    f.backward()\n    return x.grad\n\ncomp_model = Model(dfdx, prefix=\"m1_\") + Model(dfdx, prefix=\"m2_\")\nparams = comp_model.make_params(\n    m1_mu = dict(value=0, vary=False),\n    m2_mu = dict(value=0, vary=False),\n    m1_A = dict(value=0.5, min=0),\n    m2_A = dict(value=0.2, min=0),\n    m1_sigma = dict(value=1, min=0),\n    m2_sigma = dict(value=2, min=0),\n)\n\nmodel = Model(dfdx)\nparams1 = model.make_params(\n    A = dict(value=0.5, min=0),\n    mu = dict(value=0, vary=False),\n    sigma = dict(value=1, min=0),\n)\n\n\n\n\nCode\nfrom lmfit.model import ModelResult\n\ndef plot_result(x, y, result: ModelResult, result1=None, ax: Axes = None):\n    # print(result.fit_report())\n    x_plot = np.linspace(x.min(), x.max(), 1000)\n    comps = result.eval_components(x=x_plot)\n    \n    if ax is None:\n        fig, ax = plt.subplots()\n\n    ax.plot(x, y, \"o\")\n    ax.plot(x_plot, result.eval(x=x_plot), label=\"best fit\")\n    ax.plot(x_plot, comps[\"m1_\"], \"--\", label=\"comp#1 fit\")\n    ax.plot(x_plot, comps[\"m2_\"], \"--\", label=\"comp#2 fit\")\n    if result1:\n        ax.plot(x, result1.best_fit, label=\"best fit with one component\")\n    return ax\n\n\n\n\nCode\nx = SEAarray.index.values[8:-8]/bin\nfit_cols = [\"j_m_median\", \"j_m_norm_median\"]\nfit_result: list[ModelResult] = []\nfit_result1: list[ModelResult] = []\n\nSEAarrays = dict(sorted(SEAarrays.items()))\n\nfig, axes = plt.subplots(len(SEAarrays), len(fit_cols), sharex=True, figsize=(12, 12))\n\nfor i, (r, SEAarray) in enumerate(SEAarrays.items()):\n    for j, col in enumerate(fit_cols):\n        y = SEAarray[col].values[8:-8]\n\n        result = comp_model.fit(y, params, x=x)\n        result_dict: dict = result.params.valuesdict()\n        result_dict.update({\"r\": r, \"col\": col})\n        fit_result.append(result_dict)\n\n        result1 = model.fit(y, params1, x=x)\n        result_dict1: dict = result1.params.valuesdict()\n        result_dict1.update({\"r\": r, \"col\": col})\n        fit_result1.append(result_dict1)\n        \n        ax :Axes = axes[i, j]\n        plot_result(x, y, result, result1, ax=ax)\n        \n        if i == 0:\n            ax.set_title(f\"{col}\")\n        if j == 0:\n            ax.set_ylabel(f\"r = {r:.0f}\")\n        \n        if i == len(SEAarrays) - 1:\n            ax.set_xlabel(\"Normalized Time\")\n        \naxes.flatten()[-1].legend()\n\nfit_result_df = pd.DataFrame(fit_result).sort_values('r')\nfit_result1_df = pd.DataFrame(fit_result1).sort_values('r')\n\n\n\n\n\n\n\n\nFigure 1: SEA analysis\n\n\n\n\n\n\n\nCode\nys = ['m1_A', 'm1_sigma', 'm2_A', 'm2_sigma']\nfig, axes = plt.subplots(nrows=len(ys), sharex=True)\naxes : list[Axes]\nfor param, ax in zip(ys, axes):\n    group_df = fit_result_df.groupby('col')\n    for name, group in group_df:\n        group.plot(x='r',y=param, ax=ax, label=name)\n    ax.set_ylabel(param)\n\nplt.legend()\n\n# fit_result_df['m2_sigma'].plot()\n\n\n\n\n\n\n\n\nFigure 2: Fit parameters over the radial distance\n\n\n\n\n\n\n\nCode\ndef dvl_dt(\n    data: pl.LazyFrame,\n    start,\n    end,\n):\n    event_data = data.filter(pl.col(\"time\") &gt;= start, pl.col(\"time\") &lt;= end).collect()\n\n    event_numpy = event_data.drop(\"time\").to_numpy()\n    time = event_data.get_column(\"time\").to_numpy()\n\n    vrot, _, _ = minvar(event_numpy)\n\n    vl = vrot[:, 0]\n    vl = vl * np.sign(vl[-1] - vl[0])\n    vl_ts = xr.DataArray(vl, dims=\"time\", coords={\"time\": time})\n\n    return vl_ts.differentiate(\"time\", datetime_unit=\"s\")\n\ndef ids_j_bin_l(\n    events: pl.DataFrame,\n    dfs: list[pl.LazyFrame],\n    start_col=\"t.d_start\",\n    end_col=\"t.d_end\",\n    offset=timedelta(seconds=1),\n):\n\n    if not isinstance(dfs, list):\n        dfs = [dfs]\n\n    events = keep_good_fit(events)\n\n    starts = events.get_column(start_col) - offset\n    ends = events.get_column(end_col) + offset\n\n    logger.info(\"Compressing data by events\")\n    data_c = compress_dfs_by_intervals(dfs, starts, ends).lazy()\n\n    logger.info(\"Calculating dvl_dt\")\n    return (\n        dvl_dt(data_c, start, end) for start, end in zip(starts, ends)\n    )\n\n\n\n\nCode\ndvl_dts = list(ids_j_bin_l(events, config.mag_dfs))\n\n\n2024-02-25 15:05:05.917 | INFO     | __main__:ids_j_bin_l:35 - Compressing data by events\n2024-02-25 15:05:05.982 | INFO     | __main__:ids_j_bin_l:38 - Calculating dvl_dt\n\n\n\n\nCode\ndef func():\n    epochs = events.get_column(\"t.d_time\").to_numpy()\n    v_ks = events.get_column(\"v_k\")\n    ion_inertial_lengths = events.get_column(\"ion_inertial_length\")\n    rs = events.get_column(\"radial_distance\")\n    \n    for i, dvl_dt in enumerate(dvl_dts):\n        shift_time = (dvl_dt.time - epochs[i]).values.astype(float)\n        l_norm = shift_time / 1.0e9 * v_ks[i] / ion_inertial_lengths[i]\n        # yield l_norm.values.astype(float)\n        yield rs[i], l_norm, dvl_dt.values\n\n\n\n\nCode\nrs, ls, dvl_dts = zip(*func())\nls_all = np.concatenate(ls)\ndvl_dts_all = np.concatenate(dvl_dts)\nplt.scatter(ls_all, dvl_dts_all)\n\n\n\n\n\n\n\n\n\n\n\nCode\ny = dvl_dts_all\nx = ls_all\nresult = comp_model.fit(y, params, x=x)\nresult1 = model.fit(y, params1, x=x)\n\nplot_result(x, y, result, result1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nevents = ids_ds.events.head(10).pipe(ids_j_bin_l, dfs=config.mag_dfs[0]).with_columns(r_bin=pl.col(\"radial_distance\").round())\n\n\n2024-02-25 14:23:30.016 | INFO     | __main__:ids_j_bin_l:44 - Compressing data by events\n2024-02-25 14:23:30.068 | INFO     | __main__:ids_j_bin_l:47 - Calculating dvl_dt\n\n\n\n\nCode\nevents.select(\n    pl.col(\"times\").list\n)\n\n\n\n\nshape: (6, 1)\n\n\n\ntimes\n\n\nlist[i64]\n\n\n\n\n[1314285913062500000, 1314285913187500000, … 1314285944187500000]\n\n\n[1314286368812500000, 1314286368937500000, … 1314286403312500000]\n\n\n[1314286385562500000, 1314286385687500000, … 1314286424312500000]\n\n\n[1314287645562500000, 1314287645687500000, … 1314287690437500000]\n\n\n[1314292230062500000, 1314292230187500000, … 1314292253812500000]\n\n\n[1314292230062500000, 1314292230187500000, … 1314292253812500000]\n\n\n\n\n\n\n\n\nSuperposed epoch analysis\n\n\nCode\ngroupby = 'index'\ncolumns = ['dBdt', 'j_k']\n\n# Plot each group\nfig, axes = plt.subplots(nrows=len(columns))\n\nif len(columns) == 1:\n    axes = [axes]\n\np1groups = p1data.groupby(groupby)\np2groups = p2data.groupby(groupby)\n\nfor ax, column in zip(axes, columns):\n    for name, group in p1groups:\n        ax.plot(group['t_norm'] -1 , group[column], color='grey', alpha=0.3)\n        \n    for name, group in p2groups:\n        ax.plot(group['t_norm'], group[column], color='grey', alpha=0.3)\n\n# plt.yscale('log')\nplt.show()\n\n\n\nCode\nids_ds.plot_candidates(num=20, plot_fit_data=True, predicates=(pl.col('fit.stat.rsquared') &gt; 0.95))",
    "crumbs": [
      "Home",
      "Notebooks",
      "Archive",
      "Superposed epoch analysis"
    ]
  },
  {
    "objectID": "notebooks/analysis/10_classification.html",
    "href": "notebooks/analysis/10_classification.html",
    "title": "ID classification",
    "section": "",
    "text": "Code\nimport numpy as np\nIn this method, TDs and RDs satisfy $ &lt; 0.2$ and $ | | &gt; 0.4$ B BN bg ∣∣ ∣∣ , &lt; D 0.2 B B bg ∣∣ ∣ ∣ , respectively. Moreover, IDs with &lt; 0.4 B BN bg ∣∣ ∣∣ , &lt; D 0.2 B B bg ∣∣ ∣ ∣ could be either TDs or RDs, and so are termed EDs. Similarly, NDs are defined as &gt; 0.4 B BN bg ∣∣ ∣∣ , &gt; D 0.2 B B bg ∣∣ ∣ ∣ because they can be neither TDs nor RDs. It is worth noting that EDs and NDs here are not physical concepts like RDs and TDs. RDs or TDs correspond to specific types of structures in the MHD framework, while EDs and NDs are introduced just to better quantify the statistical results.\nCriteria Used to Classify Discontinuities on the Basis of Magnetic Data Type\nCode\nBnOverB_RD_lower_threshold = 0.4\ndBOverB_RD_upper_threshold = 0.2\n\nBnOverB_TD_upper_threshold = 0.2\ndBOverB_TD_lower_threshold = dBOverB_RD_upper_threshold\n\nBnOverB_ED_upper_threshold = BnOverB_RD_lower_threshold\ndBOverB_ED_upper_threshold = dBOverB_TD_lower_threshold\n\nBnOverB_ND_lower_threshold = BnOverB_TD_upper_threshold\ndBOverB_ND_lower_threshold = dBOverB_RD_upper_threshold\nCode\ndef classify_id(BnOverB, dBOverB):\n    BnOverB = np.abs(np.asarray(BnOverB))\n    dBOverB = np.asarray(dBOverB)\n\n    s1 = (BnOverB &gt; BnOverB_RD_lower_threshold)\n    s2 = (dBOverB &gt; dBOverB_RD_upper_threshold)\n    s3 = (BnOverB &gt; BnOverB_TD_upper_threshold)\n    s4 = s2 # note: s4 = (dBOverB &gt; dBOverB_TD_lower_threshold)\n    \n    RD = s1 & ~s2\n    TD = ~s3 & s4\n    ED = ~s1 & ~s4\n    ND = s3 & s2\n\n    # Create an empty result array with the same shape\n    result = np.empty_like(BnOverB, dtype=object)\n\n    result[RD] = \"RD\"\n    result[TD] = \"TD\"\n    result[ED] = \"ED\"\n    result[ND] = \"ND\"\n\n    return result",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "ID classification"
    ]
  },
  {
    "objectID": "notebooks/analysis/10_classification.html#classification-of-candidates",
    "href": "notebooks/analysis/10_classification.html#classification-of-candidates",
    "title": "ID classification",
    "section": "Classification of candidates",
    "text": "Classification of candidates\n\n\nCode\nsns.jointplot(\n    candidates_jno_tau_60s,\n    x='dBOverB', y='BnOverB',\n    # kind='kde',\n    kind=\"hex\",\n)\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:2                                                                                    │\n│                                                                                                  │\n│   1 sns.jointplot(                                                                               │\n│ ❱ 2 │   candidates_jno_tau_60s,                                                                  │\n│   3 │   x='dBOverB', y='BnOverB',                                                                │\n│   4 │   # kind='kde',                                                                            │\n│   5 │   kind=\"hex\",                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'candidates_jno_tau_60s' is not defined\n\n\n\n\n\nCode\nsns.jointplot(\n    all_candidates,\n    x='dBOverB', y='BnOverB',\n    # kind='kde',\n    kind=\"hex\",\n)\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:2                                                                                    │\n│                                                                                                  │\n│   1 sns.jointplot(                                                                               │\n│ ❱ 2 │   all_candidates,                                                                          │\n│   3 │   x='dBOverB', y='BnOverB',                                                                │\n│   4 │   # kind='kde',                                                                            │\n│   5 │   kind=\"hex\",                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'all_candidates' is not defined\n\n\n\n\nDistribution of the DD types\n\nPlot distribution of types for each missions\n\n\nCode\nalt.Chart(all_candidates).encode(\n    x=alt.X(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),\n    y=alt.Y('sat').title(None),\n    color='type',\n).mark_bar()\n\n# alt.Chart(distributions).encode(\n#     alt.X('ratio:Q', title='DD type distribution').axis(format='.0%'),\n#     y=alt.Y('sat', title=None),\n#     color='type',\n# ).mark_bar()\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱  1 alt.Chart(all_candidates).encode(                                                           │\n│    2 │   x=alt.X(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),                       │\n│    3 │   y=alt.Y('sat').title(None),                                                             │\n│    4 │   color='type',                                                                           │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'alt' is not defined\n\n\n\n\n\nCode\ndistributions = all_candidates.group_by(\"sat\", \"type\").agg(pl.count()).with_columns(\n    (pl.col(\"count\") / pl.sum(\"count\").over(\"sat\")).alias(\"ratio\")\n)\ncount_table = distributions.to_pandas().pivot(index='sat', columns='type', values='count')[['RD', 'TD', 'ED', 'ND']]\nratio_table = distributions.to_pandas().pivot(index='sat', columns='type', values='ratio')[['RD', 'TD', 'ED', 'ND']]\n# display(distributions.to_pandas().pivot(index='sat', columns='type', values='ratio')[['RD', 'TD', 'ED', 'ND']].style.format(\"{:.0%}\"))\ndisplay(count_table, ratio_table.style.format(\"{:.0%}\"))\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 distributions = all_candidates.group_by(\"sat\", \"type\").agg(pl.count()).with_columns(         │\n│   2 │   (pl.col(\"count\") / pl.sum(\"count\").over(\"sat\")).alias(\"ratio\")                           │\n│   3 )                                                                                            │\n│   4 count_table = distributions.to_pandas().pivot(index='sat', columns='type', values='count     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'all_candidates' is not defined\n\n\n\n\n\nPlot distribution of types for each missions over time\n\n\nCode\nalt.Chart(all_candidates).mark_bar(binSpacing=0).encode(\n    x=\"yearmonth(time)\",\n    y=alt.Y(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),\n    row=alt.Row(\"sat\").title(None),\n    color=\"type\",\n).configure_axis(grid=False).properties(height=100)\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 alt.Chart(all_candidates).mark_bar(binSpacing=0).encode(                                     │\n│   2 │   x=\"yearmonth(time)\",                                                                     │\n│   3 │   y=alt.Y(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),                        │\n│   4 │   row=alt.Row(\"sat\").title(None),                                                          │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'alt' is not defined\n\n\n\n\n\nCode\n%%R\nplot_type_distribution &lt;- function(data, bin_width = 30) { \n    data$date_only &lt;- as.Date(data$time)\n    \n    p &lt;- ggplot(data, aes(date_only, fill = type)) +\n        geom_histogram(binwidth = bin_width, position = \"fill\") + \n        theme_pubr(base_size = 16)\n        \n    return(p)\n}\n\np1 &lt;- plot_type_distribution(jno_candidates)\np2 &lt;- plot_type_distribution(thb_candidates)\np3 &lt;- plot_type_distribution(sta_candidates)\n\np &lt;- ggarrange(\n    p1 + rremove(\"xlab\"),\n    p2 + rremove(\"xlab\"), p3, \n    nrow = 3, align = 'hv', \n    labels=list(\"JUNO\", \"ARTEMIS-B\", \"STEREO-A\"), hjust=0, vjust=0,\n    legend = 'right', common.legend = TRUE\n)\n# save_plot(filename = \"type_distribution\")\np\n\n\nUsageError: Cell magic `%%R` not found.",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "ID classification"
    ]
  },
  {
    "objectID": "notebooks/analysis/01_occurence_rate.html",
    "href": "notebooks/analysis/01_occurence_rate.html",
    "title": "Occurence Rate",
    "section": "",
    "text": "Code\nfrom fastcore.utils import patch\nfrom fastcore.test import *\n\nfrom discontinuitypy.utils.basic import load_catalog\nfrom discontinuitypy.utils.basic import filter_tranges_df\nfrom discontinuitypy.utils.analysis import filter_before_jupiter\nfrom discontinuitypy.utils.analysis import link_coord2dim\nfrom discontinuitypy.datasets import cIDsDataset\n\nfrom beforerr.basics import pmap\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\nfrom datetime import timedelta\n\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\n\nimport warnings\n\n# This will filter out all FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n%load_ext autoreload\n%autoreload 2\n\n\n04-Dec-23 09:02:03 INFO     04-Dec-23 09:02:03: cffi mode is CFFI_MODE.ANY                          situation.py:41\n\n\n\n                   INFO     04-Dec-23 09:02:03: R home found:                                      situation.py:218\n                            /Library/Frameworks/R.framework/Resources                                              \n\n\n\n                   INFO     04-Dec-23 09:02:03: R library path:                                    situation.py:161\n\n\n\n                   INFO     04-Dec-23 09:02:03: LD_LIBRARY_PATH:                                   situation.py:165\n\n\n\n                   INFO     04-Dec-23 09:02:03: Default options to initialize R: rpy2, --quiet,      embedded.py:20\n                            --no-save                                                                              \n\n\n\n                   INFO     04-Dec-23 09:02:03: R is already initialized. No need to initialize.    embedded.py:269\n\n\n\n\n\nCode\ncatalog = load_catalog()\n\n\n[12/04/23 09:02:04] WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro_datasets/polars/lazy_polars_dataset.py:14: KedroDeprecationWarning:               \n                             'AbstractVersionedDataSet' has been renamed to                                        \n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                               from kedro.io.core import (                                                         \n                                                                                                                   \n\n\n\nConnect python with R kernel\n\n\nCode\n%load_ext rpy2.ipython\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\n[12/04/23 09:02:05] WARNING  R[write to console]:                                                  callbacks.py:124\n                             Attaching package: ‘dplyr’                                                            \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:stats’:                                                                      \n                                                                                                                   \n                                 filter, lag                                                                       \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:base’:                                                                       \n                                                                                                                   \n                                 intersect, setdiff, setequal, union                                               \n                                                                                                                   \n                                                                                                                   \n\n\n\n\n\n\n\n\nCode\nSTA_ds = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\nJNO_ds = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nTHB_ds = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\nWind_ds = cIDsDataset(sat_id=\"Wind\", tau=60, ts=1, catalog=catalog)\n\n\n                    INFO     Loading data from 'events.STA_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'STA.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.JNO_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'JNO.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.THB_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'THB.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.Wind_ts_1s_tau_60s'                      data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'Wind.MAG.primary_data_ts_1s'                    data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\nARTEMIS missions needs additional care as they are not always in the solar wind.\n\n\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('THB.STATE.inter_data_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nTHB_sw_ds = cIDsDataset(\n    sat_id=\"THB\", tau=60, ts=1, catalog=catalog,\n    candidates = filter_tranges_df(THB_ds.candidates, (start, end)), \n    data = filter_tranges_df(THB_ds.data.collect(), (start, end)).lazy()\n)\n\nTHB_sw_ds = THB_ds.copy(\n    update=dict(\n        candidates = THB_ds.candidates.pipe(filter_tranges_df, (start, end)),\n        data = THB_ds.data.collect().pipe(filter_tranges_df, (start, end)).lazy()\n    )\n)\n\n\n                    INFO     Loading data from 'THB.STATE.inter_data_sw' (LazyPolarsDataset)... data_catalog.py:502\n\n\n\n\n\nCode\nall_ds = [JNO_ds, Wind_ds, STA_ds, THB_sw_ds]",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "notebooks/analysis/01_occurence_rate.html#setup",
    "href": "notebooks/analysis/01_occurence_rate.html#setup",
    "title": "Occurence Rate",
    "section": "",
    "text": "Code\nfrom fastcore.utils import patch\nfrom fastcore.test import *\n\nfrom discontinuitypy.utils.basic import load_catalog\nfrom discontinuitypy.utils.basic import filter_tranges_df\nfrom discontinuitypy.utils.analysis import filter_before_jupiter\nfrom discontinuitypy.utils.analysis import link_coord2dim\nfrom discontinuitypy.datasets import cIDsDataset\n\nfrom beforerr.basics import pmap\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\nfrom datetime import timedelta\n\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\n\nimport warnings\n\n# This will filter out all FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n%load_ext autoreload\n%autoreload 2\n\n\n04-Dec-23 09:02:03 INFO     04-Dec-23 09:02:03: cffi mode is CFFI_MODE.ANY                          situation.py:41\n\n\n\n                   INFO     04-Dec-23 09:02:03: R home found:                                      situation.py:218\n                            /Library/Frameworks/R.framework/Resources                                              \n\n\n\n                   INFO     04-Dec-23 09:02:03: R library path:                                    situation.py:161\n\n\n\n                   INFO     04-Dec-23 09:02:03: LD_LIBRARY_PATH:                                   situation.py:165\n\n\n\n                   INFO     04-Dec-23 09:02:03: Default options to initialize R: rpy2, --quiet,      embedded.py:20\n                            --no-save                                                                              \n\n\n\n                   INFO     04-Dec-23 09:02:03: R is already initialized. No need to initialize.    embedded.py:269\n\n\n\n\n\nCode\ncatalog = load_catalog()\n\n\n[12/04/23 09:02:04] WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro_datasets/polars/lazy_polars_dataset.py:14: KedroDeprecationWarning:               \n                             'AbstractVersionedDataSet' has been renamed to                                        \n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                               from kedro.io.core import (                                                         \n                                                                                                                   \n\n\n\nConnect python with R kernel\n\n\nCode\n%load_ext rpy2.ipython\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\n[12/04/23 09:02:05] WARNING  R[write to console]:                                                  callbacks.py:124\n                             Attaching package: ‘dplyr’                                                            \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:stats’:                                                                      \n                                                                                                                   \n                                 filter, lag                                                                       \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:base’:                                                                       \n                                                                                                                   \n                                 intersect, setdiff, setequal, union                                               \n                                                                                                                   \n                                                                                                                   \n\n\n\n\n\n\n\n\nCode\nSTA_ds = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\nJNO_ds = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nTHB_ds = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\nWind_ds = cIDsDataset(sat_id=\"Wind\", tau=60, ts=1, catalog=catalog)\n\n\n                    INFO     Loading data from 'events.STA_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'STA.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.JNO_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'JNO.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.THB_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'THB.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.Wind_ts_1s_tau_60s'                      data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'Wind.MAG.primary_data_ts_1s'                    data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\nARTEMIS missions needs additional care as they are not always in the solar wind.\n\n\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('THB.STATE.inter_data_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nTHB_sw_ds = cIDsDataset(\n    sat_id=\"THB\", tau=60, ts=1, catalog=catalog,\n    candidates = filter_tranges_df(THB_ds.candidates, (start, end)), \n    data = filter_tranges_df(THB_ds.data.collect(), (start, end)).lazy()\n)\n\nTHB_sw_ds = THB_ds.copy(\n    update=dict(\n        candidates = THB_ds.candidates.pipe(filter_tranges_df, (start, end)),\n        data = THB_ds.data.collect().pipe(filter_tranges_df, (start, end)).lazy()\n    )\n)\n\n\n                    INFO     Loading data from 'THB.STATE.inter_data_sw' (LazyPolarsDataset)... data_catalog.py:502\n\n\n\n\n\nCode\nall_ds = [JNO_ds, Wind_ds, STA_ds, THB_sw_ds]",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "notebooks/analysis/01_occurence_rate.html#distance-and-occurrence-rates-versus-time-for-juno",
    "href": "notebooks/analysis/01_occurence_rate.html#distance-and-occurrence-rates-versus-time-for-juno",
    "title": "Occurence Rate",
    "section": "Distance and Occurrence rates versus time for JUNO",
    "text": "Distance and Occurrence rates versus time for JUNO\n\n\nCode\ndef calc_or_df(events: pl.DataFrame, avg_window: timedelta, by=None):\n    \"\"\"Calculate the occurence rate of the candidates with the average window.\n\n    Notes: occurence rate is defined as the number of candidates per day.\n    \"\"\"\n\n    every = avg_window\n    or_factor = every / timedelta(days=1)\n\n    return (\n        events.sort(\"time\")\n        .group_by_dynamic(\"time\", every=every, by=by)\n        .agg(\n            cs.float().mean(),\n            o_rates=pl.count() / or_factor\n        )\n        .upsample(\"time\", every=every)  # upsample to fill the missing time\n    )\n\n\n\n\nCode\ndf = JNO_ds.candidates.pipe(calc_or_df, timedelta(days=5))\n%R -i df -c conv_pl\n\n\n\n\nCode\n%%R\np1 &lt;- ggplot(df, aes(x = time, y = radial_distance)) + \n  geom_line() + # Plot distance by date\n  labs(x = \"Date\", y = \"Distance (AU)\") +\n  theme_pubr(base_size = 16) + \n  theme(aspect.ratio=0.25)\n  \np2 &lt;- ggplot(df, aes(x = time, y = o_rates)) + \n  geom_line() + # Plot distance by date\n  labs(x = \"Date\", y = \"Occurrence Rates (#/day)\") +\n  theme_pubr(base_size = 16) + \n  theme(aspect.ratio=0.25)\n\np &lt;- ggarrange(p1, p2, nrow = 2)\n\n# save_plot(\"distance_and_or\")\np",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "notebooks/analysis/01_occurence_rate.html#occurrence-rates-versus-time-for-all-missions",
    "href": "notebooks/analysis/01_occurence_rate.html#occurrence-rates-versus-time-for-all-missions",
    "title": "Occurence Rate",
    "section": "Occurrence rates versus time for all missions",
    "text": "Occurrence rates versus time for all missions\n\nData normalization\nDifferent levels of normalization are applied to the data. The normalization is done in the following order:\n\nN1: normalize the data by the the effective time of every duration due to the data gap as we may miss some potential IDs. We assume the data gap is independent of the magnetic discontinuities.\nN2: normalize the data by the mean value of data near 1 AU. This is to remove the effect of the temporal variation of the solar wind.\n\n\n\nCode\ndef calc_n1_factor(\n    data: pl.LazyFrame,\n    s_resolution: timedelta,\n    l_resolution: timedelta,\n):\n    return (\n        data.sort(\"time\")\n        .group_by_dynamic(\"time\", every=s_resolution)\n        .agg(pl.lit(1).alias(\"availablity\"))\n        .group_by_dynamic(\"time\", every=l_resolution)\n        .agg(n1_factor=pl.sum(\"availablity\") * s_resolution / l_resolution)\n    )\n\n\ndef n1_normalize(\n    df: pl.DataFrame,  # the dataframe with count to be normalized\n    data: pl.LazyFrame,  # the data used to calculate the duration ratio\n    s_resolution,  # the smallest resolution to check if the data is available\n    avg_window,\n):\n    duration_df = calc_n1_factor(data, s_resolution, avg_window).with_columns(\n        cs.datetime().dt.cast_time_unit(\"ns\"),\n    )\n\n    return df.lazy().join(duration_df, how=\"left\", on=\"time\").with_columns(\n        o_rates_normalized=pl.col(\"o_rates\") / pl.col(\"n1_factor\")\n    ).collect()\n    \n@patch\ndef calc_or_normalized(\n    self: cIDsDataset, s_resolution: timedelta, avg_window: timedelta\n):\n    count_df = calc_or_df(self.candidates, avg_window)\n    return n1_normalize(count_df, self.data, s_resolution, avg_window)\n\n\n\n\nCode\ndef n2_normalize(df: pl.DataFrame, avg_sats = [\"STA\", \"THB\", \"Wind\"]):\n    avg_df = (\n        df.filter(pl.col(\"sat\").is_in(avg_sats))\n        .group_by(\"time\")\n        .agg(n2_factor=pl.mean(\"o_rates_normalized\"))\n    )\n    return df.join(avg_df, on=\"time\").with_columns(\n        o_rates_normalized=pl.col(\"o_rates_normalized\") / pl.col(\"n2_factor\")\n    )\n\n\n\n\nCode\ns_resolution = timedelta(minutes=1)\navg_window = timedelta(days=30)\n\n\n\n\nCode\nall_events_or_N1: pl.DataFrame = pl.concat(\n    all_ds\n    | pmap(\n        lambda x: x.calc_or_normalized(s_resolution, avg_window).with_columns(\n            sat=pl.lit(x.sat_id)\n        )\n    ),\n    how=\"diagonal\",\n)\n\n\n\n\nCode\nall_events_or_N2 = all_events_or_N1.pipe(n2_normalize)\n\n\n\n\nCode\n%%R\nplot_or_time &lt;- function(df) {\n  p &lt;- ggline(\n    df, x = \"time\", y = \"o_rates_normalized\", \n    color = \"sat\", linetype = \"sat\") \n  \n  p +   \n    labs(x = \"Date\", y = \"Occurrence Rates (#/day)\", color=\"Satellites\", linetype=\"Satellites\") + \n    theme_pubr(base_size = 16) + \n    theme(legend.text = element_text(size=16)) +\n    scale_color_okabeito(palette = \"black_first\")\n}\n\n\n\n\nCode\n%%R -i all_events_or_N1 -c conv_pl\np &lt;- plot_or_time(all_events_or_N1)\np\n\n\n[12/04/23 09:02:37] WARNING  R[write to console]: In addition:                                     callbacks.py:124\n\n\n\n                    WARNING  R[write to console]: Warning message:                                 callbacks.py:124\n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: Removed 6 rows containing missing values         callbacks.py:124\n                             (`geom_point()`).                                                                     \n                                                                                                                   \n\n\n\n\n\n\n\n\n\n\n\n\n\nWe noticed some anomalies in the occurrence rates of the magnetic discontinuities for Stereo-A data. Also for Juno, its occurrence rate is much higher when approaching Jupiter.\n\n\nCode\nall_events_or_N1.filter(\n    pl.col('time').is_in(pd.date_range('2014-01-01', '2015-01-01')),\n    sat='STA'\n)[[\"time\", \"o_rates\", \"o_rates_normalized\", 'n1_factor']]\n\n\n\n\n\n\n\nshape: (12, 4)\n\n\n\ntime\no_rates\no_rates_normalized\nn1_factor\n\n\ndatetime[ns]\nf64\nf64\nf64\n\n\n\n\n2014-01-10 00:00:00\n64.466667\n67.349278\n0.957199\n\n\n2014-02-09 00:00:00\n71.2\n71.232978\n0.999537\n\n\n2014-03-11 00:00:00\n71.666667\n74.137931\n0.966667\n\n\n2014-04-10 00:00:00\n72.2\n72.210029\n0.999861\n\n\n2014-05-10 00:00:00\n74.166667\n74.182121\n0.999792\n\n\n2014-06-09 00:00:00\n65.733333\n71.530265\n0.918958\n\n\n2014-07-09 00:00:00\n68.233333\n76.778496\n0.888704\n\n\n2014-08-08 00:00:00\n4.033333\n9.013968\n0.447454\n\n\n2014-09-07 00:00:00\n11.0\n69.53468\n0.158194\n\n\n2014-10-07 00:00:00\n6.6\n77.163735\n0.085532\n\n\n2014-11-06 00:00:00\n9.866667\n66.548009\n0.148264\n\n\n2014-12-06 00:00:00\n9.766667\n63.686038\n0.153356\n\n\n\n\n\n\n\nSurprisingly, we found out that the anomaly of STEREO-A data is not mainly due to data gap. We can inspect this data further. See appendix.\nWe remove the intervals which do not have enough data points to calculate the normalized occurrence rates accurately and restrict the time range to exclude Jupiter’s effect.\n\n\nCode\nall_events_or_N1_cleaned = (\n    all_events_or_N1.sort(\"time\")\n    .pipe(filter_before_jupiter)\n    .filter(pl.col(\"o_rates\") &gt; 5)\n    .upsample(\"time\", every=avg_window, by=\"sat\", maintain_order=True)\n    .with_columns(pl.col(\"sat\").forward_fill())\n)\n\nall_events_or_N2_cleaned = n2_normalize(all_events_or_N1_cleaned)\n\n\n\n\nCode\n%%R -i all_events_or_N1_cleaned -c conv_pl\np &lt;- plot_or_time(all_events_or_N1_cleaned)\n# save_plot(\"ocr/ocr_time_cleaned\")\np\n\n\n[12/04/23 09:07:11] WARNING  R[write to console]: In addition:                                     callbacks.py:124\n\n\n\n                    WARNING  R[write to console]: Warning message:                                 callbacks.py:124\n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: Removed 9 rows containing missing values         callbacks.py:124\n                             (`geom_point()`).                                                                     \n                                                                                                                   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R -i all_events_or_N2_cleaned -c conv_pl\np &lt;- plot_or_time(all_events_or_N2_cleaned)\np &lt;- p + labs(y = \"Normalized Occurrence Rates\")\nsave_plot(\"ocr/ocr_time_N2_cleaned\")\np\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n[12/04/23 09:16:01] WARNING  R[write to console]: In addition:                                     callbacks.py:124\n\n\n\n                    WARNING  R[write to console]: Warning message:                                 callbacks.py:124\n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: Removed 9 rows containing missing values         callbacks.py:124\n                             (`geom_point()`).                                                                     \n                                                                                                                   \n\n\n\nIn addition: Warning messages:\n1: Removed 9 rows containing missing values (`geom_point()`). \n2: Removed 9 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot the occurrence rates with radial distance\n\n\nCode\n%%R\nplot_or_r &lt;- function(df, target_sat = \"JNO\") {\n  \"plot normalized occurrence rate over radial distance\"\n  \n  # Filter data for target_sat\n  df_target &lt;- df[df$sat == target_sat,]\n  \n  # Compute the linear model\n  fit &lt;- lm(o_rates_normalized ~ I(1/ref_radial_distance), data = df_target)\n  \n  # Extract coefficients\n  intercept &lt;- coef(fit)[1]\n  slope &lt;- coef(fit)[2]\n  \n  # Format equation\n  equation &lt;- sprintf(\"y ~ %.2f / x\", slope)\n  \n  p &lt;- ggscatter(df, x = \"ref_radial_distance\", y = \"o_rates_normalized\", color = \"sat\") +\n    geom_smooth(data = df_target, formula = y ~ I(1/x), method = \"lm\", color=\"gray\", linetype=\"dashed\")\n    \n  p &lt;- p +\n    labs(x = \"Referred Radial Distance (AU)\", y = \"Occurrence Rate  (#/day)\", color=\"Satellites\") +\n    annotate(\"text\", label = equation, x = Inf, y = Inf, hjust = 1.1, vjust = 1.5, size = 7) +\n    theme_pubr(base_size = 16) + \n    theme(legend.text = element_text(size=16)) +\n    scale_color_okabeito(palette = \"black_first\")\n  \n  return(p)\n}\n\n\n\n\nCode\ndf = link_coord2dim(all_events_or_N1_cleaned).sort(\"ref_radial_distance\")\n\n\n\n\nCode\n%%R -i df -c conv_pl\np &lt;- plot_or_r(df)\nprint(p)\n# save_plot(\"ocr_r_cleaned\")\n\n\nIn addition: Warning messages:\n1: Removed 4 rows containing non-finite values (`stat_smooth()`). \n2: Removed 21 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf = link_coord2dim(all_events_or_N2_cleaned).sort(\"ref_radial_distance\")\n\n\n\n\nCode\n%%R -i df -c conv_pl\n#| code-summary: R Code\np &lt;- plot_or_r(df) + labs(y = \"Normalized Occurrence Rates\")\nprint(p)\n# save_plot(\"ocr/ocr_r_N2_cleaned\")\n\n\nIn addition: Warning messages:\n1: Removed 4 rows containing non-finite values (`stat_smooth()`). \n2: Removed 21 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "notebooks/analysis/01_occurence_rate.html#appendix",
    "href": "notebooks/analysis/01_occurence_rate.html#appendix",
    "title": "Occurence Rate",
    "section": "Appendix",
    "text": "Appendix\n\nObsolete codes\nNotes: seaborn.lineplot drops nans from the DataFrame before plotting, this is not desired…\n\n\nCode\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport scienceplots\nfrom discontinuitypy.utils.plot import savefig\n\n\n\n\nCode\nplt.style.use(['science', 'nature', 'notebook'])\n\n\n\n\nCode\n\ndef plot_or_r(df: pl.DataFrame):\n    \"plot normalized occurence rate over radial distance\"\n\n    sns.lineplot(x=\"ref_radial_distance\", y=\"o_rates_normalized\", hue=\"sat\", data=df)\n\n    ax = plt.gca()  # Get current axis\n    ax.set_yscale(\"log\")\n    ax.set_xlabel(\"Referred Radial Distance (AU)\")\n    ax.set_ylabel(\"Normalized Occurrence Rate\")\n    # savefig('occurrence_rate_ratio')\n\n    return ax.figure\n\n\n\n\nCode\ndef plot_or_time(df: pl.DataFrame):\n    \"\"\"Plot the occurence rate of the candidates with time.    \n    \"\"\"\n    # Create a unique list of all satellites and sort them to let JNO' be plotted first\n    all_sats = df[\"sat\"].unique().to_list()\n    all_sats.sort(key=lambda x: x != \"JNO\")\n\n    # Plot each satellite separately\n    for sat in all_sats:\n        sat_df = df.filter(sat=sat)\n        if sat == \"JNO\":\n            sns.lineplot(sat_df, x=\"time\", y=\"o_rates_normalized\", label=sat)\n        else:\n            # Making the other satellites more distinct with linestyle and alpha\n            sns.lineplot(\n                sat_df,\n                x=\"time\",\n                y=\"o_rates_normalized\",\n                linestyle=\"--\",  # dashed line style\n                alpha=0.5,  # keep the order of the legend\n                label=sat,\n            )\n\n    ax = plt.gca()  # Get current axis\n    # Set the y-axis and x-axis labels\n    ax.set_ylabel(\"Occurrence Rates (#/day)\")\n    ax.set_xlabel(\"Date\")\n    ax.legend(title=\"Satellites\")\n\n    # savefig(\"occurrence_rates\")\n    return ax.figure\n\n\n\n\nCode\nfig = plot_or_time(\n    all_events_or_N1\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTEREO-A anomaly during 2014-08 period\n\n\nCode\nmag: pl.DataFrame = catalog.load('sta.inter_mag_1s').filter(pl.col('B')&gt;0).collect()\n\n\n\n\nCode\nimport plotly.graph_objects as go;\nfrom plotly_resampler import FigureResampler\nimport plotly.express as px\n\n\n\n\nCode\n# px.line(mag, x='time', y='B') # This is extremely slow for large datasets\n\nfig = FigureResampler(go.Figure())\nfig.add_trace({\"x\": mag[\"time\"], \"y\": mag['B']})\nfig",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "notebooks/analysis/12_tau_data.html",
    "href": "notebooks/analysis/12_tau_data.html",
    "title": "Justification for the tau (data)",
    "section": "",
    "text": "::: {#cell-1 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=3}\n\nCode\nfrom utils.config import JunoConfig\n\n:::\n\n\nCode\nJunoConfig(tau=180).get_and_process_data().export()\nJunoConfig(tau=20).get_and_process_data().export()\n\n\n28-Feb-24 19:54:43: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n\n\n\n(_deploy_ray_func pid=36713) RuntimeWarning: overflow encountered in exp\n(_deploy_ray_func pid=36720) RuntimeWarning: overflow encountered in exp [repeated 32x across cluster]\n(_deploy_ray_func pid=36718) RuntimeWarning: overflow encountered in exp [repeated 21x across cluster]\n(_deploy_ray_func pid=36717) RuntimeWarning: overflow encountered in exp [repeated 17x across cluster]\n(_deploy_ray_func pid=36712) RuntimeWarning: overflow encountered in exp [repeated 22x across cluster]\n(_deploy_ray_func pid=36712) RuntimeWarning: overflow encountered in exp [repeated 5x across cluster]\n(_deploy_ray_func pid=36716) RuntimeWarning: overflow encountered in exp [repeated 8x across cluster]\n(_deploy_ray_func pid=36716) RuntimeWarning: overflow encountered in exp [repeated 3x across cluster]\n28-Feb-24 19:54:52: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n28-Feb-24 19:55:04: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n28-Feb-24 19:55:14: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n28-Feb-24 19:55:24: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n28-Feb-24 19:55:31: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n28-Feb-24 19:55:37: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n28-Feb-24 19:55:42: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n2024-02-28 19:55:44.719 | INFO     | discontinuitypy.datasets:write:40 - Dataframe written to ../data/05_reporting/events.JNO.fit.ts_1.00s_tau_180s.arrow\n\n\nJunoConfig(name='JNO', data=&lt;LazyFrame [4 cols, {\"BX SE\": Float64 … \"time\": Datetime(time_unit='us', time_zone=None)}] at 0x2A3CBBBD0&gt;, ts=datetime.timedelta(seconds=1), tau=datetime.timedelta(seconds=180), events=shape: (42_984, 94)\n┌─────────────┬────────────┬─────┬──────────┬───┬────────────┬───────────┬────────────┬────────────┐\n│ time        ┆ index_diff ┆ len ┆ std      ┆ … ┆ v.ion.chan ┆ B.change  ┆ v.Alfven.c ┆ v.Alfven.c │\n│ ---         ┆ ---        ┆ --- ┆ ---      ┆   ┆ ge.l       ┆ ---       ┆ hange      ┆ hange.l    │\n│ datetime[ns ┆ f64        ┆ u32 ┆ f64      ┆   ┆ ---        ┆ f64       ┆ ---        ┆ ---        │\n│ ]           ┆            ┆     ┆          ┆   ┆ f64        ┆           ┆ f64        ┆ f64        │\n╞═════════════╪════════════╪═════╪══════════╪═══╪════════════╪═══════════╪════════════╪════════════╡\n│ 2011-08-25  ┆ 1.457943   ┆ 180 ┆ 2.655685 ┆ … ┆ 0.0        ┆ 0.030584  ┆ 0.394842   ┆ 81.897526  │\n│ 15:33:00    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆            │\n│ 2011-08-25  ┆ 0.37205    ┆ 180 ┆ 0.937567 ┆ … ┆ -0.032502  ┆ 0.242502  ┆ 3.52999    ┆ -50.821981 │\n│ 17:49:30    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆            │\n│ 2011-08-25  ┆ 0.802401   ┆ 180 ┆ 1.548479 ┆ … ┆ -0.012796  ┆ 0.024578  ┆ 0.411154   ┆ -69.345459 │\n│ 17:51:00    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆            │\n│ 2011-08-25  ┆ 1.47258    ┆ 180 ┆ 2.647456 ┆ … ┆ 0.013626   ┆ -0.271046 ┆ -3.9196    ┆ 90.960015  │\n│ 18:36:00    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆            │\n│ 2011-08-25  ┆ 1.005952   ┆ 180 ┆ 1.614257 ┆ … ┆ -0.009481  ┆ 0.087428  ┆ 1.359867   ┆ 79.308791  │\n│ 20:39:00    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆            │\n│ …           ┆ …          ┆ …   ┆ …        ┆ … ┆ …          ┆ …         ┆ …          ┆ …          │\n│ 2016-06-29  ┆ 1.286955   ┆ 180 ┆ 3.264029 ┆ … ┆ NaN        ┆ 0.939178  ┆ NaN        ┆ NaN        │\n│ 17:37:30    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆            │\n│ 2016-06-29  ┆ 0.858761   ┆ 180 ┆ 2.697777 ┆ … ┆ NaN        ┆ 0.663428  ┆ NaN        ┆ NaN        │\n│ 17:49:30    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆            │\n│ 2016-06-29  ┆ 0.864486   ┆ 180 ┆ 2.724823 ┆ … ┆ -0.011618  ┆ 0.872487  ┆ 111.977492 ┆ 1179.18351 │\n│ 17:51:00    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆ 2          │\n│ 2016-06-29  ┆ 0.891457   ┆ 180 ┆ 3.344919 ┆ … ┆ -0.023165  ┆ 0.083389  ┆ 10.257005  ┆ -1231.4827 │\n│ 23:37:30    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆ 57         │\n│ 2016-06-29  ┆ 0.933747   ┆ 180 ┆ 2.632613 ┆ … ┆ -0.030708  ┆ 2.284448  ┆ 283.917387 ┆ -979.80710 │\n│ 23:39:00    ┆            ┆     ┆          ┆   ┆            ┆           ┆            ┆ 6          │\n└─────────────┴────────────┴─────┴──────────┴───┴────────────┴───────────┴────────────┴────────────┘, method='fit', mag_meta=Meta(dataset=None, parameters=None), bcols=None, plasma_data=&lt;LazyFrame [14 cols, {\"radial_distance\": Float64 … \"B_background_z\": Float64}] at 0x174C37B50&gt;, plasma_meta=PlasmaMeta(dataset=None, parameters=None, density_col='plasma_density', velocity_cols=['v_x', 'v_y', 'v_z'], speed_col=None, temperature_col=None), ion_temp_data=None, ion_temp_meta=TempMeta(dataset=None, parameters=None, para_col=None, perp_cols=None), e_temp_data=None, e_temp_meta=TempMeta(dataset=None, parameters=None, para_col=None, perp_cols=None), timerange=[datetime.datetime(2011, 8, 25, 0, 0, tzinfo=TzInfo(UTC)), datetime.datetime(2016, 6, 30, 0, 0, tzinfo=TzInfo(UTC))], split=8, fmt='arrow')",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Justification for the tau (data)"
    ]
  },
  {
    "objectID": "notebooks/analysis/02_orientation.html",
    "href": "notebooks/analysis/02_orientation.html",
    "title": "Orientation",
    "section": "",
    "text": "::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=9}\n\nCode\nimport polars as pl\nfrom discontinuitypy.utils.basic import load_catalog\n\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\nfrom discontinuitypy.pipelines.project.pipeline import process_events_l2\nfrom discontinuitypy.utils.polars import pl_norm\n\nimport hvplot.polars\nimport panel as pn\nimport warnings\n\n# Suppress specific FutureWarning from pandas in Holoviews\nwarnings.filterwarnings(\n    \"ignore\", category=FutureWarning, module=\"holoviews.core.data.pandas\"\n)\n\n:::\n::: {#cell-3 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=2}\n\nCode\n%load_ext autoreload\n%autoreload 2\n%load_ext rpy2.ipython\n\ncatalog = load_catalog()\n\nr = robjects.r\nr.source('utils.R')\nconv_pl = py2rpy_polars()\n\n\n[11/30/23 13:28:07] WARNING  KedroDeprecationWarning: 'AbstractVersionedDataSet' has been renamed to  logger.py:205\n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]:                                                  callbacks.py:124\n                             Attaching package: ‘dplyr’                                                            \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:stats’:                                                                      \n                                                                                                                   \n                                 filter, lag                                                                       \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:base’:                                                                       \n                                                                                                                   \n                                 intersect, setdiff, setequal, union                                               \n                                                                                                                   \n                                                                                                                   \n\n\n:::\n\n\nCode\nall_events_l1: pl.DataFrame = catalog.load(\"events.l1.ALL_sw_ts_1s_tau_60s\").collect()\n\ncols = [\"dB_x_norm\", \"dB_y_norm\", \"dB_z_norm\"]\nall_events_l1 = (\n    all_events_l1.with_columns(dB=pl_norm(cols))\n    .with_columns(\n        dB_x_n2=pl.col(\"dB_x_norm\") / pl.col(\"dB\"),\n        dB_y_n2=pl.col(\"dB_y_norm\") / pl.col(\"dB\"),\n        dB_z_n2=pl.col(\"dB_z_norm\") / pl.col(\"dB\"),\n    )\n    .with_columns(\n        theta_dB=pl.col(\"dB_z_n2\").arccos().degrees(),\n        phi_dB=(pl.col(\"dB_y_n2\") / pl.col(\"dB_x_n2\")).arctan().degrees(),\n    )\n)\n\nJNO_events_l1 = all_events_l1.filter(pl.col(\"sat\") == \"JNO\")\nother_events_l1 = all_events_l1.filter(pl.col(\"sat\") != \"JNO\")\nall_events_l2 = all_events_l1.pipe(process_events_l2)\n\n\n[11/30/23 13:28:08] INFO     Loading data from 'events.l1.ALL_sw_ts_1s_tau_60s'                 data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n\n\nCode\n%R -i JNO_events_l1 -c conv_pl\n%R -i all_events_l1 -c conv_pl\n%R -i other_events_l1 -c conv_pl\n%R -i all_events_l2 -c conv_pl\n\n\n\n\nCode\n%%R\nprobs &lt;- c(0.99, 0.9, 0.7, 0.5, 0.3, 0.1)\n\nplot_plot &lt;- function(df, x, y, type = \"hdr\", facets = NULL, color = NULL) {\n  p &lt;- ggplot(df, aes(x = .data[[x]], y = .data[[y]]) ) +\n    scale_color_okabeito(palette = \"black_first\")\n\n\n  if (!is.null(facets)) {\n    p &lt;- p + facet_wrap(vars(.data[[facets]]))\n  }\n\n  if (type == \"hdr\") {\n    p &lt;- p + geom_hdr_lines(\n      aes_string(color = color),\n      probs = probs\n    )\n  } else if (type == \"density\") {\n    p &lt;- p + geom_density_2d_filled()\n  }\n\n  return (p)\n}",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Orientation"
    ]
  },
  {
    "objectID": "notebooks/analysis/02_orientation.html#setup",
    "href": "notebooks/analysis/02_orientation.html#setup",
    "title": "Orientation",
    "section": "",
    "text": "::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=9}\n\nCode\nimport polars as pl\nfrom discontinuitypy.utils.basic import load_catalog\n\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\nfrom discontinuitypy.pipelines.project.pipeline import process_events_l2\nfrom discontinuitypy.utils.polars import pl_norm\n\nimport hvplot.polars\nimport panel as pn\nimport warnings\n\n# Suppress specific FutureWarning from pandas in Holoviews\nwarnings.filterwarnings(\n    \"ignore\", category=FutureWarning, module=\"holoviews.core.data.pandas\"\n)\n\n:::\n::: {#cell-3 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=2}\n\nCode\n%load_ext autoreload\n%autoreload 2\n%load_ext rpy2.ipython\n\ncatalog = load_catalog()\n\nr = robjects.r\nr.source('utils.R')\nconv_pl = py2rpy_polars()\n\n\n[11/30/23 13:28:07] WARNING  KedroDeprecationWarning: 'AbstractVersionedDataSet' has been renamed to  logger.py:205\n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]:                                                  callbacks.py:124\n                             Attaching package: ‘dplyr’                                                            \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:stats’:                                                                      \n                                                                                                                   \n                                 filter, lag                                                                       \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:base’:                                                                       \n                                                                                                                   \n                                 intersect, setdiff, setequal, union                                               \n                                                                                                                   \n                                                                                                                   \n\n\n:::\n\n\nCode\nall_events_l1: pl.DataFrame = catalog.load(\"events.l1.ALL_sw_ts_1s_tau_60s\").collect()\n\ncols = [\"dB_x_norm\", \"dB_y_norm\", \"dB_z_norm\"]\nall_events_l1 = (\n    all_events_l1.with_columns(dB=pl_norm(cols))\n    .with_columns(\n        dB_x_n2=pl.col(\"dB_x_norm\") / pl.col(\"dB\"),\n        dB_y_n2=pl.col(\"dB_y_norm\") / pl.col(\"dB\"),\n        dB_z_n2=pl.col(\"dB_z_norm\") / pl.col(\"dB\"),\n    )\n    .with_columns(\n        theta_dB=pl.col(\"dB_z_n2\").arccos().degrees(),\n        phi_dB=(pl.col(\"dB_y_n2\") / pl.col(\"dB_x_n2\")).arctan().degrees(),\n    )\n)\n\nJNO_events_l1 = all_events_l1.filter(pl.col(\"sat\") == \"JNO\")\nother_events_l1 = all_events_l1.filter(pl.col(\"sat\") != \"JNO\")\nall_events_l2 = all_events_l1.pipe(process_events_l2)\n\n\n[11/30/23 13:28:08] INFO     Loading data from 'events.l1.ALL_sw_ts_1s_tau_60s'                 data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n\n\nCode\n%R -i JNO_events_l1 -c conv_pl\n%R -i all_events_l1 -c conv_pl\n%R -i other_events_l1 -c conv_pl\n%R -i all_events_l2 -c conv_pl\n\n\n\n\nCode\n%%R\nprobs &lt;- c(0.99, 0.9, 0.7, 0.5, 0.3, 0.1)\n\nplot_plot &lt;- function(df, x, y, type = \"hdr\", facets = NULL, color = NULL) {\n  p &lt;- ggplot(df, aes(x = .data[[x]], y = .data[[y]]) ) +\n    scale_color_okabeito(palette = \"black_first\")\n\n\n  if (!is.null(facets)) {\n    p &lt;- p + facet_wrap(vars(.data[[facets]]))\n  }\n\n  if (type == \"hdr\") {\n    p &lt;- p + geom_hdr_lines(\n      aes_string(color = color),\n      probs = probs\n    )\n  } else if (type == \"density\") {\n    p &lt;- p + geom_density_2d_filled()\n  }\n\n  return (p)\n}",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Orientation"
    ]
  },
  {
    "objectID": "notebooks/analysis/02_orientation.html#theta_nb",
    "href": "notebooks/analysis/02_orientation.html#theta_nb",
    "title": "Orientation",
    "section": "\\(\\theta_{n,b}\\)",
    "text": "\\(\\theta_{n,b}\\)\n\n\nCode\nall_events_l1['theta_n_b'].describe()\n\n\n\n\n\n\n\nshape: (9, 2)\n\n\n\nstatistic\nvalue\n\n\nstr\nf64\n\n\n\n\n\"count\"\n280740.0\n\n\n\"null_count\"\n0.0\n\n\n\"mean\"\n89.565227\n\n\n\"std\"\n24.914059\n\n\n\"min\"\n0.046898\n\n\n\"25%\"\n78.403561\n\n\n\"50%\"\n89.87696\n\n\n\"75%\"\n100.96636\n\n\n\"max\"\n179.86788\n\n\n\n\n\n\n\n\n\nCode\n%%R\ny &lt;- \"theta_n_b\"\nylab &lt;- expression(theta[\"n,b\"])\ny_lim &lt;- c(60, 120)\np1 &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim, y_log=FALSE)\np1",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Orientation"
    ]
  },
  {
    "objectID": "notebooks/analysis/02_orientation.html#change-of-magnetic-field",
    "href": "notebooks/analysis/02_orientation.html#change-of-magnetic-field",
    "title": "Orientation",
    "section": "Change of magnetic field",
    "text": "Change of magnetic field\n\n\nCode\n%%R -w 1200 -h 1200\n\nx &lt;- \"dB_x_n2\"\ny &lt;- \"dB_y_n2\"\nz &lt;- \"dB_z_n2\"\nfacets &lt;- \"r_bin\"\n\np1_JNO &lt;- plot_plot(JNO_events_l1, x, y, facets = facets) + ggtitle(\"JUNO\") + theme(legend.position = \"none\")\np1_other &lt;- plot_plot(other_events_l1, x, y)  + ggtitle(\"Others\") + theme(legend.position = \"none\")\n\np2_JNO &lt;- plot_plot(JNO_events_l1, x, z, facets = facets) + theme(legend.position = \"none\")\np2_other &lt;- plot_plot(other_events_l1, x, z, facets = facets)\n\np3_JNO &lt;- plot_plot(JNO_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\np3_other &lt;- plot_plot(other_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\n\n\np &lt;- (p1_JNO | p1_other) / (p2_JNO | p2_other) / (p3_JNO | p3_other)\nsave_plot(\"orientation/orientation_dB_xyz\")\np\n\n\nSaving 16.7 x 16.7 in image\nSaving 16.7 x 16.7 in image\n\n\nIn addition: Warning message:\n`aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was\ngenerated. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(JNO_events_l1.hvplot.kde('theta_dB', by=\"r_bin\",subplots=True).cols(1) + JNO_events_l1.hvplot.kde('phi_dB', by=\"r_bin\",subplots=True).cols(1))\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nCode\n(all_events_l1.hvplot.hist('theta_dB', by=\"sat\",subplots=True).cols(1) + all_events_l1.hvplot.hist('phi_dB', by=\"sat\",subplots=True).cols(1))",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Orientation"
    ]
  },
  {
    "objectID": "notebooks/analysis/02_orientation.html#normal-direction-obtained-from-dot-product",
    "href": "notebooks/analysis/02_orientation.html#normal-direction-obtained-from-dot-product",
    "title": "Orientation",
    "section": "Normal direction obtained from dot product",
    "text": "Normal direction obtained from dot product\n\n\nCode\ndef dist_plot(df: pl.LazyFrame, var, by=None):\n    return df.hvplot.density(var, by=by, subplots=True, width=300, height=300)\n\n\nMost discontinuities has normal direction with large \\(k_x\\) and small \\(k_y\\) with evenly distributed \\(k_z\\).\n\n\nCode\n# (dist_plot(JNO_events_l1, \"k_x\", by=\"r_bin\") +  dist_plot(other_events_l1, \"k_x\")).cols(3) + (dist_plot(JNO_events_l1, \"k_y\", by=\"r_bin\") +  dist_plot(other_events_l1, \"k_y\")).cols(3)\n\npn.Column(\n    dist_plot(JNO_events_l1, \"k_x\", by=\"r_bin\").cols(5) +  dist_plot(other_events_l1, \"k_x\"),\n    dist_plot(JNO_events_l1, \"k_y\", by=\"r_bin\").cols(5) +  dist_plot(other_events_l1, \"k_y\"),\n    dist_plot(JNO_events_l1, \"k_z\", by=\"r_bin\").cols(5) +  dist_plot(other_events_l1, \"k_z\"),\n)\n\n\n\n\n\n\n\nCode\n%%R -w 1200 -h 1200\n\nx &lt;- \"k_x\"\ny &lt;- \"k_y\"\nz &lt;- \"k_z\"\nfacets &lt;- \"r_bin\"\n\np1_JNO &lt;- plot_plot(JNO_events_l1, x, y, facets = facets) + ggtitle(\"JUNO\") + theme(legend.position = \"none\")\np1_other &lt;- plot_plot(other_events_l1, x, y)  + ggtitle(\"Others\") + theme(legend.position = \"none\")\n\np2_JNO &lt;- plot_plot(JNO_events_l1, x, z, facets = facets) + theme(legend.position = \"none\")\np2_other &lt;- plot_plot(other_events_l1, x, z, facets = facets)\n\np3_JNO &lt;- plot_plot(JNO_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\np3_other &lt;- plot_plot(other_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\n\n\np &lt;- (p1_JNO | p1_other) / (p2_JNO | p2_other) / (p3_JNO | p3_other)\nsave_plot(\"orientation/orientation_k_xyz\")\np\n\n\nSaving 16.7 x 16.7 in image\nSaving 16.7 x 16.7 in image\n\n\nIn addition: Warning message:\n`aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was\ngenerated.",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Orientation"
    ]
  },
  {
    "objectID": "notebooks/analysis/02_orientation.html#v_l",
    "href": "notebooks/analysis/02_orientation.html#v_l",
    "title": "Orientation",
    "section": "\\(V_l\\)",
    "text": "\\(V_l\\)\n\n\nCode\n%%R -w 1200 -h 1200\n\nx &lt;- \"Vl_x\"\ny &lt;- \"Vl_y\"\nz &lt;- \"Vl_z\"\nfacets &lt;- \"r_bin\"\n\np1_JNO &lt;- plot_plot(JNO_events_l1, x, y, facets = facets) + ggtitle(\"JUNO\") + theme(legend.position = \"none\")\np1_other &lt;- plot_plot(other_events_l1, x, y)  + ggtitle(\"Others\") + theme(legend.position = \"none\")\n\np2_JNO &lt;- plot_plot(JNO_events_l1, x, z, facets = facets) + theme(legend.position = \"none\")\np2_other &lt;- plot_plot(other_events_l1, x, z, facets = facets)\n\np3_JNO &lt;- plot_plot(JNO_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\np3_other &lt;- plot_plot(other_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\n\n\np &lt;- (p1_JNO | p1_other) / (p2_JNO | p2_other) / (p3_JNO | p3_other)\nsave_plot(\"orientation/orientation_Vl_xyz\")\np\n\n\nSaving 16.7 x 16.7 in image\nSaving 16.7 x 16.7 in image\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution\n\n\nCode\n%%R\nx_var &lt;- \"time\"\ny_vars &lt;- c(\"k_x\", \"k_y\", \"k_z\")\nxlab &lt;- \"Time\"\nylabs &lt;- c(\"Orientation (k_x)\", \"k_y\", \"k_z\")\np &lt;- plot_util(all_events_l2, x_var = x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"orientation/orientation_k_time\")\n\nx_var &lt;- \"ref_radial_distance\"\nxlab &lt;- \"Referred Radial Distance (AU)\"\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"orientation/orientation_k_r\")\n\nx_var &lt;- \"ref_radial_distance\"\ny_vars &lt;- c(\"Vl_x\", \"Vl_y\", \"Vl_z\")\nxlab &lt;- \"Referred Radial Distance (AU)\"\nylabs &lt;- c(\"Orientation (l_x)\", \"l_y\", \"l_z\")\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"orientation/orientation_l_r\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: There were 30 warnings (use warnings() to see them)\n\n\n\n\nCode\n%%R\nx_col &lt;- \"radial_distance\"\ny_col &lt;- \"k_x\"\ny_lim &lt;- NULL\nx_bins &lt;- 16\ny_bins &lt;- 32\nxlab &lt;- \"Radial Distance (AU)\"\nylab &lt;- \"Orientation (k_x)\"\np &lt;- plot_binned_data(JNO_events_l1, x_col = x_col, y_col = y_col, x_bins = x_bins, y_bins=y_bins, y_lim = y_lim, log_y = FALSE)\np &lt;- p + labs(x = xlab, y= ylab)\nsave_plot(\"orientation/orientation_kx_r_dist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Orientation"
    ]
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Discontinuities in log-log scale\n\n\n\n\n\nSame as above but with multiple time resolutions and time windows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of plamsa property using fitting method\n\n\n\n\n\n\n\nEvolution of plamsa property using derivative method\n\n\n\n\n\n\nFigure 1: From top to bottom is Juno observation in 1,2,3,4,5 AU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Year\n\n\n\n\n\n\n\nLast Year\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Fit Width\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOld\n\n\n\n\n\n\n\n\n\n\n\n\nWind\n\n\n\n\n\n\n\nJuno\n\n\n\n\n\n\n\nDerivative method\n\n\n\n\n\n\n\nFit method",
    "crumbs": [
      "Home",
      "Notes"
    ]
  },
  {
    "objectID": "notes.html#updates",
    "href": "notes.html#updates",
    "title": "Notes",
    "section": "",
    "text": "Discontinuities in log-log scale\n\n\n\n\n\nSame as above but with multiple time resolutions and time windows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of plamsa property using fitting method\n\n\n\n\n\n\n\nEvolution of plamsa property using derivative method\n\n\n\n\n\n\nFigure 1: From top to bottom is Juno observation in 1,2,3,4,5 AU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Year\n\n\n\n\n\n\n\nLast Year\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Fit Width\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOld\n\n\n\n\n\n\n\n\n\n\n\n\nWind\n\n\n\n\n\n\n\nJuno\n\n\n\n\n\n\n\nDerivative method\n\n\n\n\n\n\n\nFit method",
    "crumbs": [
      "Home",
      "Notes"
    ]
  },
  {
    "objectID": "notes.html#solar-wind-model",
    "href": "notes.html#solar-wind-model",
    "title": "Notes",
    "section": "Solar Wind Model",
    "text": "Solar Wind Model\nSadly, JUNO does not provide plasma data during the cruise phase, so to estimate the plasma state we will use MHD model.\nWe are using Michigan Solar WInd Model 2D (MSWIM2D), which models the solar wind propagation in 2D using the BATSRUS MHD solver. Keebler et al. (2022)\nSome key points about the model\n\nRepresenting the solar wind in the ecliptic plane from 1 to 75 AU\n2D MHD model, using the BATSRUS MHD solver\nInclusion of neutral hydrogen (important for the outer heliosphere)\nInner boundary is filled by time-shifting in situ data from multiple spacecraft\n\nFor model validation part, please see JUNO Model Report.",
    "crumbs": [
      "Home",
      "Notes"
    ]
  },
  {
    "objectID": "notes.html#infos",
    "href": "notes.html#infos",
    "title": "Notes",
    "section": "Infos",
    "text": "Infos\n\n\nCode\nimport yaml\nimport polars as pl\nfrom functools import reduce\nfrom great_tables import GT\n\nwith open(\"data/info.yml\", \"r\") as file:\n    data = yaml.safe_load(file)\n\n\n# Recursive function to find magnetometers\ndef find_instrument(instruments, ins_type):\n    instrument = {}\n    for key, value in instruments.items():\n        if value.get(\"type\") == ins_type:\n            instrument = value\n        elif \"instruments\" in value:\n            instrument = find_instrument(value[\"instruments\"], ins_type)\n    return instrument\n\n\ndef parse_yaml(data, info, ins_type=None):\n    rows = []\n\n    for mission, details in data[\"missions\"].items():\n        if ins_type is None:\n            rows.append([mission, details.get(info)])\n            continue\n        instruments = details.get(\"instruments\", {})\n        instrument = find_instrument(instruments, ins_type)\n        rows.append([mission, instrument.get(info)])\n\n    return pl.DataFrame(rows, schema=[\"Mission\", info])\n\n\n# Define the merge function\ndef merge_dfs(left: pl.DataFrame, right, on=\"Mission\", how=\"outer_coalesce\"):\n    return left.join(right, on=on, how=how)\n\n# Parse and display the table\ndf_mag = parse_yaml(data, \"time_resolutions\", \"magnetometer\").rename(\n    {\"time_resolutions\": \"δt(B)\"}\n)\ndf_plasma = parse_yaml(data, \"time_resolutions\", \"plasma\").rename(\n    {\"time_resolutions\": \"δt(plasma)\"}\n)\ndf_r = parse_yaml(data, \"radial_coverage\")\ndf_time = parse_yaml(data, \"launch_date\")\ndf_link = parse_yaml(data, \"website\")\n\ndf_list = [df_mag, df_plasma, df_r, df_time, df_link]  # Replace with your DataFrames\n\ndf_merged = reduce(merge_dfs, df_list).with_columns(pl.col(\"website\").list.join(\", \"))\n\nGT(df_merged.to_pandas()).fmt_markdown(\"website\").cols_label(\n    radial_coverage=\"Radial coverage\",\n    launch_date=\"Launch date\",\n)\n\n\n\n\nTable 1: Missions info\n\n\n\n\n\n\n\n\n\n\nMission\nδt(B)\nδt(plasma)\nRadial coverage\nLaunch date\nwebsite\n\n\n\n\nJuno\n1 Hz\n\n1 - 5.5 AU\n2011-08-05\nNASA\n\n\nARTEMIS\n5 Hz, ...\n\n\n2007-02-17\n\n\n\nSTEREO-A\n8 Hz\n\n\n2006-10-26\n\n\n\nWIND\n11 Hz\n\n\n1994-11-01\nNASA\n\n\nSolar Orbiter\n\n\n0.28 - 0.9 AU\n2020-02-10\nESA\n\n\nParker Solar Probe\n\n\n0.05 - 1 AU\n2018-08-12\nNASA",
    "crumbs": [
      "Home",
      "Notes"
    ]
  },
  {
    "objectID": "presentations/AGU23_poster.html#main-findings",
    "href": "presentations/AGU23_poster.html#main-findings",
    "title": "Solar wind discontinuities spatial evolution and energetic ion scattering",
    "section": "Main findings",
    "text": "Main findings\n\nSolar wind discontinuities evolve in space with occurrence rate decreasing, thickness increasing and current density decreasing with distance from the Sun. And they are probably generated locally beyond 1 AU.\nBackground sheared magnetic field plays an important role in determining the efficiency of ion pitch angle scattering, and characterize three ion populations: transient, trapped, regular.",
    "crumbs": [
      "Home",
      "Presentations",
      "Solar wind discontinuities spatial evolution and energetic ion scattering"
    ]
  },
  {
    "objectID": "presentations/AGU23_poster.html#introduction-motivation",
    "href": "presentations/AGU23_poster.html#introduction-motivation",
    "title": "Solar wind discontinuities spatial evolution and energetic ion scattering",
    "section": "Introduction & Motivation",
    "text": "Introduction & Motivation\n‘Discontinuities’ are discontinuous spatial changes in plasma parameters/characteristics and magnetic fields (colburn1966?).\n\n\n(söding2001?) studied the radial distribution of discontinuities in the solar wind.\n\n\nJoint observations of JUNO & ARTEMIS & Other missions really provides a unique opportunity!!!",
    "crumbs": [
      "Home",
      "Presentations",
      "Solar wind discontinuities spatial evolution and energetic ion scattering"
    ]
  },
  {
    "objectID": "presentations/AGU23_poster.html#method",
    "href": "presentations/AGU23_poster.html#method",
    "title": "Solar wind discontinuities spatial evolution and energetic ion scattering",
    "section": "Method",
    "text": "Method\n\nWe use (liu2022a?) method to identify IDs, which has better compatibility for the IDs with minor field changes.\nThen the minimum variance analysis is applied to each ID event to obtain the boundary normal (LMN) coordinate and extract IDs’ features.\nHamiltonian model is applied for investigation of ion dynamics in the solar wind discontinuity configuration.\n\nThe most generalized form of dimensionless hamiltonian equation for ions in force-free rotational magnetic discontinuities configuration is\n\\[\nH=\\frac{1}{2}\\left(\\frac{1}{6} \\alpha^2 c_2 z^3-c_1 z+p_x\\right)^2+\\frac{p_z^2}{2}+\\frac{1}{2}\\left(\\kappa x-\\frac{\\alpha z^2}{2}\\right)^2\n\\]\nWith \\(B_l^2+B_m^2=\\text { const }\\), we have\n\\[\nH=\\frac{1}{2}\\left(\\frac{\\alpha^2 z^3}{6 \\sqrt{\\kappa_m^2+1}}-z \\sqrt{\\kappa_m^2+1}+p_x\\right)^2+\\frac{p_z^2}{2}+\\frac{1}{2}\\left(\\kappa_n x-\\frac{\\alpha z^2}{2}\\right)^2\n\\]\nThe system has three parameters\n\\[\n\\kappa_n=\\frac{B_n}{B_{l, \\max }}\n\\quad\n\\kappa_m=\\frac{B_{m, 0}}{B_{l, \\max }}\n\\quad\n\\alpha=\\frac{l_0}{L}=\\frac{\\text { gyro radius }}{\\text { system length }}\n\\]",
    "crumbs": [
      "Home",
      "Presentations",
      "Solar wind discontinuities spatial evolution and energetic ion scattering"
    ]
  },
  {
    "objectID": "presentations/AGU23_poster.html#results",
    "href": "presentations/AGU23_poster.html#results",
    "title": "Solar wind discontinuities spatial evolution and energetic ion scattering",
    "section": "Results",
    "text": "Results\n\nNormalized occurrence rate of IDs drops with the radial distance from the Sun, following 1/r law.\n\n\n\n\n\n\nHigh energy particle has higher chance to cross uncertainty curve\nHigh shear magnetic field will make separatrix vanishes and the geometrical jumps of the quasiadiabatic invariant disappear",
    "crumbs": [
      "Home",
      "Presentations",
      "Solar wind discontinuities spatial evolution and energetic ion scattering"
    ]
  },
  {
    "objectID": "notebooks/00_overview.html",
    "href": "notebooks/00_overview.html",
    "title": "Overview",
    "section": "",
    "text": "Snunspot data\nCode\nimport matplotlib.pyplot as plt\nfrom beforerr.matplotlib import unify_axes_fontsize\nCode\nimport matplotlib.pyplot as plt\nimport scienceplots\n\nfrom space_analysis.utils.cdas import Variables\nfrom space_analysis.plot.basic import savefig\nimport pandas as pd\nimport polars as pl\n\nfrom matplotlib.pyplot import Axes\n\nplt.rc(\"savefig\", dpi=300)\nplt.rc('figure.subplot', wspace = 0, hspace = 0)\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nInput In [2], in &lt;cell line: 2&gt;()\n      1 import matplotlib.pyplot as plt\n----&gt; 2 import scienceplots\n      4 from space_analysis.utils.cdas import Variables\n      5 from space_analysis.plot.basic import savefig\n\nModuleNotFoundError: No module named 'scienceplots'\nCode\ntimerange = [\"2011-08-01\", \"2016-07-01\"]\nfname = \"juno-hg-loc_sunspot-number\"\n\ndatasets = ['JUNO_HELIO1DAY_POSITION', 'EARTH_HELIO1DAY_POSITION', 'STA_HELIO1DAY_POSITION']\nnames = ['JUNO', 'EARTH', 'STA']",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "notebooks/00_overview.html#plotting-juno-and-other-mission-location",
    "href": "notebooks/00_overview.html#plotting-juno-and-other-mission-location",
    "title": "Overview",
    "section": "Plotting Juno and other mission location",
    "text": "Plotting Juno and other mission location\n\n\nCode\ndfs = []\n\nfor dataset, name in zip(datasets, names):\n    ds_variables = Variables(\n        dataset=dataset,\n        timerange=timerange,\n    )\n    df = ds_variables.to_pandas()\n    df = df.rename(columns={c: f\"{name}_{c}\" for c in df.columns})\n    dfs.append(df)\n\n\n2024-02-08 23:52:27.545 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - RAD_AU\n2024-02-08 23:52:27.546 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - SE_LAT\n2024-02-08 23:52:27.546 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - SE_LON\n2024-02-08 23:52:27.546 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HG_LAT\n2024-02-08 23:52:27.547 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HG_LON\n2024-02-08 23:52:27.547 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HGI_LAT\n2024-02-08 23:52:27.548 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HGI_LON\n2024-02-08 23:52:28.326 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - RAD_AU\n2024-02-08 23:52:28.327 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - SE_LAT\n2024-02-08 23:52:28.327 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - SE_LON\n2024-02-08 23:52:28.328 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HG_LAT\n2024-02-08 23:52:28.328 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HG_LON\n2024-02-08 23:52:28.328 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HGI_LAT\n2024-02-08 23:52:28.329 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HGI_LON\n2024-02-08 23:52:29.032 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - RAD_AU\n2024-02-08 23:52:29.033 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - SE_LAT\n2024-02-08 23:52:29.033 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - SE_LON\n2024-02-08 23:52:29.034 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HG_LAT\n2024-02-08 23:52:29.034 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HG_LON\n2024-02-08 23:52:29.034 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HGI_LAT\n2024-02-08 23:52:29.035 | INFO     | space_analysis.utils.cdas:get_dataset_variables:25 - HGI_LON\n\n\n\n\nCode\ndf_concated = pd.concat(dfs, axis=1)\n\n\n\n\nCode\ndf = pl.DataFrame(df_concated.reset_index()).with_columns(\n    JUNO_EARTH_HGI_LON_diff = (pl.col(\"JUNO_HGI_LON\") - pl.col(\"EARTH_HGI_LON\")).abs(),\n    JUNO_STA_HGI_LON_diff = (pl.col(\"JUNO_HGI_LON\") - pl.col(\"STA_HGI_LON\")).abs(),\n    JUNO_EARTH_HG_LON_diff = (pl.col(\"JUNO_HG_LON\") - pl.col(\"EARTH_HG_LON\")).abs(),\n    JUNO_STA_HG_LON_diff = (pl.col(\"JUNO_HG_LON\") - pl.col(\"STA_HG_LON\")).abs(),\n).with_columns(\n    JUNO_EARTH_HG_LON_diff = pl.when(pl.col(\"JUNO_EARTH_HG_LON_diff\")&gt;180).then(360-pl.col(\"JUNO_EARTH_HG_LON_diff\")).otherwise(pl.col(\"JUNO_EARTH_HG_LON_diff\")),\n    JUNO_STA_HG_LON_diff = pl.when(pl.col(\"JUNO_STA_HG_LON_diff\")&gt;180).then(360-pl.col(\"JUNO_STA_HG_LON_diff\")).otherwise(pl.col(\"JUNO_STA_HG_LON_diff\")),\n)\n\n\n\n\nCode\ndef plot_lon_diff(df,ax: Axes):\n    ax.plot(df[\"Epoch\"], df[\"JUNO_EARTH_HG_LON_diff\"], label=\"Earth\")\n    ax.plot(df[\"Epoch\"], df[\"JUNO_STA_HG_LON_diff\"], label=\"STA\")\n    ax.set_ylabel(\"HG Longitude\\nDifference\\n[deg]\")",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "notebooks/00_overview.html#plotting-a-solar-cycle-index",
    "href": "notebooks/00_overview.html#plotting-a-solar-cycle-index",
    "title": "Overview",
    "section": "Plotting a solar cycle index",
    "text": "Plotting a solar cycle index\nhttps://docs.sunpy.org/en/stable/generated/gallery/plotting/solar_cycle_example.html\n\n\nCode\n%pip install \"sunpy[net, timeseries]\"\n\n\n\n\nCode\nfrom astropy.time import Time\n\nimport sunpy.timeseries as ts\nfrom sunpy.net import Fido\nfrom sunpy.net import attrs as a\nfrom sunpy.time import TimeRange\n\n\n08-Feb-24 23:52:31: /Users/zijin/micromamba/envs/cool_planet/lib/python3.10/site-packages/drms/version.py:25: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  for j, piece in enumerate(LooseVersion(version).version[:3]):\n\n\n\nThe U.S. Dept. of Commerce, NOAA, Space Weather Prediction Center (SWPC)\nprovides recent solar cycle indices which includes different sunspot numbers,\nradio flux, and geomagnetic index. They also provide predictions for how the\nsunspot number and radio flux will evolve. Predicted values are based on the\nconsensus of the Solar Cycle 24 Prediction Panel.\nWe will first search for and then download the data.\n\n\nCode\ntime_range = TimeRange(timerange)\nresult = Fido.search(a.Time(time_range), a.Instrument('noaa-indices'))\nf_noaa_indices = Fido.fetch(result)\n\nnoaa = ts.TimeSeries(f_noaa_indices, source='noaaindices').truncate(time_range)\n\n\nWe then load them into individual ~sunpy.timeseries.TimeSeries objects.\nFinally, we plot both noaa and noaa_predict for the sunspot number.\nIn this case we use the S.I.D.C. Brussels International Sunspot Number (RI).\nThe predictions provide both a high and low values, which we plot below as\nranges.\n\n\nCode\n\ndef plot_sunspot_number(noaa_indices_ts, ax):\n    \n    time : Time = noaa_indices_ts.time\n    time = time.to_datetime()\n    \n    ax.scatter(time, noaa_indices_ts.quantity('sunspot RI'))\n    ax.plot(time, noaa_indices_ts.quantity('sunspot RI'), label='Monthly')\n    ax.plot(\n        time, noaa_indices_ts.quantity('sunspot RI smooth'), label='Smoothed'\n    )\n    \n    ax.set_ylim(bottom=0)\n    ax.set_ylabel('Sunspot\\nNumber\\n[#]')",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "notebooks/00_overview.html#plotting-omni-and-stereo-plasma-data",
    "href": "notebooks/00_overview.html#plotting-omni-and-stereo-plasma-data",
    "title": "Overview",
    "section": "Plotting OMNI and STEREO plasma data",
    "text": "Plotting OMNI and STEREO plasma data\n\n\nCode\nimport sys\nimport yaml\n\nfrom space_analysis.ds.tplot import Config, export, process_panel, plot\n\nfile_path = 'omni_stereo.yml'\nconfig = yaml.load(open(file_path), Loader=yaml.FullLoader)\nconfig = Config(**config)\ntvars2plot = []\n\n\n\n\nCode\nfor p_config in config.panels:\n    tvar2plot = process_panel(p_config)\n    tvars2plot.append(tvar2plot)\n\n\n08-Feb-24 23:52:12: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2011/\n08-Feb-24 23:52:13: File is current: /Users/zijin/data/omni/hourly/2011/omni2_h0_mrg1hr_20110701_v01.cdf\n08-Feb-24 23:52:13: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2012/\n08-Feb-24 23:52:13: File is current: /Users/zijin/data/omni/hourly/2012/omni2_h0_mrg1hr_20120101_v01.cdf\n08-Feb-24 23:52:13: File is current: /Users/zijin/data/omni/hourly/2012/omni2_h0_mrg1hr_20120701_v01.cdf\n08-Feb-24 23:52:13: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2013/\n08-Feb-24 23:52:13: File is current: /Users/zijin/data/omni/hourly/2013/omni2_h0_mrg1hr_20130101_v01.cdf\n08-Feb-24 23:52:14: File is current: /Users/zijin/data/omni/hourly/2013/omni2_h0_mrg1hr_20130701_v01.cdf\n08-Feb-24 23:52:14: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2014/\n08-Feb-24 23:52:14: File is current: /Users/zijin/data/omni/hourly/2014/omni2_h0_mrg1hr_20140101_v01.cdf\n08-Feb-24 23:52:14: File is current: /Users/zijin/data/omni/hourly/2014/omni2_h0_mrg1hr_20140701_v01.cdf\n08-Feb-24 23:52:14: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2015/\n08-Feb-24 23:52:15: File is current: /Users/zijin/data/omni/hourly/2015/omni2_h0_mrg1hr_20150101_v01.cdf\n08-Feb-24 23:52:15: File is current: /Users/zijin/data/omni/hourly/2015/omni2_h0_mrg1hr_20150701_v01.cdf\n08-Feb-24 23:52:15: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2016/\n08-Feb-24 23:52:15: File is current: /Users/zijin/data/omni/hourly/2016/omni2_h0_mrg1hr_20160101_v01.cdf\n08-Feb-24 23:52:16: File is current: /Users/zijin/data/omni/hourly/2016/omni2_h0_mrg1hr_20160701_v01.cdf\n08-Feb-24 23:52:16: avg_data was applied to: N-avg\n2024-02-08 23:52:16.820 | DEBUG    | space_analysis.ds.tplot:process_panel:369 - Processed tvar: ['N-avg']\n08-Feb-24 23:52:16: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2011/\n08-Feb-24 23:52:17: File is current: /Users/zijin/data/omni/hourly/2011/omni2_h0_mrg1hr_20110701_v01.cdf\n08-Feb-24 23:52:17: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2012/\n08-Feb-24 23:52:17: File is current: /Users/zijin/data/omni/hourly/2012/omni2_h0_mrg1hr_20120101_v01.cdf\n08-Feb-24 23:52:17: File is current: /Users/zijin/data/omni/hourly/2012/omni2_h0_mrg1hr_20120701_v01.cdf\n08-Feb-24 23:52:17: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2013/\n08-Feb-24 23:52:18: File is current: /Users/zijin/data/omni/hourly/2013/omni2_h0_mrg1hr_20130101_v01.cdf\n08-Feb-24 23:52:18: File is current: /Users/zijin/data/omni/hourly/2013/omni2_h0_mrg1hr_20130701_v01.cdf\n08-Feb-24 23:52:18: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2014/\n08-Feb-24 23:52:18: File is current: /Users/zijin/data/omni/hourly/2014/omni2_h0_mrg1hr_20140101_v01.cdf\n08-Feb-24 23:52:19: File is current: /Users/zijin/data/omni/hourly/2014/omni2_h0_mrg1hr_20140701_v01.cdf\n08-Feb-24 23:52:19: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2015/\n08-Feb-24 23:52:19: File is current: /Users/zijin/data/omni/hourly/2015/omni2_h0_mrg1hr_20150101_v01.cdf\n08-Feb-24 23:52:19: File is current: /Users/zijin/data/omni/hourly/2015/omni2_h0_mrg1hr_20150701_v01.cdf\n08-Feb-24 23:52:19: Downloading remote index: https://spdf.gsfc.nasa.gov/pub/data/omni/omni_cdaweb/hourly/2016/\n08-Feb-24 23:52:19: File is current: /Users/zijin/data/omni/hourly/2016/omni2_h0_mrg1hr_20160101_v01.cdf\n08-Feb-24 23:52:20: File is current: /Users/zijin/data/omni/hourly/2016/omni2_h0_mrg1hr_20160701_v01.cdf\n08-Feb-24 23:52:20: avg_data was applied to: V-avg\n2024-02-08 23:52:20.979 | DEBUG    | space_analysis.ds.tplot:process_panel:369 - Processed tvar: ['V-avg']\n08-Feb-24 23:52:24: avg_data was applied to: plasmaDensity-avg\n2024-02-08 23:52:24.183 | DEBUG    | space_analysis.ds.tplot:process_panel:369 - Processed tvar: ['plasmaDensity-avg']\n08-Feb-24 23:52:26: avg_data was applied to: plasmaSpeed-avg\n2024-02-08 23:52:26.524 | DEBUG    | space_analysis.ds.tplot:process_panel:369 - Processed tvar: ['plasmaSpeed-avg']\n\n\n\n\nCode\nfig, axes = plot(tvars2plot, config)\nfig\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(10, 6))\n\nplot_lon_diff(df, axes[0])\nplot_sunspot_number(noaa, axes[1])",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "notebooks/00_overview.html#overview-plot",
    "href": "notebooks/00_overview.html#overview-plot",
    "title": "Overview",
    "section": "Overview plot",
    "text": "Overview plot\n\n\nCode\nwith plt.style.context(['science', 'nature', 'no-latex']):\n    fig, axes = plt.subplots(nrows=6, ncols=1, sharex=True, figsize=(7.5, 7.5))\n\n    plot_lon_diff(df, axes[0])\n    plot_sunspot_number(noaa, axes[1])\n    plot(tvars2plot=tvars2plot, fig=fig, axes=axes[2:], config=config)\n    \n    for ax in axes:\n        legend = ax.legend(handletextpad=0, handlelength=0)\n    \n        for line, text in zip(legend.get_lines(),legend.get_texts()):\n            text.set_color(line.get_color())\n    \n    unify_axes_fontsize(axes, fontsize=8)\n    # plt.subplots_adjust(wspace=0, hspace=0)\n    \n    savefig(fname)\n\nfig\n\n\n09-Feb-24 00:39:00: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n09-Feb-24 00:39:00: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n09-Feb-24 00:39:00: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n09-Feb-24 00:39:00: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "notebooks/analysis/00_base.html",
    "href": "notebooks/analysis/00_base.html",
    "title": "Loading all datasets from different sources",
    "section": "",
    "text": "Code\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\nimport numpy as np\n\nfrom loguru import logger\n::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=2}\n:::\nCode\nfrom discontinuitypy.utils.basic import load_catalog\n\ncatalog = load_catalog()\nCode\nfrom discontinuitypy.datasets import cIDsDataset\n\nsta_dataset = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\njno_dataset = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nthb_dataset = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\n\n\n16-Nov-23 23:42:22 INFO     16-Nov-23 23:42:22: Loading data from 'events.STA_ts_1s_tau_60s'    data_catalog.py:502\n                            (LazyPolarsDataset)...                                                                 \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'STA.MAG.primary_data_ts_1s'  data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'events.JNO_ts_1s_tau_60s'    data_catalog.py:502\n                            (LazyPolarsDataset)...                                                                 \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'JNO.MAG.primary_data_ts_1s'  data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'events.THB_ts_1s_tau_60s'    data_catalog.py:502\n                            (LazyPolarsDataset)...                                                                 \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'THB.MAG.primary_data_ts_1s'  data_catalog.py:502\n                            (PartitionedDataset)...\nCode\nfrom beforerr.basics import pmap\nfrom discontinuitypy.utils.analysis import filter_tranges_ds\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('thb.inter_state_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nthb_sw_dataset = filter_tranges_ds(thb_dataset, (start, end))\n\n\n[11/13/23 20:28:03] INFO     Loading data from 'thb.inter_state_sw' (LazyPolarsDataset)...      data_catalog.py:502\nCode\nall_datasets = [sta_dataset, jno_dataset, thb_sw_dataset]\nCode\nall_candidates_l0 : pl.DataFrame = pl.concat(\n    all_datasets | pmap(lambda x: x.candidates),\n    how=\"diagonal\",\n)\nCode\ndef combine_candidates(datasets):\n    return pl.concat(\n        datasets | pmap(lambda x: x.candidates),\n        how=\"diagonal\",\n    )",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Loading all datasets from different sources"
    ]
  },
  {
    "objectID": "notebooks/analysis/00_base.html#processing-datasets",
    "href": "notebooks/analysis/00_base.html#processing-datasets",
    "title": "Loading all datasets from different sources",
    "section": "Processing datasets",
    "text": "Processing datasets\nSome extreme values are present in the data. We will remove them.\n\n\nCode\n\nNVARS = ['d_star', 'L_mn', 'L_mn_norm', 'j0', 'j0_norm', 'duration', 'v_mn']\nDISPLAY_VARS = ['time', 'sat'] + NVARS\n\n\ndef check_candidates(df):\n    return df[NVARS].describe()\n\ncheck_candidates(all_candidates_l0)\n\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n185066.0\n185066.0\n185066.0\n185066.0\n185066.0\n\"185066\"\n185066.0\n\n\n\"null_count\"\n0.0\n4120.0\n4389.0\n4120.0\n4389.0\n\"0\"\n4120.0\n\n\n\"mean\"\n2.611712\n2798.843381\n22.307474\n11.654787\n4.713652\n\"0:00:08.198437…\n343.811034\n\n\n\"std\"\n491.756741\n2179.474212\n20.649185\n2894.040891\n1473.838227\nnull\n99.930132\n\n\n\"min\"\n0.019601\n3.381065\n0.014144\n0.0561\n0.00082\n\"0:00:01.999999…\n0.41411\n\n\n\"25%\"\n0.247087\n1582.102536\n11.284664\n0.601477\n0.028203\n\"0:00:05\"\n286.126017\n\n\n\"50%\"\n0.510951\n2240.279834\n17.513617\n1.239019\n0.051221\n\"0:00:07\"\n343.325961\n\n\n\"75%\"\n0.983944\n3346.020528\n27.236719\n2.34897\n0.091488\n\"0:00:10\"\n402.282733\n\n\n\"max\"\n152023.367594\n103745.212024\n1614.132093\n1.1500e6\n583059.205803\n\"0:03:16\"\n864.604665\n\n\n\n\n\n\n\n\n\nCode\nfrom datetime import timedelta\ndef process_candidates_l1(raw_df: pl.DataFrame):\n    \"clean data to remove extreme values\"\n\n    df = raw_df.filter(\n        pl.col(\"d_star\") &lt; 100, # exclude JUNO extreme values\n        pl.col('v_mn') &gt; 10,\n        pl.col('duration') &lt; timedelta(seconds=60),\n        # pl.col(\"j0\") &lt; 100\n    ).with_columns(\n        pl.col('radial_distance').fill_null(1) # by default, fill with 1 AU\n    ).with_columns(\n        r_bin = pl.col('radial_distance').round(),\n        j0_norm_log = pl.col('j0_norm').log10(),\n        L_mn_norm_log = pl.col('L_mn_norm').log10(),\n    )\n\n    logger.info(\n        f\"candidates_l1: {len(df)}, with effective ratio: {len(df) / len(raw_df):.2%}\"\n    )\n\n    return df\n\nall_candidates_l1 = process_candidates_l1(all_candidates_l0)\n%R -i all_candidates_l1 -c conv_pl\ncheck_candidates(all_candidates_l1)\n\n\n2023-11-08 14:11:23.225 | INFO     | __main__:process_candidates_l1:18 - candidates_l1: 180718, with effective ratio: 97.65%\n\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n180718.0\n180718.0\n180718.0\n180718.0\n180718.0\n\"180718\"\n180718.0\n\n\n\"null_count\"\n0.0\n0.0\n264.0\n0.0\n264.0\n\"0\"\n0.0\n\n\n\"mean\"\n0.745751\n2768.506268\n22.033678\n1.865352\n0.075518\n\"0:00:08.118150…\n343.880697\n\n\n\"std\"\n0.771981\n1909.065522\n17.629565\n2.599027\n0.097857\nnull\n99.846681\n\n\n\"min\"\n0.019601\n48.94197\n0.124168\n0.0561\n0.00082\n\"0:00:01.999999…\n10.240242\n\n\n\"25%\"\n0.243875\n1581.58393\n11.279769\n0.60174\n0.028229\n\"0:00:05\"\n286.190021\n\n\n\"50%\"\n0.50421\n2238.553736\n17.499906\n1.239576\n0.051251\n\"0:00:07\"\n343.360031\n\n\n\"75%\"\n0.97075\n3340.552536\n27.186555\n2.348456\n0.091501\n\"0:00:10\"\n402.301723\n\n\n\"max\"\n13.805873\n35975.767016\n439.323024\n393.479096\n9.634978\n\"0:00:59\"\n864.604665\n\n\n\n\n\n\n\n\n\nCode\njno_candidates_l1 = all_candidates_l1.filter(pl.col('sat') == 'JNO')\n%R -i jno_candidates_l1 -c conv_pl\n\n\n\n\nCode\nfrom discontinuitypy.utils.analysis import filter_before_jupiter\nfrom discontinuitypy.utils.analysis import link_coord2dim\n\n\n\n\nCode\ndef process_candidates_l2(raw_df: pl.DataFrame, avg_window=\"30d\"):\n    time_col = \"time\"\n\n    candidate = (\n        raw_df.sort(time_col)\n        .group_by_dynamic(time_col, every=avg_window, by=\"sat\")\n        .agg(cs.numeric().mean(), cs.duration().mean(), id_count=pl.count())\n        .filter(pl.col(\"id_count\") &gt; 50)  # filter out JUNO extreme large thickness\n        .sort(time_col)\n        .upsample(time_col, every=avg_window, by=\"sat\", maintain_order=True)\n        .with_columns(pl.col(\"sat\").forward_fill())\n    )\n    return candidate\n\n\n\n\nCode\nall_candidates_l2: pl.DataFrame = (\n    all_candidates_l1.pipe(filter_before_jupiter)\n    .pipe(process_candidates_l2)\n    .pipe(link_coord2dim)\n)\n\n\n\n\nCode\ninspect_df = all_candidates_l2[NVARS]\ninspect_df.describe()\n\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n172.0\n172.0\n172.0\n172.0\n172.0\n\"172\"\n172.0\n\n\n\"null_count\"\n19.0\n19.0\n19.0\n19.0\n19.0\n\"19\"\n19.0\n\n\n\"mean\"\n0.706261\n2922.959632\n22.028999\n1.937378\n0.090728\n\"0:00:08.719631…\n337.428018\n\n\n\"std\"\n0.358616\n512.032439\n8.140589\n1.077249\n0.051647\nnull\n37.917741\n\n\n\"min\"\n0.108318\n1877.983131\n7.074407\n0.229362\n0.042024\n\"0:00:06.751012…\n256.771354\n\n\n\"25%\"\n0.331532\n2590.280777\n14.498058\n0.795284\n0.060267\n\"0:00:07.707419…\n315.324913\n\n\n\"50%\"\n0.794667\n2786.745403\n22.804505\n2.087583\n0.069789\n\"0:00:08.730158…\n335.332916\n\n\n\"75%\"\n0.931735\n3182.843841\n27.726721\n2.633037\n0.094061\n\"0:00:09.315238…\n359.837854\n\n\n\"max\"\n1.539393\n4458.507484\n41.436617\n4.784021\n0.306938\n\"0:00:12.305699…\n445.849288\n\n\n\n\n\n\n\n\n\nCode\nfrom discontinuitypy.utils.analysis import n2_normalize\n\nall_candidates_l2_n2 = n2_normalize(all_candidates_l2, NVARS)",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Loading all datasets from different sources"
    ]
  },
  {
    "objectID": "notebooks/analysis/11_ts_data.html",
    "href": "notebooks/analysis/11_ts_data.html",
    "title": "Time resolution effect (data)",
    "section": "",
    "text": "Code\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nfrom space_analysis.utils.speasy import Variables\nfrom discontinuitypy.datasets import IDsDataset\nfrom discontinuitypy.utils.basic import resample\n\nfrom datetime import timedelta\n\nfrom discontinuitypy.missions import wind_mag_h4_rtn_meta, wind_plasma_k0_swe_meta\n\n\n\n\nCode\nts = timedelta(seconds=1 / 11)\ntau = timedelta(seconds=60)\n\n\n\n\nCode\ntimerange = [\"2016-01-01\", \"2016-06-29\"]\nprovider = \"archive/local\"\nmag_meta = wind_mag_h4_rtn_meta\nplasma_meta = wind_plasma_k0_swe_meta\n\nwind_mag_vars = Variables(\n    timerange=timerange,\n    **mag_meta.model_dump(),\n    provider=provider,\n)\n\nwind_plasma_vars = Variables(\n    timerange=timerange,\n    **plasma_meta.model_dump(),\n    provider=provider,\n)\n\n\n\n\nCode\nwind_mag_data = wind_mag_vars.to_polars()\nwind_plasma_data = wind_plasma_vars.to_polars()\n\n\n\n\nCode\n# for freq in [11, 5 , 2, 1, 0.5]:\nfor freq in [5]:\n    ts = timedelta(seconds=1 / freq)\n\n    wind_ids_dataset = (\n        IDsDataset(\n            mag_data=wind_mag_data.pipe(resample, every=ts),\n            plasma_data=wind_plasma_data,\n            tau=tau,\n            ts=ts,\n            mag_meta=mag_meta,\n            plasma_meta=plasma_meta,\n        )\n        .find_events(return_best_fit=False)\n        .update_events()\n        .export(f\"data/ts_effect/events.Wind.ts_{1/freq:.2f}s_tau_60s.arrow\")\n    )\n\n\n15-May-24 18:09:44: UserWarning: Ray execution environment not yet initialized. Initializing...\nTo remove this warning, run the following python code before doing dataframe operations:\n\n    import ray\n    ray.init()\n\n\n2024-05-15 18:09:46,850 INFO worker.py:1724 -- Started a local Ray instance.\n15-May-24 18:09:48: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n\n\n\n(_deploy_ray_func pid=78514) 15-May-24 18:09:56: RuntimeWarning: overflow encountered in exp\n(_deploy_ray_func pid=78514) \n(_deploy_ray_func pid=78514) 15-May-24 18:09:57: RuntimeWarning: overflow encountered in exp\n(_deploy_ray_func pid=78514) \n(_deploy_ray_func pid=78514) 15-May-24 18:10:02: RuntimeWarning: overflow encountered in exp [repeated 11x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\n(_deploy_ray_func pid=78514)  [repeated 11x across cluster]\n(_deploy_ray_func pid=78514) 15-May-24 18:10:07: RuntimeWarning: overflow encountered in exp [repeated 10x across cluster]\n(_deploy_ray_func pid=78514)  [repeated 10x across cluster]\n(_deploy_ray_func pid=78517) 15-May-24 18:10:13: RuntimeWarning: overflow encountered in exp [repeated 5x across cluster]\n(_deploy_ray_func pid=78517)  [repeated 5x across cluster]\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[19], line 15\n      2 for freq in [5]:\n      3     ts = timedelta(seconds=1 / freq)\n      5     wind_ids_dataset = (\n      6         IDsDataset(\n      7             mag_data=wind_mag_data.pipe(resample, every=ts),\n      8             plasma_data=wind_plasma_data,\n      9             tau=tau,\n     10             ts=ts,\n     11             mag_meta=mag_meta,\n     12             plasma_meta=plasma_meta,\n     13         )\n     14         .find_events(return_best_fit=False)\n---&gt; 15         .update_events()\n     16         .export(f\"data/ts_effect/events.Wind.ts_{1/freq:.2f}s_tau_60s.arrow\")\n     17     )\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:134, in IDsDataset.update_events(self, **kwargs)\n    133 def update_events(self, **kwargs):\n--&gt; 134     return self.update_events_with_plasma_data(\n    135         **kwargs\n    136     ).update_events_with_temp_data(**kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:148, in IDsDataset.update_events_with_plasma_data(self, **kwargs)\n    140 if self.plasma_data is not None:\n    141     df_combined = combine_features(\n    142         self.events,\n    143         self.plasma_data.collect(),\n    144         plasma_meta=self.plasma_meta,\n    145         **kwargs,\n    146     )\n--&gt; 148     self.events = calc_combined_features(\n    149         df_combined,\n    150         plasma_meta=self.plasma_meta,\n    151         **kwargs,\n    152     )\n    153 else:\n    154     logger.info(\"Plasma data is not available.\")\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/integration.py:285, in calc_combined_features(df, b_cols, detail, normal_cols, Vl_cols, Vn_cols, thickness_cols, current_cols, plasma_meta, **kwargs)\n    281 vec_cols = plasma_meta.velocity_cols\n    282 density_col = plasma_meta.density_col\n    284 result = (\n--&gt; 285     result.pipe(vector_project_pl, vec_cols, Vl_cols, name=\"v_l\")\n    286     .pipe(vector_project_pl, vec_cols, Vn_cols, name=\"v_n\")\n    287     .pipe(vector_project_pl, vec_cols, normal_cols, name=\"v_k\")\n    288     .with_columns(\n    289         pl.col(\"v_n\").abs(),\n    290         pl.col(\"v_k\").abs(),\n    291         # v_mn=(pl.col(\"plasma_speed\") ** 2 - pl.col(\"v_l\") ** 2).sqrt(),\n    292     )\n    293     .with_columns(\n    294         L_k=pl.col(\"v_k\") * pl.col(\"duration\"),\n    295         j0_k=pl.col(\"d_star\")\n    296         / pl.col(\n    297             \"v_k\"\n    298         ),  # TODO: d_star corresponding to dB/dt, which direction is not exactly perpendicular to the k direction\n    299         # NOTE: n direction is not properly determined for MVA analysis\n    300         # j0_mn=pl.col(\"d_star\") / pl.col(\"v_mn\"),\n    301         # L_n=pl.col(\"v_n\") * pl.col(\"duration\"),\n    302         # L_mn=pl.col(\"v_mn\") * pl.col(\"duration\"),\n    303         # NOTE: the duration is not properly determined for `max distance` method\n    304         # L_k=pl.col(\"v_k\") * pl.col(\"duration\"),\n    305     )\n    306     .pipe(compute_inertial_length)\n    307     .pipe(compute_Alfven_speed, n=density_col, B=\"b_mag\")\n    308     .pipe(compute_Alfven_current)\n    309     .with_columns(\n    310         cs.by_name(current_cols) * J_FACTOR.value,\n    311     )\n    312     .with_columns(\n    313         (cs.by_name(thickness_cols) / length_norm).name.suffix(\"_norm\"),\n    314         (cs.by_name(current_cols) / current_norm).name.suffix(\"_norm\"),\n    315         (cs.by_name(b_cols) / b_norm).name.suffix(\"_norm\"),\n    316     )\n    317 )\n    319 if detail:\n    320     result = (\n    321         result.pipe(\n    322             vector_project_pl,\n   (...)\n    335         .pipe(calc_plasma_parameter_change, plasma_meta=plasma_meta)\n    336     )\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/polars/dataframe/frame.py:5249, in DataFrame.pipe(self, function, *args, **kwargs)\n   5184 def pipe(\n   5185     self,\n   5186     function: Callable[Concatenate[DataFrame, P], T],\n   5187     *args: P.args,\n   5188     **kwargs: P.kwargs,\n   5189 ) -&gt; T:\n   5190     \"\"\"\n   5191     Offers a structured way to apply a sequence of user-defined functions (UDFs).\n   5192 \n   (...)\n   5247     └─────┴─────┘\n   5248     \"\"\"\n-&gt; 5249     return function(self, *args, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/integration.py:128, in vector_project_pl(df, v1_cols, v2_cols, name)\n    127 def vector_project_pl(df: pl.DataFrame, v1_cols, v2_cols, name=None):\n--&gt; 128     v1 = df2ts(df, v1_cols).assign_coords(v_dim=[\"x\", \"y\", \"z\"])\n    129     v2 = df2ts(df, v2_cols).assign_coords(v_dim=[\"x\", \"y\", \"z\"])\n    130     result = vector_project(v1, v2, dim=\"v_dim\")\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/xarray/core/common.py:621, in DataWithCoords.assign_coords(self, coords, **coords_kwargs)\n    618 else:\n    619     results = self._calc_assign_results(coords_combined)\n--&gt; 621 data.coords.update(results)\n    622 return data\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/xarray/core/coordinates.py:566, in Coordinates.update(self, other)\n    560 # special case for PandasMultiIndex: updating only its dimension coordinate\n    561 # is still allowed but depreciated.\n    562 # It is the only case where we need to actually drop coordinates here (multi-index levels)\n    563 # TODO: remove when removing PandasMultiIndex's dimension coordinate.\n    564 self._drop_coords(self._names - coords_to_align._names)\n--&gt; 566 self._update_coords(coords, indexes)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/xarray/core/coordinates.py:842, in DataArrayCoordinates._update_coords(self, coords, indexes)\n    840 coords_plus_data = coords.copy()\n    841 coords_plus_data[_THIS_ARRAY] = self._data.variable\n--&gt; 842 dims = calculate_dimensions(coords_plus_data)\n    843 if not set(dims) &lt;= set(self.dims):\n    844     raise ValueError(\n    845         \"cannot add coordinates with new dimensions to a DataArray\"\n    846     )\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/xarray/core/variable.py:3008, in calculate_dimensions(variables)\n   3006             last_used[dim] = k\n   3007         elif dims[dim] != size:\n-&gt; 3008             raise ValueError(\n   3009                 f\"conflicting sizes for dimension {dim!r}: \"\n   3010                 f\"length {size} on {k!r} and length {dims[dim]} on {last_used!r}\"\n   3011             )\n   3012 return dims\n\nValueError: conflicting sizes for dimension 'v_dim': length 50 on &lt;this-array&gt; and length 3 on {'time': 'time', 'v_dim': 'v_dim'}",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Time resolution effect (data)"
    ]
  },
  {
    "objectID": "notebooks/analysis/11_ts_data.html#process-wind-data",
    "href": "notebooks/analysis/11_ts_data.html#process-wind-data",
    "title": "Time resolution effect (data)",
    "section": "",
    "text": "Code\n%load_ext autoreload\n%autoreload 2\n\n\n\n\nCode\nfrom space_analysis.utils.speasy import Variables\nfrom discontinuitypy.datasets import IDsDataset\nfrom discontinuitypy.utils.basic import resample\n\nfrom datetime import timedelta\n\nfrom discontinuitypy.missions import wind_mag_h4_rtn_meta, wind_plasma_k0_swe_meta\n\n\n\n\nCode\nts = timedelta(seconds=1 / 11)\ntau = timedelta(seconds=60)\n\n\n\n\nCode\ntimerange = [\"2016-01-01\", \"2016-06-29\"]\nprovider = \"archive/local\"\nmag_meta = wind_mag_h4_rtn_meta\nplasma_meta = wind_plasma_k0_swe_meta\n\nwind_mag_vars = Variables(\n    timerange=timerange,\n    **mag_meta.model_dump(),\n    provider=provider,\n)\n\nwind_plasma_vars = Variables(\n    timerange=timerange,\n    **plasma_meta.model_dump(),\n    provider=provider,\n)\n\n\n\n\nCode\nwind_mag_data = wind_mag_vars.to_polars()\nwind_plasma_data = wind_plasma_vars.to_polars()\n\n\n\n\nCode\n# for freq in [11, 5 , 2, 1, 0.5]:\nfor freq in [5]:\n    ts = timedelta(seconds=1 / freq)\n\n    wind_ids_dataset = (\n        IDsDataset(\n            mag_data=wind_mag_data.pipe(resample, every=ts),\n            plasma_data=wind_plasma_data,\n            tau=tau,\n            ts=ts,\n            mag_meta=mag_meta,\n            plasma_meta=plasma_meta,\n        )\n        .find_events(return_best_fit=False)\n        .update_events()\n        .export(f\"data/ts_effect/events.Wind.ts_{1/freq:.2f}s_tau_60s.arrow\")\n    )\n\n\n15-May-24 18:09:44: UserWarning: Ray execution environment not yet initialized. Initializing...\nTo remove this warning, run the following python code before doing dataframe operations:\n\n    import ray\n    ray.init()\n\n\n2024-05-15 18:09:46,850 INFO worker.py:1724 -- Started a local Ray instance.\n15-May-24 18:09:48: UserWarning: Distributing &lt;class 'pandas.core.frame.DataFrame'&gt; object. This may take some time.\n\n\n\n\n\n\n\n\n\n(_deploy_ray_func pid=78514) 15-May-24 18:09:56: RuntimeWarning: overflow encountered in exp\n(_deploy_ray_func pid=78514) \n(_deploy_ray_func pid=78514) 15-May-24 18:09:57: RuntimeWarning: overflow encountered in exp\n(_deploy_ray_func pid=78514) \n(_deploy_ray_func pid=78514) 15-May-24 18:10:02: RuntimeWarning: overflow encountered in exp [repeated 11x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\n(_deploy_ray_func pid=78514)  [repeated 11x across cluster]\n(_deploy_ray_func pid=78514) 15-May-24 18:10:07: RuntimeWarning: overflow encountered in exp [repeated 10x across cluster]\n(_deploy_ray_func pid=78514)  [repeated 10x across cluster]\n(_deploy_ray_func pid=78517) 15-May-24 18:10:13: RuntimeWarning: overflow encountered in exp [repeated 5x across cluster]\n(_deploy_ray_func pid=78517)  [repeated 5x across cluster]\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[19], line 15\n      2 for freq in [5]:\n      3     ts = timedelta(seconds=1 / freq)\n      5     wind_ids_dataset = (\n      6         IDsDataset(\n      7             mag_data=wind_mag_data.pipe(resample, every=ts),\n      8             plasma_data=wind_plasma_data,\n      9             tau=tau,\n     10             ts=ts,\n     11             mag_meta=mag_meta,\n     12             plasma_meta=plasma_meta,\n     13         )\n     14         .find_events(return_best_fit=False)\n---&gt; 15         .update_events()\n     16         .export(f\"data/ts_effect/events.Wind.ts_{1/freq:.2f}s_tau_60s.arrow\")\n     17     )\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:134, in IDsDataset.update_events(self, **kwargs)\n    133 def update_events(self, **kwargs):\n--&gt; 134     return self.update_events_with_plasma_data(\n    135         **kwargs\n    136     ).update_events_with_temp_data(**kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/datasets.py:148, in IDsDataset.update_events_with_plasma_data(self, **kwargs)\n    140 if self.plasma_data is not None:\n    141     df_combined = combine_features(\n    142         self.events,\n    143         self.plasma_data.collect(),\n    144         plasma_meta=self.plasma_meta,\n    145         **kwargs,\n    146     )\n--&gt; 148     self.events = calc_combined_features(\n    149         df_combined,\n    150         plasma_meta=self.plasma_meta,\n    151         **kwargs,\n    152     )\n    153 else:\n    154     logger.info(\"Plasma data is not available.\")\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/integration.py:285, in calc_combined_features(df, b_cols, detail, normal_cols, Vl_cols, Vn_cols, thickness_cols, current_cols, plasma_meta, **kwargs)\n    281 vec_cols = plasma_meta.velocity_cols\n    282 density_col = plasma_meta.density_col\n    284 result = (\n--&gt; 285     result.pipe(vector_project_pl, vec_cols, Vl_cols, name=\"v_l\")\n    286     .pipe(vector_project_pl, vec_cols, Vn_cols, name=\"v_n\")\n    287     .pipe(vector_project_pl, vec_cols, normal_cols, name=\"v_k\")\n    288     .with_columns(\n    289         pl.col(\"v_n\").abs(),\n    290         pl.col(\"v_k\").abs(),\n    291         # v_mn=(pl.col(\"plasma_speed\") ** 2 - pl.col(\"v_l\") ** 2).sqrt(),\n    292     )\n    293     .with_columns(\n    294         L_k=pl.col(\"v_k\") * pl.col(\"duration\"),\n    295         j0_k=pl.col(\"d_star\")\n    296         / pl.col(\n    297             \"v_k\"\n    298         ),  # TODO: d_star corresponding to dB/dt, which direction is not exactly perpendicular to the k direction\n    299         # NOTE: n direction is not properly determined for MVA analysis\n    300         # j0_mn=pl.col(\"d_star\") / pl.col(\"v_mn\"),\n    301         # L_n=pl.col(\"v_n\") * pl.col(\"duration\"),\n    302         # L_mn=pl.col(\"v_mn\") * pl.col(\"duration\"),\n    303         # NOTE: the duration is not properly determined for `max distance` method\n    304         # L_k=pl.col(\"v_k\") * pl.col(\"duration\"),\n    305     )\n    306     .pipe(compute_inertial_length)\n    307     .pipe(compute_Alfven_speed, n=density_col, B=\"b_mag\")\n    308     .pipe(compute_Alfven_current)\n    309     .with_columns(\n    310         cs.by_name(current_cols) * J_FACTOR.value,\n    311     )\n    312     .with_columns(\n    313         (cs.by_name(thickness_cols) / length_norm).name.suffix(\"_norm\"),\n    314         (cs.by_name(current_cols) / current_norm).name.suffix(\"_norm\"),\n    315         (cs.by_name(b_cols) / b_norm).name.suffix(\"_norm\"),\n    316     )\n    317 )\n    319 if detail:\n    320     result = (\n    321         result.pipe(\n    322             vector_project_pl,\n   (...)\n    335         .pipe(calc_plasma_parameter_change, plasma_meta=plasma_meta)\n    336     )\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/polars/dataframe/frame.py:5249, in DataFrame.pipe(self, function, *args, **kwargs)\n   5184 def pipe(\n   5185     self,\n   5186     function: Callable[Concatenate[DataFrame, P], T],\n   5187     *args: P.args,\n   5188     **kwargs: P.kwargs,\n   5189 ) -&gt; T:\n   5190     \"\"\"\n   5191     Offers a structured way to apply a sequence of user-defined functions (UDFs).\n   5192 \n   (...)\n   5247     └─────┴─────┘\n   5248     \"\"\"\n-&gt; 5249     return function(self, *args, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/discontinuitypy/integration.py:128, in vector_project_pl(df, v1_cols, v2_cols, name)\n    127 def vector_project_pl(df: pl.DataFrame, v1_cols, v2_cols, name=None):\n--&gt; 128     v1 = df2ts(df, v1_cols).assign_coords(v_dim=[\"x\", \"y\", \"z\"])\n    129     v2 = df2ts(df, v2_cols).assign_coords(v_dim=[\"x\", \"y\", \"z\"])\n    130     result = vector_project(v1, v2, dim=\"v_dim\")\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/xarray/core/common.py:621, in DataWithCoords.assign_coords(self, coords, **coords_kwargs)\n    618 else:\n    619     results = self._calc_assign_results(coords_combined)\n--&gt; 621 data.coords.update(results)\n    622 return data\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/xarray/core/coordinates.py:566, in Coordinates.update(self, other)\n    560 # special case for PandasMultiIndex: updating only its dimension coordinate\n    561 # is still allowed but depreciated.\n    562 # It is the only case where we need to actually drop coordinates here (multi-index levels)\n    563 # TODO: remove when removing PandasMultiIndex's dimension coordinate.\n    564 self._drop_coords(self._names - coords_to_align._names)\n--&gt; 566 self._update_coords(coords, indexes)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/xarray/core/coordinates.py:842, in DataArrayCoordinates._update_coords(self, coords, indexes)\n    840 coords_plus_data = coords.copy()\n    841 coords_plus_data[_THIS_ARRAY] = self._data.variable\n--&gt; 842 dims = calculate_dimensions(coords_plus_data)\n    843 if not set(dims) &lt;= set(self.dims):\n    844     raise ValueError(\n    845         \"cannot add coordinates with new dimensions to a DataArray\"\n    846     )\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/xarray/core/variable.py:3008, in calculate_dimensions(variables)\n   3006             last_used[dim] = k\n   3007         elif dims[dim] != size:\n-&gt; 3008             raise ValueError(\n   3009                 f\"conflicting sizes for dimension {dim!r}: \"\n   3010                 f\"length {size} on {k!r} and length {dims[dim]} on {last_used!r}\"\n   3011             )\n   3012 return dims\n\nValueError: conflicting sizes for dimension 'v_dim': length 50 on &lt;this-array&gt; and length 3 on {'time': 'time', 'v_dim': 'v_dim'}",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Time resolution effect (data)"
    ]
  },
  {
    "objectID": "notebooks/analysis/11_ts_analysis.html",
    "href": "notebooks/analysis/11_ts_analysis.html",
    "title": "Time resolution effect Analysis",
    "section": "",
    "text": "Code\nimport polars as pl\nimport polars.selectors as cs\n\n\n::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=2}\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n%load_ext rpy2.ipython\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nR[write to console]: \nAttaching package: ‘dplyr’\n\n\nR[write to console]: The following objects are masked from ‘package:stats’:\n\n    filter, lag\n\n\nR[write to console]: The following objects are masked from ‘package:base’:\n\n    intersect, setdiff, setequal, union\n\n\nR[write to console]: \nAttaching package: ‘scales’\n\n\nR[write to console]: The following object is masked from ‘package:purrr’:\n\n    discard\n\n\n\n:::\n\n\nCode\nts = 1.00 # unit: seconds\ntau = 60 # unit: seconds\ndata_dir = \"../data\"\nformat = \"arrow\"\n\n\n\n\nCode\nparameters = ['j0_k', 'j0_k_norm', 'L_k', 'L_k_norm']\n\njuno_fit = pl.read_ipc(f\"{data_dir}/05_reporting/events.JNO.fit.ts_{ts:.2f}s_tau_{tau}s.arrow\").with_columns(\n    pl.col(parameters).abs(),\n).with_columns(cs.numeric().cast(pl.Float64)).drop_nulls().filter(\n    pl.col(\"d_star\") &lt; 100,  # exclude extreme values\n    pl.col('j0_k') &lt; 100\n).with_columns(pl.col('count').cast(pl.UInt32)).lazy()\n\n\n\n---------------------------------------------------------------------------\nColumnNotFoundError                       Traceback (most recent call last)\nCell In[11], line 8\n      1 parameters = ['j0_k', 'j0_k_norm', 'L_k', 'L_k_norm']\n      3 juno_fit = pl.read_ipc(f\"{data_dir}/05_reporting/events.JNO.fit.ts_{ts:.2f}s_tau_{tau}s.arrow\").with_columns(\n      4     pl.col(parameters).abs(),\n      5 ).with_columns(cs.numeric().cast(pl.Float64)).drop_nulls().filter(\n      6     pl.col(\"d_star\") &lt; 100,  # exclude extreme values\n      7     pl.col('j0_k') &lt; 100\n----&gt; 8 ).with_columns(pl.col('count').cast(pl.UInt32)).lazy()\n\nFile ~/micromamba/envs/juno/lib/python3.10/site-packages/polars/dataframe/frame.py:8315, in DataFrame.with_columns(self, *exprs, **named_exprs)\n   8169 def with_columns(\n   8170     self,\n   8171     *exprs: IntoExpr | Iterable[IntoExpr],\n   8172     **named_exprs: IntoExpr,\n   8173 ) -&gt; DataFrame:\n   8174     \"\"\"\n   8175     Add columns to this DataFrame.\n   8176 \n   (...)\n   8313     └─────┴──────┴─────────────┘\n   8314     \"\"\"\n-&gt; 8315     return self.lazy().with_columns(*exprs, **named_exprs).collect(_eager=True)\n\nFile ~/micromamba/envs/juno/lib/python3.10/site-packages/polars/lazyframe/frame.py:1940, in LazyFrame.collect(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, no_optimization, streaming, background, _eager)\n   1937 if background:\n   1938     return InProcessQuery(ldf.collect_concurrently())\n-&gt; 1940 return wrap_df(ldf.collect())\n\nColumnNotFoundError: count\n\nError originated just after this operation:\nDF [\"time\", \"tstart\", \"tstop\", \"t.d_end\"]; PROJECT */92 COLUMNS; SELECTION: \"None\"\n\n\n\n\n\nCode\nparameters = ['j0', 'j0_norm', 'j0_k', 'j0_k_norm', 'L_mn', 'L_k', 'L_mn_norm', 'L_k_norm']\n\nwind_fit = pl.read_ipc(f\"{data_dir}/05_reporting/events.Wind.ts_{ts}s_tau_{tau}s.arrow\").with_columns(\n    pl.col(parameters).abs(),\n).with_columns(cs.numeric().cast(pl.Float64)).drop_nulls().filter(\n    pl.col(\"d_star\") &lt; 100,  # exclude extreme values\n    pl.col('j0') &lt; 100\n).with_columns(pl.col('count').cast(pl.UInt32)).lazy()\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n/var/folders/tg/rfd0nr_970s3mv1fspgvkkxm0000gn/T/ipykernel_31903/3242119788.py in ?()\n      1 parameters = ['j0', 'j0_norm', 'j0_k', 'j0_k_norm', 'L_mn', 'L_k', 'L_mn_norm', 'L_k_norm']\n      2 \n----&gt; 3 wind_fit = pl.read_ipc(f\"{data_dir}/05_reporting/events.Wind.ts_{ts}s_tau_{tau}s.arrow\").with_columns(\n      4     pl.col(parameters).abs(),\n      5 ).with_columns(cs.numeric().cast(pl.Float64)).drop_nulls().filter(\n      6     pl.col(\"d_star\") &lt; 100,  # exclude extreme values\n\n~/micromamba/envs/juno/lib/python3.10/site-packages/polars/utils/deprecation.py in ?(*args, **kwargs)\n    132         def wrapper(*args: P.args, **kwargs: P.kwargs) -&gt; T:\n    133             _rename_keyword_argument(\n    134                 old_name, new_name, kwargs, function.__name__, version\n    135             )\n--&gt; 136             return function(*args, **kwargs)\n\n~/micromamba/envs/juno/lib/python3.10/site-packages/polars/utils/deprecation.py in ?(*args, **kwargs)\n    132         def wrapper(*args: P.args, **kwargs: P.kwargs) -&gt; T:\n    133             _rename_keyword_argument(\n    134                 old_name, new_name, kwargs, function.__name__, version\n    135             )\n--&gt; 136             return function(*args, **kwargs)\n\n~/micromamba/envs/juno/lib/python3.10/site-packages/polars/io/ipc/functions.py in ?(source, columns, n_rows, use_pyarrow, memory_map, storage_options, row_index_name, row_index_offset, rechunk)\n     99             if n_rows is not None:\n    100                 df = df.slice(0, n_rows)\n    101             return df\n    102 \n--&gt; 103         return pl.DataFrame._read_ipc(\n    104             data,\n    105             columns=columns,\n    106             n_rows=n_rows,\n\n~/micromamba/envs/juno/lib/python3.10/site-packages/polars/dataframe/frame.py in ?(cls, source, columns, n_rows, row_index_name, row_index_offset, rechunk, memory_map)\n    964             return cls._from_pydf(df._df)\n    965 \n    966         projection, columns = handle_projection_columns(columns)\n    967         self = cls.__new__(cls)\n--&gt; 968         self._df = PyDataFrame.read_ipc(\n    969             source,\n    970             columns,\n    971             projection,\n\nFileNotFoundError: No such file or directory (os error 2): ../data/05_reporting/events.Wind.ts_1.0s_tau_60s.arrow\n\n\n\n\n\nCode\ndef load_events(name: str, ts: float, tau: float, method ='derivative') -&gt; pl.DataFrame:\n    if method == 'derivative':\n        format = 'parquet'\n        filepath = f\"{data_dir}/08_reporting/events/l1/{name}_ts_{ts}s_tau_{tau}s.{format}\"\n        df = pl.scan_parquet(filepath)\n    elif method == 'fit':\n        format = 'arrow'\n        filepath = f\"{data_dir}/05_reporting/events.{name}.ts_{ts:.2f}s_tau_{tau}s.{format}\"\n        df = pl.read_ipc(filepath).lazy()\n    return df.with_columns(\n        pl.col(parameters).abs(),\n        sat = pl.lit(name),\n        ts = pl.lit(f'{ts}s'),\n        method = pl.lit(method),\n        ts_method = pl.lit(f'{ts}s {method}'),\n        label = pl.lit(f'{name} {ts}s {method}')\n    ).with_columns(cs.numeric().cast(pl.Float64))\n\n\n\n\nCode\nwind_ts_009_all = load_events('Wind', 0.09, 60)\nwind_ts_01_all = load_events('Wind', 0.1, 60)\nwind_ts_02_all = load_events('Wind', 0.2, 60)\nwind_ts_05_all = load_events('Wind', 0.5, 60)\nwind_ts_1_all = load_events('Wind', 1, 60)\n\njuno_ts_1_all = load_events('JNO', 1, 60)\njuno_ts_1_fit = load_events('JNO', 1, 60, method='fit')\n\n\nwind_ts_01_fit = load_events('Wind', 0.09, 60, method='fit')\nwind_ts_02_fit = load_events('Wind', 0.2, 60, method='fit')\nwind_ts_05_fit = load_events('Wind', 0.5, 60, method='fit')\nwind_ts_1_fit = load_events('Wind', 1, 60, method='fit')\nwind_ts_2_fit = load_events('Wind', 2, 60, method='fit')\n\nwind_fit_dfs = [wind_ts_01_fit, wind_ts_02_fit, wind_ts_05_fit, wind_ts_1_fit, wind_ts_2_fit]\n\n\n\n\nCode\nwind_ts_01_fit\n\n\nnaive plan: (run LazyFrame.explain(optimized=True) to see the optimized plan)\n    \n     WITH_COLUMNS: [col(\"count\").strict_cast(Float64), col(\"B_std\").strict_cast(Float64), col(\"B_mean\").strict_cast(Float64), col(\"dB_vec\").strict_cast(Float64), col(\"index_diff\").strict_cast(Float64), col(\"index_std\").strict_cast(Float64), col(\"index_fluctuation\").strict_cast(Float64), col(\"B.after\").strict_cast(Float64), col(\"B.before\").strict_cast(Float64), col(\"b_mag\").strict_cast(Float64), col(\"b_n\").strict_cast(Float64), col(\"bn_over_b\").strict_cast(Float64), col(\"d_star\").strict_cast(Float64), col(\"db_mag\").strict_cast(Float64), col(\"db_over_b\").strict_cast(Float64), col(\"db_over_b_max\").strict_cast(Float64), col(\"fit.stat.chisqr\").strict_cast(Float64), col(\"fit.stat.rsquared\").strict_cast(Float64), col(\"fit.vars.amplitude\").strict_cast(Float64), col(\"fit.vars.c\").strict_cast(Float64), col(\"fit.vars.sigma\").strict_cast(Float64), col(\"rotation_angle\").strict_cast(Float64), col(\"dB_x\").strict_cast(Float64), col(\"dB_y\").strict_cast(Float64), col(\"dB_z\").strict_cast(Float64), col(\"dB_lmn_x\").strict_cast(Float64), col(\"dB_lmn_y\").strict_cast(Float64), col(\"dB_lmn_z\").strict_cast(Float64), col(\"k_x\").strict_cast(Float64), col(\"k_y\").strict_cast(Float64), col(\"k_z\").strict_cast(Float64), col(\"Vl_x\").strict_cast(Float64), col(\"Vl_y\").strict_cast(Float64), col(\"Vl_z\").strict_cast(Float64), col(\"Vn_x\").strict_cast(Float64), col(\"Vn_y\").strict_cast(Float64), col(\"Vn_z\").strict_cast(Float64), col(\"plasma_density\").strict_cast(Float64), col(\"VX (GSM)\").strict_cast(Float64), col(\"VY (GSM)\").strict_cast(Float64), col(\"VZ (GSM)\").strict_cast(Float64), col(\"SW Vth\").strict_cast(Float64), col(\"plasma_speed\").strict_cast(Float64), col(\"plasma_temperature\").strict_cast(Float64), col(\"n.before\").strict_cast(Float64), col(\"T.before\").strict_cast(Float64), col(\"v.ion.before\").strict_cast(Float64), col(\"n.after\").strict_cast(Float64), col(\"T.after\").strict_cast(Float64), col(\"v.ion.after\").strict_cast(Float64), col(\"v_l\").strict_cast(Float64), col(\"v_n\").strict_cast(Float64), col(\"v_k\").strict_cast(Float64), col(\"v_mn\").strict_cast(Float64), col(\"L_n\").strict_cast(Float64), col(\"L_mn\").strict_cast(Float64), col(\"L_k\").strict_cast(Float64), col(\"j0\").strict_cast(Float64), col(\"j0_k\").strict_cast(Float64), col(\"ion_inertial_length\").strict_cast(Float64), col(\"Alfven_speed\").strict_cast(Float64), col(\"j_Alfven\").strict_cast(Float64), col(\"L_n_norm\").strict_cast(Float64), col(\"L_mn_norm\").strict_cast(Float64), col(\"L_k_norm\").strict_cast(Float64), col(\"j0_norm\").strict_cast(Float64), col(\"j0_k_norm\").strict_cast(Float64), col(\"v.Alfven.before\").strict_cast(Float64), col(\"v.Alfven.after\").strict_cast(Float64), col(\"n.change\").strict_cast(Float64), col(\"v.ion.change\").strict_cast(Float64), col(\"T.change\").strict_cast(Float64), col(\"B.change\").strict_cast(Float64), col(\"v.Alfven.change\").strict_cast(Float64)]   WITH_COLUMNS:   [col(\"j0\").abs(), col(\"j0_norm\").abs(), col(\"j0_k\").abs(), col(\"j0_k_norm\").abs(), col(\"L_mn\").abs(), col(\"L_k\").abs(), col(\"L_mn_norm\").abs(), col(\"L_k_norm\").abs(), String(Wind).alias(\"sat\"), String(0.09s).alias(\"ts\"), String(fit).alias(\"method\"), String(0.09s fit).alias(\"ts_method\"), String(Wind 0.09s fit).alias(\"label\")]    DF [\"time\", \"tstart\", \"tstop\", \"d_tstart\"]; PROJECT */83 COLUMNS; SELECTION: \"None\"\n\n\n\n\nCode\ntime_filter = pl.col('time').dt.year()==2016\ndfs = [juno_ts_1_all, wind_ts_1_all, juno_ts_1_fit, wind_ts_009_all, wind_ts_01_all, wind_ts_02_all, wind_ts_05_all] + wind_fit_dfs\n\ndf = pl.concat([_.filter(time_filter) for _ in dfs], how='diagonal').collect()\n%R -i df -c conv_pl\n\n\n\n\nCode\n%%R\nfilename &lt;- \"ts_effect\"\nfilename &lt;- \"ts_effect_new\"\n\n\n\n\nCode\n%%R\n# sort color with 'JUNO 1s' first\nplot_ts_effect &lt;- function(df, labels=NULL, color =\"label\", L_mn_norm_lim = c(0,60), j_norm_lim = c(0,0.8), legend_title = NULL) {\n\n  # if labels is not null, select df with labels and reorder labels\n  if (!is.null(labels)){\n    df &lt;- df %&gt;%\n      filter(label %in% labels) %&gt;%\n      mutate(label = factor(label, levels = labels))\n  }\n\n  \n  add &lt;- \"mean\" # this seems to be computed before the limits are set\n  add &lt;- NULL\n  \n  common_custom &lt;- scale_color_okabeito(palette = \"black_first\")\n\n  x &lt;- \"L_mn\"\n  x_lim &lt;- c(0,7500)\n  p1 &lt;- ggdensity(df, x = x, color = color, add = add, alpha = 0) + xlim(x_lim) + common_custom\n\n  x &lt;- \"L_mn_norm\"\n  x_lab &lt;- expression(paste(\"Normalized Thickness (\", d[i], \")\"))\n  p2 &lt;- ggdensity(df, x = x, color = color, add = add, alpha = 0) + xlim(L_mn_norm_lim) + common_custom + labs(x=x_lab)\n\n  x &lt;- \"j0\"\n  x_lim &lt;- c(0,20)\n  p3 &lt;- ggdensity(df, x = x, color = color, add = add, alpha = 0) + xlim(x_lim) + common_custom\n\n  x &lt;- \"j0_norm\"\n  x_lab &lt;- expression(paste(\"Normalized Current Intensity (\", J[A], \")\"))\n  p4 &lt;- ggdensity(df, x = x, color = color, add = add, alpha = 0) + xlim(j_norm_lim) + common_custom + labs(x=x_lab)\n\n\n  output &lt;- list(p2, p4)\n  # change the legend title for each plot\n  if (!is.null(legend_title)){\n    for (i in 1:length(output)){\n      output[[i]] &lt;- ggpar(output[[i]], legend.title = legend_title)\n    }\n  }\n\n  purrr::reduce(output, `+`) + plot_layout(guides = 'collect', nrow=2) &\n    theme(legend.position='top')\n}\n\n\n\n\n\nCode\n%%R\nfilename &lt;- \"ts_method/all\"\np &lt;- plot_ts_effect(df)\nsave_plot(filename)\np\n\n\nR[write to console]: In addition: \nR[write to console]: Warning messages:\n\nR[write to console]: 1: Removed 12428 rows containing non-finite values (`stat_density()`). \n\nR[write to console]: 2: Removed 1 rows containing missing values (`geom_vline()`). \n\nR[write to console]: 3: Removed 719 rows containing non-finite values (`stat_density()`). \n\n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R\n# filter df with sat = 'Wind'\np1 &lt;- df %&gt;%\n  filter(sat == 'Wind') %&gt;%\n    plot_ts_effect(color = \"ts_method\")\n\nfilename &lt;- \"ts_method/wind\"\nsave_plot(filename)\n\np2 &lt;- df %&gt;%\n  filter(sat == 'JNO') %&gt;%\n    plot_ts_effect(color = \"ts_method\")\n    \nfilename &lt;- \"ts_method/juno\"\nsave_plot(filename)\n\np1 | p2\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nR[write to console]: In addition: \nR[write to console]: Warning messages:\n\nR[write to console]: 1: Removed 12233 rows containing non-finite values (`stat_density()`). \n\nR[write to console]: 2: Removed 658 rows containing non-finite values (`stat_density()`). \n\nR[write to console]: 3: Removed 195 rows containing non-finite values (`stat_density()`). \n\nR[write to console]: 4: Removed 61 rows containing non-finite values (`stat_density()`). \n\n\n\nIn addition: Warning messages:\n1: Removed 12233 rows containing non-finite values (`stat_density()`). \n2: Removed 658 rows containing non-finite values (`stat_density()`). \n3: Removed 12233 rows containing non-finite values (`stat_density()`). \n4: Removed 658 rows containing non-finite values (`stat_density()`). \n5: Removed 195 rows containing non-finite values (`stat_density()`). \n6: Removed 61 rows containing non-finite values (`stat_density()`). \n7: Removed 195 rows containing non-finite values (`stat_density()`). \n8: Removed 61 rows containing non-finite values (`stat_density()`). \n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R\np &lt;- df %&gt;%\n  filter(method == 'fit', sat == 'Wind') %&gt;%\n    plot_ts_effect(color = 'ts', L_mn_norm_lim = c(0,300), j_norm_lim = c(0,0.6), legend_title = \"time resolution\")\n\np &lt;- p + plot_annotation(title = \"Wind - Fit\")\n\nfilename &lt;- \"method/time_res_effect\"\nsave_plot(filename)\np\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nR[write to console]: In addition: \nR[write to console]: Warning messages:\n\nR[write to console]: 1: Removed 4927 rows containing non-finite values (`stat_density()`). \n\nR[write to console]: 2: Removed 3013 rows containing non-finite values (`stat_density()`). \n\n\n\nIn addition: Warning messages:\n1: Removed 4927 rows containing non-finite values (`stat_density()`). \n2: Removed 3013 rows containing non-finite values (`stat_density()`). \n3: Removed 4927 rows containing non-finite values (`stat_density()`). \n4: Removed 3013 rows containing non-finite values (`stat_density()`). \n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R\np1 &lt;- df %&gt;%\n  filter(method == 'derivative') %&gt;%\n    plot_ts_effect\n \nfilename &lt;- \"ts_method/derivative\"\nsave_plot(filename)\np1\n\np2 &lt;- df %&gt;%\n  filter(method == 'fit') %&gt;%\n    plot_ts_effect(L_mn_norm_lim = c(0,300), j_norm_lim = c(0,0.6))\n\nfilename &lt;- \"ts_method/fit\"\nsave_plot(filename)\np2\n\np1 | p2\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nR[write to console]: In addition: \nR[write to console]: Warning messages:\n\nR[write to console]: 1: Removed 334 rows containing non-finite values (`stat_density()`). \n\nR[write to console]: 2: Removed 502 rows containing non-finite values (`stat_density()`). \n\nR[write to console]: 3: Removed 335 rows containing non-finite values (`stat_density()`). \n\nR[write to console]: 4: Removed 313 rows containing non-finite values (`stat_density()`). \n\n\n\nIn addition: Warning messages:\n1: Removed 334 rows containing non-finite values (`stat_density()`). \n2: Removed 502 rows containing non-finite values (`stat_density()`). \n3: Removed 334 rows containing non-finite values (`stat_density()`). \n4: Removed 502 rows containing non-finite values (`stat_density()`). \n5: Removed 335 rows containing non-finite values (`stat_density()`). \n6: Removed 313 rows containing non-finite values (`stat_density()`). \n7: Removed 335 rows containing non-finite values (`stat_density()`). \n8: Removed 313 rows containing non-finite values (`stat_density()`).",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Time resolution effect Analysis"
    ]
  },
  {
    "objectID": "notebooks/analysis/03_map.html",
    "href": "notebooks/analysis/03_map.html",
    "title": "Map of thickness and current intensity",
    "section": "",
    "text": "Bin the data and fit the shape does not work. Using 2d gaussian kernel density estimation instead.\n\n\nCode\ndef bin_df(df: pl.DataFrame, col_to_bin, bins=10):\n    binned_col = f\"{col_to_bin}_bin\"\n    \n    return (\n        df.with_columns(\n            pl.col(col_to_bin).qcut(bins).alias(binned_col),\n        )\n        .group_by(binned_col)\n        .agg(cs.numeric().median(), pl.count().alias(\"bin_count\"))\n        .drop(binned_col)\n    )\n\ncol_to_bin=\"L_mn_norm_log\"\n# col_to_bin=\"j0_norm_log\"\n\nall_events_l1_L_binned = pl.concat(\n    [\n        data.pipe(bin_df, col_to_bin=col_to_bin, bins=64).with_columns(sat= pl.lit(name))\n        for name, data in all_events_l1.group_by(\"sat\")\n    ]\n)\n\njno_events_l1_L_binned = pl.concat(\n    [\n        data.pipe(bin_df, col_to_bin=col_to_bin, bins=64).with_columns(sat= pl.lit(name))\n        for name, data in jno_candidates_l1.group_by(\"r_bin\")\n    ]\n)\n%R -i all_events_l1_L_binned -c conv_pl\n\n\n\n\nCode\ndf = all_events_l1.filter(pl.col('L_mn_norm_log').is_not_nan())\n# df = jno_candidates_l1.filter(pl.col('L_mn_norm_log').is_not_nan())\n%R -i df -c conv_pl\n\n\n\n\nCode\nimport lmfit\nimport numpy as np\n\ndef gaussian2d(x, y, amplitude=1., centerx=0., centery=0., sigmax=1., sigmay=1.,\n                 rotation=0, A0=0.):\n    \"\"\"Return a two dimensional lorentzian.\n\n    The maximum of the peak occurs at ``centerx`` and ``centery``\n    with widths ``sigmax`` and ``sigmay`` in the x and y directions\n    respectively. The peak can be rotated by choosing the value of ``rotation``\n    in radians.\n    \"\"\"\n    xp = (x - centerx)*np.cos(rotation) - (y - centery)*np.sin(rotation)\n    yp = (x - centerx)*np.sin(rotation) + (y - centery)*np.cos(rotation)\n    R = (xp/sigmax)**2 + (yp/sigmay)**2\n\n    return A0 + amplitude * np.exp(-R/2)\n\nmodel = lmfit.Model(gaussian2d, independent_vars=['x', 'y'])\n# params = model.make_params(amplitude=10, centerx=x[np.argmax(z)], centery=y[np.argmax(z)])\n\n\n\n\nCode\n%%R\nfit_gaussian_2D_pdf(df)\n\n\n\n\nCode\n%%R\nlibrary(purrr)\nlibrary(tidyr)\nfit_gaussian_2D_pdf &lt;- function(data) {\n  kde_result &lt;- MASS::kde2d(data$L_mn_norm_log, data$j0_norm_log)\n  x_values &lt;- rep(kde_result$x, each = length(kde_result$y))\n  y_values &lt;- rep(kde_result$y, length(kde_result$x))\n  response &lt;- as.vector(kde_result$z)\n\n  density &lt;- data.frame(X_values = x_values, Y_values = y_values, response = response)\n  model &lt;- fit_gaussian_2D(density)\n  return(model)\n}\n\nresults &lt;- df %&gt;% \n  # group_by(r_bin) %&gt;% \n  nest() %&gt;% \n  mutate(fitted = map(data, fit_gaussian_2D_pdf))\n\n\n\n\nCode\n%%R -w 1000 -h 500\n# Creating a list of layers for the binned data\n# model &lt;- lm(j0_norm_log ~ L_mn_norm_log, data = all_events_l1_L_binned)\n# slope &lt;- coef(model)[2]\n\nbinned_layer &lt;- list(\n  geom_line(data = all_events_l1_L_binned, color = 'blue'),\n  geom_point(data = all_events_l1_L_binned, color = 'blue'), \n  geom_smooth(data = all_events_l1_L_binned, method = \"glm\", color = 'red')\n)\n\n\n# Plot creation\np &lt;- ggplot(mapping = aes(x = L_mn_norm_log, y = j0_norm_log)) +\n  geom_density_2d(data = all_events_l1) +\n  # stat_density_2d(data = all_events_l1, aes(fill = after_stat(density)), geom = \"raster\", contour = FALSE) +\n  binned_layer +\n  facet_wrap(~ sat, scales = \"free\")\n\n  \n# Print the plot\nprint(p)\n\n\n\n\nCode\n%%R\np &lt;- ggplot() +\n  geom_point(data = all_events_l1, aes(x = L_mn_norm_log, y = j0_norm_log)) +\n  binned_layer +\n  facet_wrap(~ sat, scales = \"free\")\n\nprint(p)\n\n\n\n\nCode\n%%R\n# Fit a linear model to the log-transformed data\nlm_fit &lt;- lm(j0_norm_log ~ L_mn_norm_log, data = all_events_l1)\n\n# Extract the coefficients\nintercept &lt;- coef(lm_fit)[1]\nslope &lt;- coef(lm_fit)[2]\n\n# Create a scatter plot with the log-log transformation\np &lt;- ggplot(all_events_l1, aes(x = L_mn_norm_log, y = j0_norm_log)) +\n  geom_point() + # Add the scatter points\n  geom_abline(intercept = intercept, slope = slope, color = 'blue', size = 1) + # Add the fitted line\n  facet_wrap(~ sat, scales = \"free\") + # Facet by 'sat'\n  labs(x = \"Log10(L_mn_norm)\", y = \"Log10(j0_norm)\") # Label axes\n\nprint(p)\n\n\n\n\nCode\n%%R\n# Plot creation\np &lt;- ggplot(all_events_l1_L_binned, aes(x = L_mn_norm_log, y = j0_norm_log)) +\n    geom_line(color = 'blue') +\n    geom_point(color = 'blue') +\n    geom_smooth(method = \"glm\", color = 'red') +\n    facet_wrap(~ sat, scales = \"free\") +\n    stat_regline_equation()\n\n  \n# Print the plot\nprint(p)\n\n\n\n\nCode\n%%R -i jno_events_l1_L_binned -c conv_pl\n# Plot creation\np &lt;- ggplot(jno_events_l1_L_binned, aes(x = L_mn_norm_log, y = j0_norm_log)) +\n    geom_line(color = 'blue') +\n    geom_point(color = 'blue') +\n    geom_smooth(method = \"glm\", color = 'red') +\n    facet_wrap(~ r_bin, scales = \"free\") +\n    stat_regline_equation()\n\n  \n# Print the plot\nprint(p)\n\n\n\n\nCode\n%%R -i jno_candidates_l1 -c conv_pl\n\np &lt;- ggplot(jno_candidates_l1, aes(x = L_mn_norm, y = j0_norm)) +\n  stat_density_2d(aes(fill = ..density..), geom = \"raster\", contour = FALSE) +\n  facet_wrap(~ r_bin, nrow = length(unique(jno_candidates_l1$r_bin))) +\n  scale_x_log10() + \n  scale_y_log10() +\n  labs(fill = \"Density\")\n\n\nprint(p)",
    "crumbs": [
      "Home",
      "Notebooks",
      "Analysis",
      "Map of thickness and current intensity"
    ]
  },
  {
    "objectID": "notebooks/archive/04_properties.html",
    "href": "notebooks/archive/04_properties.html",
    "title": "SWD Properties",
    "section": "",
    "text": "::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=36}\n\nCode\nimport polars as pl\nfrom loguru import logger\n\n:::\n::: {#cell-3 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=37}\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n:::\nConnect python with R kernel\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n%load_ext rpy2.ipython\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython",
    "crumbs": [
      "Home",
      "Notebooks",
      "Archive",
      "SWD Properties"
    ]
  },
  {
    "objectID": "notebooks/archive/04_properties.html#setup",
    "href": "notebooks/archive/04_properties.html#setup",
    "title": "SWD Properties",
    "section": "",
    "text": "::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=36}\n\nCode\nimport polars as pl\nfrom loguru import logger\n\n:::\n::: {#cell-3 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=37}\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n:::\nConnect python with R kernel\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n%load_ext rpy2.ipython\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython",
    "crumbs": [
      "Home",
      "Notebooks",
      "Archive",
      "SWD Properties"
    ]
  },
  {
    "objectID": "notebooks/archive/04_properties.html#processing-datasets",
    "href": "notebooks/archive/04_properties.html#processing-datasets",
    "title": "SWD Properties",
    "section": "Processing datasets",
    "text": "Processing datasets\n\n\nCode\nts = 1 # unit: seconds\ntau = 60 # unit: seconds\ndata_dir = \"../data\"\nformat = \"arrow\"\n\n\n\n\nCode\nfrom utils import load_events, PARAMETERS\n\nwind_df = load_events('Wind', 1, 60, 'fit')\nsta_df = load_events('STA', 1, 60, 'fit')\njno_df = load_events('JNO', 1, 60, 'fit')\njno_df_tau_300 = load_events('JNO', 1, 300, 'fit')\njno_df_8hz = load_events('JNO', 0.125, 60, 'fit')\njno_df_8hz_tau_20 = load_events('JNO', 0.125, 20, 'fit')\njno_df_8hz_tau_20_der = load_events('JNO', 0.125, 20, 'derivative')\n\nother_events = pl.concat([wind_df, sta_df], how='diagonal')\n\nlogger.info(f\"Columns: {jno_df.columns}\")\n\n\n2024-02-18 21:55:57.510 | INFO     | __main__:&lt;module&gt;:13 - Columns: ['time', 'tstart', 'tstop', 'd_tstart', 'd_tstop', 'd_time', 'time_before', 'time_after', 'count', 'B_std', 'B_mean', 'dB_vec', 'index_diff', 'index_std', 'index_fluctuation', 'B.after', 'B.before', 'b_mag', 'b_n', 'bn_over_b', 'd_star', 'db_mag', 'db_over_b', 'db_over_b_max', 'fit.stat.chisqr', 'fit.stat.rsquared', 'fit.vars.amplitude', 'fit.vars.c', 'fit.vars.sigma', 'rotation_angle', 'dB_x', 'dB_y', 'dB_z', 'dB_lmn_x', 'dB_lmn_y', 'dB_lmn_z', 'k_x', 'k_y', 'k_z', 'Vl_x', 'Vl_y', 'Vl_z', 'Vn_x', 'Vn_y', 'Vn_z', 'duration', 'radial_distance', 'plasma_density', 'plasma_temperature', 'model_b_r', 'model_b_t', 'model_b_n', 'v_x', 'v_y', 'v_z', 'plasma_speed', 'B_background_x', 'B_background_y', 'B_background_z', 'v_x_before', 'v_y_before', 'v_z_before', 'n.before', 'v.ion.before', 'plasma_temperature_before', 'v_x_after', 'v_y_after', 'v_z_after', 'n.after', 'v.ion.after', 'plasma_temperature_after', 'v_l', 'v_n', 'v_k', 'v_mn', 'L_k', 'j0_k', 'ion_inertial_length', 'Alfven_speed', 'j_Alfven', 'L_k_norm', 'j0_k_norm', 'v.ion.before.l', 'v.ion.after.l', 'B.vec.before.l', 'B.vec.before.m', 'B.vec.before.n', 'B.vec.after.l', 'B.vec.after.m', 'B.vec.after.n', 'v.Alfven.before', 'v.Alfven.after', 'v.Alfven.before.l', 'v.Alfven.after.l', 'n.change', 'v.ion.change', 'v.ion.change.l', 'B.change', 'v.Alfven.change', 'v.Alfven.change.l', 'sat', 'ts', 'method', 'ts_method', 'label', 'index_d_diff']\n\n\nSome extreme values are present in the data. We will remove them.\n\n\nCode\nDISPLAY_VARS = ['time', 'sat'] + PARAMETERS\n\ndef check_candidates(df):\n    if isinstance(df, pl.LazyFrame):\n        df = df.collect()\n    logger.info(df.columns)\n    return df.select(DISPLAY_VARS).describe()\n\ncheck_candidates(jno_df)\n\n\n2024-02-18 21:55:57.523 | INFO     | __main__:check_candidates:6 - ['time', 'tstart', 'tstop', 'd_tstart', 'd_tstop', 'd_time', 'time_before', 'time_after', 'count', 'B_std', 'B_mean', 'dB_vec', 'index_diff', 'index_std', 'index_fluctuation', 'B.after', 'B.before', 'b_mag', 'b_n', 'bn_over_b', 'd_star', 'db_mag', 'db_over_b', 'db_over_b_max', 'fit.stat.chisqr', 'fit.stat.rsquared', 'fit.vars.amplitude', 'fit.vars.c', 'fit.vars.sigma', 'rotation_angle', 'dB_x', 'dB_y', 'dB_z', 'dB_lmn_x', 'dB_lmn_y', 'dB_lmn_z', 'k_x', 'k_y', 'k_z', 'Vl_x', 'Vl_y', 'Vl_z', 'Vn_x', 'Vn_y', 'Vn_z', 'duration', 'radial_distance', 'plasma_density', 'plasma_temperature', 'model_b_r', 'model_b_t', 'model_b_n', 'v_x', 'v_y', 'v_z', 'plasma_speed', 'B_background_x', 'B_background_y', 'B_background_z', 'v_x_before', 'v_y_before', 'v_z_before', 'n.before', 'v.ion.before', 'plasma_temperature_before', 'v_x_after', 'v_y_after', 'v_z_after', 'n.after', 'v.ion.after', 'plasma_temperature_after', 'v_l', 'v_n', 'v_k', 'v_mn', 'L_k', 'j0_k', 'ion_inertial_length', 'Alfven_speed', 'j_Alfven', 'L_k_norm', 'j0_k_norm', 'v.ion.before.l', 'v.ion.after.l', 'B.vec.before.l', 'B.vec.before.m', 'B.vec.before.n', 'B.vec.after.l', 'B.vec.after.m', 'B.vec.after.n', 'v.Alfven.before', 'v.Alfven.after', 'v.Alfven.before.l', 'v.Alfven.after.l', 'n.change', 'v.ion.change', 'v.ion.change.l', 'B.change', 'v.Alfven.change', 'v.Alfven.change.l', 'sat', 'ts', 'method', 'ts_method', 'label', 'index_d_diff']\n\n\n\n\nshape: (9, 7)\n\n\n\nstatistic\ntime\nsat\nj0_k\nj0_k_norm\nL_k\nL_k_norm\n\n\nstr\nstr\nstr\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n\"63143\"\n\"63143\"\n63143.0\n63143.0\n63143.0\n63143.0\n\n\n\"null_count\"\n\"0\"\n\"0\"\n0.0\n0.0\n0.0\n0.0\n\n\n\"mean\"\n\"2013-05-14 11:…\nnull\n125.040783\n12.398706\n9410.339035\n45.121054\n\n\n\"std\"\nnull\nnull\n12236.23217\n1077.446348\n5438.269571\n34.72957\n\n\n\"min\"\n\"2011-08-25 16:…\n\"JNO\"\n0.000018\n0.000036\n1.187819\n0.001187\n\n\n\"25%\"\n\"2012-03-08 20:…\nnull\n0.124872\n0.018557\n5123.132655\n20.526554\n\n\n\"50%\"\n\"2013-03-05 15:…\nnull\n0.2963\n0.046239\n8643.437318\n36.506577\n\n\n\"75%\"\n\"2014-03-11 18:…\nnull\n0.748768\n0.108717\n12991.083881\n59.95483\n\n\n\"max\"\n\"2016-06-29 23:…\n\"JNO\"\n2.4550e6\n187435.626378\n42775.89897\n382.018872\n\n\n\n\n\n\n\n\n\nCode\ndef remove_extreme(df, cols = ['j0_k', 'L_k'], q = 0.96):\n    filter_conditions = [\n        pl.col(col) &lt; pl.col(col).quantile(q) for col in cols\n    ] + [\n        pl.col(col) &gt; pl.col(col).quantile(1-q) for col in cols\n    ]\n    \n    return df.filter(filter_conditions)\n\ndef keep_good_fit(df: pl.DataFrame, rsquared = 0.95):\n    return df.filter(pl.col('fit.stat.rsquared') &gt; rsquared)\n\n# jno_df = jno_df.pipe(remove_extreme).pipe(keep_good_fit)\n# other_events = other_events.pipe(remove_extreme).pipe(keep_good_fit)\n# jno_df_tau_300 = jno_df_tau_300.pipe(remove_extreme).pipe(keep_good_fit)\n\njno_df = jno_df.pipe(keep_good_fit)\nother_events = other_events.pipe(keep_good_fit)\njno_df_tau_300 = jno_df_tau_300.pipe(keep_good_fit)\njno_df_8hz = jno_df_8hz.pipe(keep_good_fit)\njno_df_8hz_tau_20 = jno_df_8hz_tau_20.pipe(keep_good_fit)\njno_df_8hz_tau_20_der = jno_df_8hz_tau_20_der.filter(pl.col('len')==160, pl.col('index_d_diff')&gt;0.8)\n\n\n\n\nCode\nlogger.info(f\"Number of events &gt; 5AU: {jno_df.filter(pl.col('radial_distance') &gt; 4).height}\")\nlogger.info(f\"Number of events &gt; 5AU: {jno_df.filter(pl.col('radial_distance') &gt; 3, pl.col('radial_distance') &lt; 4).height}\")\nlogger.info(f\"Number of events &gt; 5AU: {jno_df_tau_300.filter(pl.col('radial_distance') &gt; 4).height}\")\n\n\n2024-02-18 21:56:00.091 | INFO     | __main__:&lt;module&gt;:1 - Number of events &gt; 5AU: 4207\n2024-02-18 21:56:00.093 | INFO     | __main__:&lt;module&gt;:2 - Number of events &gt; 5AU: 2033\n2024-02-18 21:56:00.094 | INFO     | __main__:&lt;module&gt;:3 - Number of events &gt; 5AU: 1035",
    "crumbs": [
      "Home",
      "Notebooks",
      "Archive",
      "SWD Properties"
    ]
  },
  {
    "objectID": "notebooks/archive/04_properties.html#plotting-in-r",
    "href": "notebooks/archive/04_properties.html#plotting-in-r",
    "title": "SWD Properties",
    "section": "Plotting in R",
    "text": "Plotting in R\n\n\nCode\n%R -i jno_df -c conv_pl\n%R -i other_events -c conv_pl\n%R -i jno_df_tau_300 -c conv_pl\n%R -i jno_df_8hz -c conv_pl\n%R -i jno_df_8hz_tau_20 -c conv_pl\n%R -i jno_df_8hz_tau_20_der -c conv_pl\n\n\n\n\nCode\n%%R\np1title &lt;- \"a) Juno\"\np2title &lt;- \"b) ARTEMIS, STEREO and Wind\"\nx_lab_r &lt;- \"Radial Distance (AU)\"\nx_lab_t &lt;- \"Time\"\n\ny_lab_t &lt;- \"Time Duration (s)\"\ny_lab_L &lt;- \"Log Thickness (km)\"\ny_lab_L_norm &lt;- expression(Log~Thickness~(d[i]))\n\nlayout &lt;- plot_layout(ncol = 2, byrow = FALSE, guides = \"collect\")\n\n\n\n\nCode\n%%R -w 350 -h 350 -u mm\nadd_mode &lt;- TRUE\nx_bins &lt;- 5\ny_bins &lt;- 10\nlog_y &lt;- FALSE\ny_lim_duration &lt;- c(0.1, 60)\n\ny_col &lt;- \"duration\"\nylab &lt;- y_lab_t\n\nx_col &lt;- \"radial_distance\"\n\np &lt;- plot_binned_data(jno_df, x_col = x_col, y_col = y_col, x_bins = x_bins, y_bins = y_bins, y_lim = y_lim_duration, log_y = log_y, add_mode=add_mode)\np1 &lt;- p + labs(x = NULL, y= ylab) + ggtitle(\"a) Juno\")\n\np &lt;- plot_binned_data(jno_df_8hz, x_col = x_col, y_col = y_col, x_bins = x_bins, y_bins = y_bins, y_lim = y_lim_duration, log_y = log_y, add_mode=add_mode)\np2 &lt;- p + labs(x = NULL, y= ylab) + ggtitle(\"b) Juno High Time Resolution\")\n\ny_lim_duration &lt;- c(0.1, 300)\np &lt;- plot_binned_data(jno_df_tau_300, x_col = x_col, y_col = y_col, x_bins = x_bins, y_bins = y_bins, y_lim = y_lim_duration, log_y = log_y, add_mode=add_mode)\np3 &lt;- p + labs(x = NULL, y= ylab) + ggtitle(\"c) Juno Long Tau\")\n\ny_lim_duration &lt;- c(0.1, 20)\np &lt;- plot_binned_data(jno_df_8hz_tau_20, x_col = x_col, y_col = y_col, x_bins = x_bins, y_bins = y_bins, y_lim = y_lim_duration, log_y = log_y, add_mode=add_mode)\np4 &lt;- p + labs(x = NULL, y= ylab) + ggtitle(\"d) Juno Low Tau and High TS\")\n\n# p &lt;- (p1 + p2) + plot_layout(ncol = 1, guides = \"collect\") & scale_fill_viridis_c(limits = c(0.01, 0.20), name=\"pdf\")\np &lt;- p1 + p2 + p3 + p4 + layout \nsave_plot(\"new/duration/duration_dist_ts_comparison\")\np\n\n\nSaving 13.8 x 13.8 in image\nSaving 13.8 x 13.8 in image\n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R -w 350 -h 350 -u mm\nadd_mode &lt;- TRUE\nx_bins &lt;- 5\ny_bins &lt;- 10\nlog_y &lt;- FALSE\ny_lim_duration &lt;- c(0.1, 10)\n\ny_col &lt;- \"duration\"\nylab &lt;- y_lab_t\n\nx_col &lt;- \"radial_distance\"\n\np &lt;- plot_binned_data(jno_df_8hz_tau_20_der, x_col = x_col, y_col = y_col, x_bins = x_bins, y_bins = y_bins, y_lim = y_lim_duration, log_y = log_y, add_mode=add_mode)\np1 &lt;- p + labs(x = NULL, y= ylab) + ggtitle(\"a) Juno Derivative Method\")\np1\n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R -w 350 -h 350 -u mm\n\np &lt;- plot_current_comparison(jno_df, other_events, p1title=\"a) Juno\")\nsave_plot(\"new/current/current_k_dist\")\np\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nSaving 13.8 x 13.8 in image\nSaving 13.8 x 13.8 in image\n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R -w 350 -h 350 -u mm\np &lt;- plot_current_comparison(jno_df_8hz_tau_20, other_events, p1title=\"a) Juno: High Time Resolution, Low Tau\")\nsave_plot(\"new/current/current_k_dist-8hz-tau_20\")\n\np &lt;- plot_current_comparison(jno_df_8hz, other_events, p1title=\"a) Juno: High Time Resolution\")\nsave_plot(\"new/current/current_k_dist-8hz\")\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nSaving 13.8 x 13.8 in image\nSaving 13.8 x 13.8 in image\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nSaving 13.8 x 13.8 in image\nSaving 13.8 x 13.8 in image\n\n\n\n\nCode\n%%R -w 350 -h 350 -u mm\n\np &lt;- plot_thickness_comparison(jno_df, other_events, p1title=\"a) Juno\")\nsave_plot(\"new/thickness/thickness_k_dist\")\np\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nSaving 13.8 x 13.8 in image\nSaving 13.8 x 13.8 in image\n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R -w 350 -h 350 -u mm\n\np1 &lt;- plot_q_and_qnorm_r_l(\n    jno_df_8hz_tau_20_der,\n    y_lim1=c(100, 6000), \n    y_lim2=c(0.1,20)\n)\n\np2 &lt;- plot_q_and_qnorm_r_j0(\n    jno_df_8hz_tau_20_der,\n    y_lim1 = c(0.1, 50),\n    y_lim2 = c(0.02, 10)\n)\n\n(p1 |  p2) + plot_annotation(\n  title = 'Juno: 8Hz, Tau 20s, Derivative Method',\n  theme = theme(plot.title = element_text(size = 18))\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R -w 350 -h 350 -u mm\n\np &lt;- plot_thickness_comparison(jno_df_8hz_tau_20, other_events, p1title=\"a) Juno: High Time Resolution, Low Tau\")\nsave_plot(\"new/thickness/thickness_k_dist-8hz-tau_20\")\n\np &lt;- plot_thickness_comparison(jno_df_8hz, other_events, p1title=\"a) Juno: High Time Resolution\")\nsave_plot(\"new/thickness/thickness_k_dist-8hz\")\n\n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nSaving 13.8 x 13.8 in image\nSaving 13.8 x 13.8 in image\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\nSaving 13.8 x 13.8 in image\nSaving 13.8 x 13.8 in image\n\n\n\n\nCode\n%%R\nx_col &lt;- \"radial_distance\"\nxlab &lt;- \"Radial Distance (AU)\"\n\ny_col &lt;- \"L_k_fit\"\nylab &lt;- \"Log Thickness (km)\"\ny_lim &lt;- c(100, 40000)\n# y_lim &lt;- NULL\np &lt;- plot_binned_data(jno_df, x_col = x_col, y_col = y_col, x_bins = 8, y_bins = 32, y_lim = y_lim, log_y = TRUE)\np &lt;- p + labs(x = xlab, y= ylab)\n# save_plot(\"new/thickness/thickness_k_fit_r_dist\")\n\ny_col &lt;- \"L_k_fit_norm\"\ny_lim &lt;- c(0.2, 100)\n# y_lim &lt;- NULL\nylab &lt;- expression(Log~Thickness~(d[i]))\np &lt;- plot_binned_data(jno_df, x_col = x_col, y_col = y_col, x_bins = 8, y_bins = 32, y_lim = y_lim, log_y = TRUE)\np &lt;- p + labs(x = xlab, y= ylab)\nsave_plot(\"new/thickness/thickness_k_fit_N1_r_dist\")\np\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image",
    "crumbs": [
      "Home",
      "Notebooks",
      "Archive",
      "SWD Properties"
    ]
  },
  {
    "objectID": "notebooks/missions/stereo/index.html",
    "href": "notebooks/missions/stereo/index.html",
    "title": "IDs from STEREO",
    "section": "",
    "text": "See following notebooks for details:\nSTEREO magnetic field is already in RTN coordinates, so no need to transform it.\nDownload data using pyspedas, but load it using pycdfpp (using pyspedas to load the data directly into xarray is very slow)\nNote STEREO-A plastic data file is “Non compliant ISTP file: No data variable found, this is suspicious”, -9.999999848243207e+30 instead of -1e+31 is used as fill value.\nCode\nimport speasy as spz\nfrom space_analysis.utils.speasy import Variables\nfrom space_analysis.io.cdf import cdf2pl\nfrom discontinuitypy.datasets import IDsDataset\nfrom beforerr.polars import pl_norm\nfrom discontinuitypy.utils.basic import resample\n\nfrom datetime import timedelta\nimport polars as pl\nimport polars.selectors as cs\n\nfrom sunpy.time import TimeRange\nimport pyspedas\n\nfrom tqdm import tqdm\n\n\n25-Feb-24 01:01:20: UserWarning: Traceback (most recent call last):\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/__init__.py\", line 85, in &lt;module&gt;\n    from . import skintegrate\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/skintegrate.py\", line 20, in &lt;module&gt;\n    from sklearn.base import BaseEstimator\nModuleNotFoundError: No module named 'sklearn'\n\n\n25-Feb-24 01:01:20: UserWarning: pdpipe: Scikit-learn or skutil import failed. Scikit-learn-dependent pipeline stages will not be loaded.\n\n25-Feb-24 01:01:20: UserWarning: Traceback (most recent call last):\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/__init__.py\", line 105, in &lt;module&gt;\n    from . import nltk_stages\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/nltk_stages.py\", line 19, in &lt;module&gt;\n    import nltk\nModuleNotFoundError: No module named 'nltk'\n\n\n25-Feb-24 01:01:20: UserWarning: pdpipe: nltk import failed. nltk-dependent  pipeline stages will not be loaded.\nCode\nimport speasy as spz\nentries = spz.core.cache.entries()\n# drop internal entries\nentries = list(filter(lambda e: '__internal__' not in e, entries))\nprint(entries[::-1])\n\n\n['cda/WI_PLSP_3DP/MOM.P.MAGT3/2019-04-12T00:00:00+00:00', 'cda/WI_PLSP_3DP/MOM.P.MAGT3/2019-04-11T12:00:00+00:00', 'cda/WI_PLSP_3DP/MOM.P.MAGT3/2019-04-11T00:00:00+00:00', 'cda/WI_PLSP_3DP/MOM.P.MAGT3/2019-04-10T12:00:00+00:00', 'cda/WI_PLSP_3DP/MOM.P.MAGT3/2019-04-10T00:00:00+00:00', 'cda/WI_PLSP_3DP/MOM.P.MAGT3/2019-04-09T12:00:00+00:00', 'cda/WI_PLSP_3DP/MOM.P.MAGT3/2019-04-09T00:00:00+00:00', 'cda/WI_PLSP_3DP/MOM.P.MAGT3/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsX7LELp/wi_plsps_3dp_20190408120000_20190412115954.cdf', 'https://cdaweb.gsfc.nasa.gov/tmp/wss6lQ5G/wi_plsps_3dp_20190408120000_20190412115954.cdf', 'https://cdaweb.gsfc.nasa.gov/tmp/wsgD6oUy/wi_plsps_3dp_20190408120000_20190412115954.cdf', 'https://cdaweb.gsfc.nasa.gov/tmp/ws2WbWGk/wi_plsps_3dp_20190408120000_20190412115954.cdf', 'https://cdaweb.gsfc.nasa.gov/tmp/wsw6URGt/wi_plsps_3dp_20190408120000_20190412115954.cdf', 'https://cdaweb.gsfc.nasa.gov/tmp/wsrsvJ7P/wi_plsps_3dp_20190408120000_20190412115954.cdf', 'cda/THB_L2_MOM/thb_peim_ptens_magQ/2019-04-12T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptens_magQ/2019-04-11T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptens_magQ/2019-04-11T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptens_magQ/2019-04-10T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptens_magQ/2019-04-10T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptens_magQ/2019-04-09T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptens_magQ/2019-04-09T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptens_magQ/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsJysIZz/thb_l2s_mom_20190408120000_20190412115958.cdf', 'cda/THB_L2_MOM/thb_peem_ptens_magQ/2019-04-12T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_ptens_magQ/2019-04-11T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_ptens_magQ/2019-04-11T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_ptens_magQ/2019-04-10T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_ptens_magQ/2019-04-10T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_ptens_magQ/2019-04-09T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_ptens_magQ/2019-04-09T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_ptens_magQ/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/ws2UEYKB/thb_l2s_mom_20190408120000_20190412115958.cdf', 'cda/WI_PM_3DP/P_TEMP/2019-04-12T00:00:00+00:00', 'cda/WI_PM_3DP/P_TEMP/2019-04-11T12:00:00+00:00', 'cda/WI_PM_3DP/P_TEMP/2019-04-11T00:00:00+00:00', 'cda/WI_PM_3DP/P_TEMP/2019-04-10T12:00:00+00:00', 'cda/WI_PM_3DP/P_TEMP/2019-04-10T00:00:00+00:00', 'cda/WI_PM_3DP/P_TEMP/2019-04-09T12:00:00+00:00', 'cda/WI_PM_3DP/P_TEMP/2019-04-09T00:00:00+00:00', 'cda/WI_PM_3DP/P_TEMP/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wspZgXLc/wi_pms_3dp_20190408120000_20190412115958.cdf', 'cda/WI_PM_3DP/P_VELS/2019-04-12T00:00:00+00:00', 'cda/WI_PM_3DP/P_VELS/2019-04-11T12:00:00+00:00', 'cda/WI_PM_3DP/P_VELS/2019-04-11T00:00:00+00:00', 'cda/WI_PM_3DP/P_VELS/2019-04-10T12:00:00+00:00', 'cda/WI_PM_3DP/P_VELS/2019-04-10T00:00:00+00:00', 'cda/WI_PM_3DP/P_VELS/2019-04-09T12:00:00+00:00', 'cda/WI_PM_3DP/P_VELS/2019-04-09T00:00:00+00:00', 'cda/WI_PM_3DP/P_VELS/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsgZ9Qeq/wi_pms_3dp_20190408120000_20190412115958.cdf', 'cda/WI_PM_3DP/P_DENS/2019-04-12T00:00:00+00:00', 'cda/WI_PM_3DP/P_DENS/2019-04-11T12:00:00+00:00', 'cda/WI_PM_3DP/P_DENS/2019-04-11T00:00:00+00:00', 'cda/WI_PM_3DP/P_DENS/2019-04-10T12:00:00+00:00', 'cda/WI_PM_3DP/P_DENS/2019-04-10T00:00:00+00:00', 'cda/WI_PM_3DP/P_DENS/2019-04-09T12:00:00+00:00', 'cda/WI_PM_3DP/P_DENS/2019-04-09T00:00:00+00:00', 'cda/WI_PM_3DP/P_DENS/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wstZl9Kd/wi_pms_3dp_20190408120000_20190412115958.cdf', 'cda/WI_H2_MFI/BGSE/2019-04-12T00:00:00+00:00', 'cda/WI_H2_MFI/BGSE/2019-04-11T12:00:00+00:00', 'cda/WI_H2_MFI/BGSE/2019-04-11T00:00:00+00:00', 'cda/WI_H2_MFI/BGSE/2019-04-10T12:00:00+00:00', 'cda/WI_H2_MFI/BGSE/2019-04-10T00:00:00+00:00', 'cda/WI_H2_MFI/BGSE/2019-04-09T12:00:00+00:00', 'cda/WI_H2_MFI/BGSE/2019-04-09T00:00:00+00:00', 'cda/WI_H2_MFI/BGSE/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsxSYUNJ/wi_h2s_mfi_20190408120000_20190412115959.cdf', 'cda/THB_L2_MOM/thb_peim_ptotQ/2019-04-12T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptotQ/2019-04-11T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptotQ/2019-04-11T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptotQ/2019-04-10T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptotQ/2019-04-10T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptotQ/2019-04-09T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptotQ/2019-04-09T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_ptotQ/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsFDFj77/thb_l2s_mom_20190408120000_20190412115958.cdf', 'cda/THB_L2_MOM/thb_peim_velocity_gseQ/2019-04-12T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_velocity_gseQ/2019-04-11T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_velocity_gseQ/2019-04-11T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_velocity_gseQ/2019-04-10T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsOxDZT7/thb_l2s_mom_20190410120000_20190412115958.cdf', 'cda/THB_L2_MOM/thb_peim_velocity_gseQ/2019-04-09T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_velocity_gseQ/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsHyWZk9/thb_l2s_mom_20190408120000_20190409115958.cdf', 'cda/THB_L2_MOM/thb_peim_densityQ/2019-04-12T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_densityQ/2019-04-11T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_densityQ/2019-04-11T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_densityQ/2019-04-10T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsOkO4D6/thb_l2s_mom_20190410120000_20190412115958.cdf', 'cda/THB_L2_MOM/thb_peim_densityQ/2019-04-09T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_densityQ/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsjw9VQW/thb_l2s_mom_20190408120000_20190409115958.cdf', 'cda/THB_L2_FGM/thb_fgl_gse/2019-04-12T00:00:00+00:00', 'cda/THB_L2_FGM/thb_fgl_gse/2019-04-11T12:00:00+00:00', 'cda/THB_L2_FGM/thb_fgl_gse/2019-04-11T00:00:00+00:00', 'cda/THB_L2_FGM/thb_fgl_gse/2019-04-10T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsPOK59Q/thb_l2s_fgm_20190410154948_20190412051612.cdf', 'cda/THB_L2_FGM/thb_fgl_gse/2019-04-09T00:00:00+00:00', 'cda/THB_L2_FGM/thb_fgl_gse/2019-04-08T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wskTkvRu/thb_l2s_fgm_20190408120000_20190408192100.cdf', 'cda/THB_L2_MOM/thb_peim_densityQ/2019-04-10T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_densityQ/2019-04-09T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsjUpKuw/thb_l2s_mom_20190409120003_20190410115956.cdf', 'cda/THB_L2_FGM/thb_fgl_gse/2019-04-10T00:00:00+00:00', 'cda/THB_L2_FGM/thb_fgl_gse/2019-04-09T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsfWpYWe/thb_l2s_fgm_20190409122948_20190409223612.cdf', 'cda/THB_L2_MOM/thb_peim_velocity_gseQ/2019-04-10T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_velocity_gseQ/2019-04-09T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsF7wqKb/thb_l2s_mom_20190409120003_20190410115956.cdf', 'cda/SOLO_L2_MAG-RTN-NORMAL-1-MINUTE/B_RTN/2021-01-02T00:00:00+00:00', 'cda/SOLO_L2_MAG-RTN-NORMAL-1-MINUTE/B_RTN/2021-01-01T12:00:00+00:00', 'cda/SOLO_L2_MAG-RTN-NORMAL-1-MINUTE/B_RTN/2021-01-01T00:00:00+00:00', 'cda/SOLO_L2_MAG-RTN-NORMAL-1-MINUTE/B_RTN/2020-12-31T12:00:00+00:00', 'cda/WI_PLSP_3DP/ENERGY/2019-04-12T00:00:00+00:00', 'cda/WI_PLSP_3DP/ENERGY/2019-04-11T12:00:00+00:00', 'cda/WI_PLSP_3DP/ENERGY/2019-04-11T00:00:00+00:00', 'cda/WI_PLSP_3DP/ENERGY/2019-04-10T12:00:00+00:00', 'cda/WI_PLSP_3DP/ENERGY/2019-04-10T00:00:00+00:00', 'cda/WI_PLSP_3DP/ENERGY/2019-04-09T12:00:00+00:00', 'cda/WI_PLSP_3DP/ENERGY/2019-04-09T00:00:00+00:00', 'cda/WI_PLSP_3DP/ENERGY/2019-04-08T12:00:00+00:00', 'cda/WI_ELM2_3DP/MAGT3/2019-04-12T00:00:00+00:00', 'cda/WI_ELM2_3DP/MAGT3/2019-04-11T12:00:00+00:00', 'cda/WI_ELM2_3DP/MAGT3/2019-04-11T00:00:00+00:00', 'cda/WI_ELM2_3DP/MAGT3/2019-04-10T12:00:00+00:00', 'cda/WI_ELM2_3DP/MAGT3/2019-04-10T00:00:00+00:00', 'cda/WI_ELM2_3DP/MAGT3/2019-04-09T12:00:00+00:00', 'cda/WI_ELM2_3DP/MAGT3/2019-04-09T00:00:00+00:00', 'cda/WI_ELM2_3DP/MAGT3/2019-04-08T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_t3_magQ/2019-04-12T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_t3_magQ/2019-04-11T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_t3_magQ/2019-04-11T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_t3_magQ/2019-04-10T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_t3_magQ/2019-04-10T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_t3_magQ/2019-04-09T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_t3_magQ/2019-04-09T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peim_t3_magQ/2019-04-08T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_t3_magQ/2019-04-12T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_t3_magQ/2019-04-11T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_t3_magQ/2019-04-11T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_t3_magQ/2019-04-10T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_t3_magQ/2019-04-10T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_t3_magQ/2019-04-09T12:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_t3_magQ/2019-04-09T00:00:00+00:00', 'cda/THB_L2_MOM/thb_peem_t3_magQ/2019-04-08T12:00:00+00:00', 'cda/PSP_SWP_SPC_L3I/np_moment_gd/2019-04-07T12:00:00+00:00', 'cda/PSP_SWP_SPC_L3I/np_moment_gd/2019-04-07T00:00:00+00:00', 'cda/PSP_SWP_SPC_L3I/np_moment_gd/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wshv6OPm/psp_swps_spc_l3i_20190406120012_20190407235805.cdf', 'cda/PSP_SWP_SPI_SF00_L3_MOM/SUN_DIST/2019-04-07T12:00:00+00:00', 'cda/PSP_SWP_SPI_SF00_L3_MOM/SUN_DIST/2019-04-07T00:00:00+00:00', 'cda/PSP_SWP_SPI_SF00_L3_MOM/SUN_DIST/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wszcUkYV/psp_swps_spi_sf00_l3_mom_20190406120001_20190407235959.cdf', 'cda/PSP_SWP_SPI_SF00_L3_MOM/TEMP/2019-04-07T12:00:00+00:00', 'cda/PSP_SWP_SPI_SF00_L3_MOM/TEMP/2019-04-07T00:00:00+00:00', 'cda/PSP_SWP_SPI_SF00_L3_MOM/TEMP/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsWQttvt/psp_swps_spi_sf00_l3_mom_20190406120001_20190407235959.cdf', 'cda/PSP_SWP_SPI_SF00_L3_MOM/VEL_RTN_SUN/2019-04-07T12:00:00+00:00', 'cda/PSP_SWP_SPI_SF00_L3_MOM/VEL_RTN_SUN/2019-04-07T00:00:00+00:00', 'cda/PSP_SWP_SPI_SF00_L3_MOM/VEL_RTN_SUN/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsqD3Vl4/psp_swps_spi_sf00_l3_mom_20190406120001_20190407235959.cdf', 'cda/PSP_SWP_SPI_SF00_L3_MOM/DENS/2019-04-07T12:00:00+00:00', 'cda/PSP_SWP_SPI_SF00_L3_MOM/DENS/2019-04-07T00:00:00+00:00', 'cda/PSP_SWP_SPI_SF00_L3_MOM/DENS/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/ws1kPspF/psp_swps_spi_sf00_l3_mom_20190406120001_20190407235959.cdf', 'cda/PSP_FLD_L3_RFS_LFR_QTN/N_elec/2019-04-07T12:00:00+00:00', 'cda/PSP_FLD_L3_RFS_LFR_QTN/N_elec/2019-04-07T00:00:00+00:00', 'cda/PSP_FLD_L3_RFS_LFR_QTN/N_elec/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wscwwzey/psp_flds_l3_rfs_lfr_qtn_20190406120006_20190407235922.cdf', 'cda/PSP_SWP_SPC_L3I/wp_moment_gd/2019-04-07T12:00:00+00:00', 'cda/PSP_SWP_SPC_L3I/wp_moment_gd/2019-04-07T00:00:00+00:00', 'cda/PSP_SWP_SPC_L3I/wp_moment_gd/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsqkNkGL/psp_swps_spc_l3i_20190406120012_20190407235805.cdf', 'cda/PSP_SWP_SPC_L3I/vp_moment_RTN_gd/2019-04-07T12:00:00+00:00', 'cda/PSP_SWP_SPC_L3I/vp_moment_RTN_gd/2019-04-07T00:00:00+00:00', 'cda/PSP_SWP_SPC_L3I/vp_moment_RTN_gd/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsBcRYzd/psp_swps_spc_l3i_20190406120012_20190407235805.cdf', 'cda/PSP_FLD_L3_SQTN_RFS_V1V2/electron_density/2019-04-07T12:00:00+00:00', 'cda/PSP_FLD_L3_SQTN_RFS_V1V2/electron_density/2019-04-07T00:00:00+00:00', 'cda/PSP_FLD_L3_SQTN_RFS_V1V2/electron_density/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/ws5ziTed/psp_flds_l3_sqtn_rfs_v1v2_20190406120006_20190407235957.cdf', 'cda/PSP_FLD_L2_MAG_RTN/psp_fld_l2_mag_RTN/2019-04-07T12:00:00+00:00', 'cda/PSP_FLD_L2_MAG_RTN/psp_fld_l2_mag_RTN/2019-04-07T00:00:00+00:00', 'cda/PSP_FLD_L2_MAG_RTN/psp_fld_l2_mag_RTN/2019-04-06T12:00:00+00:00', 'https://cdaweb.gsfc.nasa.gov/tmp/wsZx9P7S/psp_flds_l2_mag_rtn_20190406120000_20190407235735.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/thb_l2_mom_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/thb_l2_fgm_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/sta_l2_pla_1dmax_1min_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/sta_l2_magplasma_1m_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/sta_l1_mag_rtn_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/wi_k0_swe_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/wi_h4-rtn_mfi_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/sst/0000/the_l2_sst_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/scm/0000/the_l2_scm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/mom/0000/the_l2_mom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/gmom/0000/the_l2_gmom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/fit/0000/the_l2_fit_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/fgm/0000/the_l2_fgm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/fft/0000/the_l2_fft_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/fbk/0000/the_l2_fbk_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/esd/0000/the_l2_esd_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/esa/0000/the_l2_esa_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/the/l2/efi/0000/the_l2_efi_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/sst/0000/thd_l2_sst_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/scm/0000/thd_l2_scm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/mom/0000/thd_l2_mom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/gmom/0000/thd_l2_gmom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/fit/0000/thd_l2_fit_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/fgm/0000/thd_l2_fgm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/fft/0000/thd_l2_fft_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/fbk/0000/thd_l2_fbk_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/esd/0000/thd_l2_esd_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/esa/0000/thd_l2_esa_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thd/l2/efi/0000/thd_l2_efi_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/sst/0000/thc_l2_sst_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/scm/0000/thc_l2_scm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/mom/0000/thc_l2_mom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/gmom/0000/thc_l2_gmom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/fit/0000/thc_l2_fit_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/fgm/0000/thc_l2_fgm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/fft/0000/thc_l2_fft_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/fbk/0000/thc_l2_fbk_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/esd/0000/thc_l2_esd_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/esa/0000/thc_l2_esa_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thc/l2/efi/0000/thc_l2_efi_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/sst/0000/thb_l2_sst_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/scm/0000/thb_l2_scm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/mom/0000/thb_l2_mom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/gmom/0000/thb_l2_gmom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/fit/0000/thb_l2_fit_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/fgm/0000/thb_l2_fgm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/fft/0000/thb_l2_fft_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/fbk/0000/thb_l2_fbk_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/esd/0000/thb_l2_esd_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/esa/0000/thb_l2_esa_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/thb/l2/efi/0000/thb_l2_efi_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/sst/0000/tha_l2_sst_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/scm/0000/tha_l2_scm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/mom/0000/tha_l2_mom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/gmom/0000/tha_l2_gmom_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/fit/0000/tha_l2_fit_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/fgm/0000/tha_l2_fgm_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/fft/0000/tha_l2_fft_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/fbk/0000/tha_l2_fbk_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/esd/0000/tha_l2_esd_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/esa/0000/tha_l2_esa_00000000_v01.cdf', 'http://cdpp.irap.omp.eu/themisdata/tha/l2/efi/0000/tha_l2_efi_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms4_fpi_fast_l2_des-moms_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms4_fpi_brst_l2_des-moms_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms4_fgm_srvy_l2_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms3_fpi_fast_l2_des-moms_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms3_fpi_brst_l2_des-moms_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms3_fgm_srvy_l2_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms2_fpi_fast_l2_des-moms_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms2_fpi_brst_l2_des-moms_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms2_fgm_srvy_l2_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms1_fpi_fast_l2_des-moms_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms1_fpi_brst_l2_des-moms_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/mms1_fgm_srvy_l2_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/erg_pwe_hfa_l3_1min_00000000_v01.cdf', 'https://cdaweb.gsfc.nasa.gov/pub/software/cdawlib/0MASTERS/erg_lepe_l3_pa_00000000_v01.cdf', 'cache/version']\nCode\nspz.update_inventories()\nCode\ntimerange = [\"2011-08-25\", \"2016-06-30\"]\n# timerange = [\"2011-08-25\", \"2011-09-01\"]\nmission = \"STA\"\nts = timedelta(seconds=1)\ntau = timedelta(seconds=60)\n\nprovider = 'archive/local'\nmag_dataset = \"STA_L1_MAG_RTN\"\nmag_parameters = [\"BFIELD\"]\n\n# plasma_dataset = \"STA_L2_MAGPLASMA_1M\"\n# plasma_parameters= [\"Np\", \"Cone_Angle\", \"Vt_Over_V_RTN\", \"Vp_RTN\", \"Tp\"]\n\nplasma_dataset = 'STA_L2_PLA_1DMAX_1MIN'\ndensity_col = 'proton_number_density'\nvec_cols = ['proton_Vr_RTN', 'proton_Vt_RTN', 'proton_Vn_RTN']\ntemperature_col = 'proton_temperature'\nplasma_parameters = [density_col] + vec_cols + [temperature_col]\n\nfmt = 'arrow'\nfname = f\"../../../data/05_reporting/events.{mission}.ts_{ts.total_seconds():.2f}s_tau_60s.{fmt}\"",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from STEREO"
    ]
  },
  {
    "objectID": "notebooks/missions/stereo/index.html#downloading-files",
    "href": "notebooks/missions/stereo/index.html#downloading-files",
    "title": "IDs from STEREO",
    "section": "Downloading files",
    "text": "Downloading files\n\n\nCode\ndef download(timerange):\n    \n    files = pyspedas.stereo.plastic(timerange, downloadonly=True)\n\n    for file in files:\n        parquet_file = file.replace('.cdf', f'.{fmt}')\n        cdf2pl(file, plasma_parameters).collect().write_ipc(parquet_file)\n        \n    mag_files = pyspedas.stereo.mag(timerange, downloadonly=True)\n    \n    return files\n    \n        \n# download(timerange)",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from STEREO"
    ]
  },
  {
    "objectID": "notebooks/missions/stereo/index.html#loading-data",
    "href": "notebooks/missions/stereo/index.html#loading-data",
    "title": "IDs from STEREO",
    "section": "Loading data",
    "text": "Loading data\n\n\nCode\nmag_vars = Variables(\n    provider = provider,\n    dataset=mag_dataset,\n    parameters=mag_parameters,\n    timerange=timerange,\n).retrieve_data()\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[7], line 6\n      1 mag_vars = Variables(\n      2     provider = provider,\n      3     dataset=mag_dataset,\n      4     parameters=mag_parameters,\n      5     timerange=timerange,\n----&gt; 6 ).retrieve_data()\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/space_analysis/utils/speasy.py:86, in Variables.retrieve_data(self)\n     83 def retrieve_data(self):\n     84     # return Variables with data set\n     85     if \"local\" in self.provider:\n---&gt; 86         self._data = spz.get_data(self.products, self.timerange)\n     87     else:\n     88         self._data = spz.get_data(\n     89             self.products, self.timerange, disable_proxy=self._disable_proxy\n     90         )\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:332, in get_data(*args, **kwargs)\n    330 product = args[0]\n    331 if is_collection(product) and not isinstance(product, SpeasyIndex):\n--&gt; 332     return list(map(lambda p: get_data(p, *args[1:], **kwargs), progress_bar(leave=True, **kwargs)(product)))\n    334 if len(args) == 1:\n    335     return _get_catalog_or_timetable(*args, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:332, in get_data.&lt;locals&gt;.&lt;lambda&gt;(p)\n    330 product = args[0]\n    331 if is_collection(product) and not isinstance(product, SpeasyIndex):\n--&gt; 332     return list(map(lambda p: get_data(p, *args[1:], **kwargs), progress_bar(leave=True, **kwargs)(product)))\n    334 if len(args) == 1:\n    335     return _get_catalog_or_timetable(*args, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:339, in get_data(*args, **kwargs)\n    337 t_range = args[1]\n    338 if _is_dtrange(t_range):\n--&gt; 339     return _get_timeserie1(*args, **kwargs)\n    340 if is_collection(t_range):\n    341     return list(\n    342         map(lambda r: get_data(product, r, *args[2:], **kwargs),\n    343             progress_bar(leave=False, **kwargs)(t_range)))\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:182, in _get_timeserie1(index, dtrange, **kwargs)\n    181 def _get_timeserie1(index, dtrange, **kwargs):\n--&gt; 182     return _scalar_get_data(index, dtrange[0], dtrange[1], **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/requests_scheduling/request_dispatch.py:173, in _scalar_get_data(index, *args, **kwargs)\n    171 provider_uid, product_uid = provider_and_product(index)\n    172 if provider_uid in PROVIDERS:\n--&gt; 173     return PROVIDERS[provider_uid].get_data(product_uid, *args, **kwargs)\n    174 raise ValueError(f\"Can't find a provider for {index}\")\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/__init__.py:223, in AllowedKwargs.__call__.&lt;locals&gt;.wrapped(*args, **kwargs)\n    220 unexpected_args = list(\n    221     filter(lambda arg_name: arg_name not in self.allowed_list, kwargs.keys()))\n    222 if not unexpected_args:\n--&gt; 223     return func(*args, **kwargs)\n    224 raise TypeError(\n    225     f\"Unexpected keyword argument {unexpected_args}, allowed keyword arguments are {self.allowed_list}\")\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/webservices/generic_archive/__init__.py:106, in GenericArchive.get_data(self, product, start_time, stop_time, **kwargs)\n    103 @AllowedKwargs(GET_DATA_ALLOWED_KWARGS + CACHE_ALLOWED_KWARGS + ['force_refresh'])\n    104 def get_data(self, product: str or ParameterIndex, start_time: AnyDateTimeType, stop_time: AnyDateTimeType,\n    105              **kwargs) -&gt; Optional[SpeasyVariable]:\n--&gt; 106     var = self._get_data(product=self._parameter_index(product), start_time=start_time, stop_time=stop_time,\n    107                          **kwargs)\n    108     return var\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/webservices/generic_archive/__init__.py:114, in GenericArchive._get_data(self, product, start_time, stop_time, **kwargs)\n    110 def _get_data(self, product: ParameterIndex, start_time: AnyDateTimeType, stop_time: AnyDateTimeType, **kwargs) -&gt; \\\n    111 Optional[\n    112     SpeasyVariable]:\n    113     ga_cfg: dict = getattr(product, 'spz_ga_cfg')\n--&gt; 114     return get_product(**ga_cfg,\n    115                        variable=product.spz_name(), start_time=start_time, stop_time=stop_time, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/direct_archive_downloader/direct_archive_downloader.py:162, in get_product(url_pattern, split_rule, variable, start_time, stop_time, use_file_list, **kwargs)\n    159 def get_product(url_pattern: str, split_rule: str, variable: str, start_time: AnyDateTimeType,\n    160                 stop_time: AnyDateTimeType, use_file_list: bool = False, **kwargs) -&gt; Optional[SpeasyVariable]:\n    161     if split_rule.lower() == \"regular\":\n--&gt; 162         return RegularSplitDirectDownload.get_product(url_pattern, variable, start_time, stop_time,\n    163                                                       use_file_list, **kwargs)\n    164     if split_rule.lower() == \"random\":\n    165         return RandomSplitDirectDownload.get_product(url_pattern, variable, start_time, stop_time,\n    166                                                      **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/direct_archive_downloader/direct_archive_downloader.py:150, in RegularSplitDirectDownload.get_product(url_pattern, variable, start_time, stop_time, use_file_list, split_frequency, **kwargs)\n    144 @staticmethod\n    145 def get_product(url_pattern: str, variable: str, start_time: AnyDateTimeType,\n    146                 stop_time: AnyDateTimeType, use_file_list: bool = False, split_frequency: str = \"daily\",\n    147                 **kwargs) -&gt; \\\n    148     Optional[SpeasyVariable]:\n    149     v = merge(\n--&gt; 150         list(map(lambda date: _read_cdf(_build_url(url_pattern, date, use_file_list=use_file_list),\n    151                                         variable=variable, **kwargs),\n    152                  spilt_range(split_frequency=split_frequency, start_time=start_time,\n    153                              stop_time=stop_time))))\n    154     if v is not None:\n    155         return v[make_utc_datetime(start_time):make_utc_datetime(stop_time)]\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/direct_archive_downloader/direct_archive_downloader.py:150, in RegularSplitDirectDownload.get_product.&lt;locals&gt;.&lt;lambda&gt;(date)\n    144 @staticmethod\n    145 def get_product(url_pattern: str, variable: str, start_time: AnyDateTimeType,\n    146                 stop_time: AnyDateTimeType, use_file_list: bool = False, split_frequency: str = \"daily\",\n    147                 **kwargs) -&gt; \\\n    148     Optional[SpeasyVariable]:\n    149     v = merge(\n--&gt; 150         list(map(lambda date: _read_cdf(_build_url(url_pattern, date, use_file_list=use_file_list),\n    151                                         variable=variable, **kwargs),\n    152                  spilt_range(split_frequency=split_frequency, start_time=start_time,\n    153                              stop_time=stop_time))))\n    154     if v is not None:\n    155         return v[make_utc_datetime(start_time):make_utc_datetime(stop_time)]\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/direct_archive_downloader/direct_archive_downloader.py:26, in _read_cdf(url, variable, **kwargs)\n     24     return None\n     25 if is_local_file(url):\n---&gt; 26     return _local_read_cdf(file=url, variable=variable, **kwargs)\n     27 return _remote_read_cdf(url=url, variable=variable, **kwargs)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/direct_archive_downloader/direct_archive_downloader.py:31, in _local_read_cdf(file, variable, **kwargs)\n     30 def _local_read_cdf(file: str, variable: str, **kwargs) -&gt; Optional[SpeasyVariable]:\n---&gt; 31     return load_variable(file=file, variable=variable)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/cdf/__init__.py:77, in load_variable(variable, file, cache_remote_files)\n     75 if type(file) is str:\n     76     if is_local_file(file):\n---&gt; 77         return _load_variable(variable=variable, file=urlparse(url=file).path)\n     78     return _load_variable(variable=variable,\n     79                           buffer=any_loc_open(file, mode='rb', cache_remote_files=cache_remote_files).read())\n     80 if type(file) is bytes:\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/speasy/core/cdf/__init__.py:52, in _load_variable(variable, file, buffer)\n     50 if istp is not None:\n     51     if variable in istp.data_variables():\n---&gt; 52         var = istp.data_variable(variable)\n     53     elif variable.replace('-', '_') in istp.data_variables():  # THX CSA/ISTP\n     54         var = istp.data_variable(variable.replace('-', '_'))\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pyistp/loader.py:21, in ISTPLoader.data_variable(self, var_name)\n     20 def data_variable(self, var_name) -&gt; DataVariable:\n---&gt; 21     return self._impl.data_variable(var_name)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pyistp/_impl.py:114, in ISTPLoaderImpl.data_variable(self, var_name)\n    113 def data_variable(self, var_name) -&gt; DataVariable:\n--&gt; 114     return _load_data_var(self.cdf, var_name)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pyistp/_impl.py:72, in _load_data_var(cdf, var)\n     71 def _load_data_var(cdf: object, var: str) -&gt; DataVariable or None:\n---&gt; 72     values = cdf.values(var)\n     73     axes = _get_axes(cdf, var, values.shape)\n     74     attributes = _get_attributes(cdf, var)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pyistp/drivers/pycdfpp.py:56, in Driver.values(self, var, is_metadata_variable)\n     54 if is_metadata_variable and self.is_char(var):\n     55     return _drop_first_dim_if_nrv(v.is_nrv, v.values_encoded)\n---&gt; 56 return _drop_first_dim_if_nrv(v.is_nrv, v.values)\n\nFile ~/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pyistp/drivers/pycdfpp.py:5, in _drop_first_dim_if_nrv(is_nrv, values)\n      1 import pycdfpp\n      2 import numpy as np\n----&gt; 5 def _drop_first_dim_if_nrv(is_nrv: bool, values):\n      6     if is_nrv:\n      7         if values.shape[0] == 1:\n\nKeyboardInterrupt: \n\n\n\n\n\nCode\ndef get_and_process_data(timerange):\n    mag_vars = Variables(\n        provider = provider,\n        dataset=mag_dataset,\n        parameters=mag_parameters,\n        timerange=timerange,\n    ).retrieve_data()\n\n    bcols = mag_vars.data[0].columns[:3]\n\n    mag_data = mag_vars.to_polars().drop('BTotal')\n    \n    files = pyspedas.stereo.plastic(timerange, downloadonly=True, no_update=True)\n    parquet_files = [file.replace('.cdf', f'.{fmt}') for file in files]\n\n    plasma_data = pl.scan_ipc(parquet_files).with_columns(\n        plasma_density = pl.when(pl.col(density_col) &lt;0 ).then(None).otherwise(pl.col(density_col)),\n        plasma_speed=pl_norm(vec_cols), ).rename({temperature_col:'plasma_temperature'}\n    )\n    \n    return IDsDataset(\n        mag_data=mag_data.pipe(resample, every=ts),\n        plasma_data=plasma_data,\n        tau=tau,\n        ts=ts,\n        bcols=bcols,\n        vec_cols=vec_cols,\n        density_col=\"plasma_density\",\n        speed_col=\"plasma_speed\",\n        temperature_col='plasma_temperature',\n    ).find_events(return_best_fit=False).update_candidates_with_plasma_data()\n\n\n\n\nCode\nids : list[pl.DataFrame] = []\nfor _tr in tqdm(TimeRange(timerange).split(8)):\n    \n    _timerange = [_tr.start.value, _tr.end.value]\n    _id = get_and_process_data(_timerange)\n    ids.append(_id.events)\n    \nid = pl.concat(ids, how='vertical_relaxed').select(cs.datetime(), cs.duration(), cs.numeric())\nid.write_ipc(fname)",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from STEREO"
    ]
  },
  {
    "objectID": "notebooks/missions/themis/state.html",
    "href": "notebooks/missions/themis/state.html",
    "title": "THEMIS State data pipeline",
    "section": "",
    "text": "We use low resolution OMNI data for plasma state data, as we did in the OMNI notebook\n::: {#cell-1 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/themis/state.html#solar-wind-state",
    "href": "notebooks/missions/themis/state.html#solar-wind-state",
    "title": "THEMIS State data pipeline",
    "section": "Solar wind state",
    "text": "Solar wind state\nAlso we have additional data file that indicate if THEMIS is in solar wind or not.\n::: {#cell-4 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef load_sw_data(raw_data: pandas.DataFrame):\n    return pl.from_dataframe(raw_data)\n\n:::\n::: {#cell-5 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef preprocess_sw_data(\n    raw_data: pl.LazyFrame,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    - Applying naming conventions for columns\n    - Parsing and typing data (like from string to datetime for time columns)\n    \"\"\"\n\n    return raw_data.with_columns(\n        # Note: For `polars`, please either specify both hour and minute, or neither.\n        pl.concat_str(pl.col(\"start\"), pl.lit(\" 00\")).str.to_datetime(\n            format=\"%Y %j %H %M\"\n        ),\n        pl.concat_str(pl.col(\"end\"), pl.lit(\" 00\")).str.to_datetime(\n            format=\"%Y %j %H %M\"\n        ),\n    )\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/themis/state.html#pipelines",
    "href": "notebooks/missions/themis/state.html#pipelines",
    "title": "THEMIS State data pipeline",
    "section": "Pipelines",
    "text": "Pipelines\n::: {#cell-7 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef create_sw_pipeline(sat_id=\"THB\", source=\"STATE\"):\n    namespace = f\"{sat_id}.{source}\"\n    node_load_sw_data = node(\n        load_sw_data,\n        inputs=\"original_sw_data\",\n        outputs=\"raw_data_sw\",\n        name=\"load_solar_wind_data\",\n    )\n    node_preprocess_sw_state = node(\n        preprocess_sw_data,\n        inputs=\"raw_data_sw\",\n        outputs=\"inter_data_sw\",\n        name=\"preprocess_solar_wind_data\",\n    )\n    return pipeline(\n        [\n            node_load_sw_data,\n            node_preprocess_sw_state,\n        ],\n        namespace=namespace,\n    )\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/themis/index.html",
    "href": "notebooks/missions/themis/index.html",
    "title": "IDs from ARTHEMIS",
    "section": "",
    "text": "See following notebooks for details:",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from ARTHEMIS"
    ]
  },
  {
    "objectID": "notebooks/missions/themis/index.html#background",
    "href": "notebooks/missions/themis/index.html#background",
    "title": "IDs from ARTHEMIS",
    "section": "Background",
    "text": "Background\nARTEMIS spacecrafts will be exposed in the solar wind at 1 AU during its orbits around the Moon. So it’s very interesting to look into its data.\n\nFor time inteval for THEMIS-B in solar wind, see Link\nFor time inteval for THEMIS-C in solar wind, see Link\n\n\n\nCode\nimport polars as pl\nfrom space_analysis.utils.speasy import Variables\nfrom discontinuitypy.datasets import IDsDataset\nfrom discontinuitypy.utils.basic import resample\nfrom beforerr.polars import pl_norm\n\nfrom datetime import timedelta\nfrom sunpy.time import TimeRange\n\nfrom space_analysis.plasma.formulary import df_thermal_spd2temp\n\nfrom tqdm import tqdm\n\n\n25-Feb-24 21:20:54: UserWarning: Traceback (most recent call last):\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/__init__.py\", line 85, in &lt;module&gt;\n    from . import skintegrate\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/skintegrate.py\", line 20, in &lt;module&gt;\n    from sklearn.base import BaseEstimator\nModuleNotFoundError: No module named 'sklearn'\n\n\n25-Feb-24 21:20:54: UserWarning: pdpipe: Scikit-learn or skutil import failed. Scikit-learn-dependent pipeline stages will not be loaded.\n\n25-Feb-24 21:20:54: UserWarning: Traceback (most recent call last):\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/__init__.py\", line 105, in &lt;module&gt;\n    from . import nltk_stages\n  File \"/Users/zijin/micromamba/envs/psp_conjunction/lib/python3.11/site-packages/pdpipe/nltk_stages.py\", line 19, in &lt;module&gt;\n    import nltk\nModuleNotFoundError: No module named 'nltk'\n\n\n25-Feb-24 21:20:54: UserWarning: pdpipe: nltk import failed. nltk-dependent  pipeline stages will not be loaded.\n\n\n\n\n\nCode\ntimerange = [\"2011-08-25\", \"2016-06-30\"]\ntimerange = [\"2011-08-25\", \"2011-09-01\"]\nmission = \"THB\"\nts = timedelta(seconds=1)\ntau = timedelta(seconds=60)\n\nprovider = 'archive/local'\nmag_dataset = \"THB_L2_FGM\"\nmag_parameters = [\"thb_fgl_gse\"]\nplasma_dataset = 'THB_L2_MOM'\nplasma_parameters = [\"thb_peim_densityQ\", \"thb_peim_velocity_gseQ\"]\n\nfmt = 'arrow'\nfname = f\"../../../data/05_reporting/events.{mission}.ts_{ts.total_seconds():.2f}s_tau_60s.{fmt}\"\n\n\n\n\nCode\ntimerange = [\"2011-08-25\", \"2016-06-30\"]\ntimerange = [\"2011-08-25\", \"2011-09-01\"]\n\nmission = \"Wind\"\nts = timedelta(seconds=1)\ntau = timedelta(seconds=60)\n\nprovider = 'archive/local'\nmag_dataset = \"WI_H4-RTN_MFI\"\nmag_parameters = [\"BRTN\"]\nplasma_dataset = \"WI_K0_SWE\"\nplasma_parameters= [\"Np\", \"V_GSM\", \"THERMAL_SPD\"]\n\nfmt = 'arrow'\nfname = f\"../../data/05_reporting/events.{mission}.ts_{ts.total_seconds():.2f}s_tau_60s.{fmt}\"\n\n\n\n\nCode\nmag_vars = Variables(\n    provider = provider,\n    dataset=mag_dataset,\n    parameters=mag_parameters,\n    timerange=timerange,\n).retrieve_data()\n\n\n\n\nCode\nmag_vars.time_resolutions\n\n\n\n\nshape: (8, 2)\n\n\n\nstatistic\nvalue\n\n\nstr\nstr\n\n\n\n\n\"count\"\n\"6053744\"\n\n\n\"null_count\"\n\"1\"\n\n\n\"mean\"\n\"0:00:00.092974…\n\n\n\"min\"\n\"0:00:00.091000…\n\n\n\"25%\"\n\"0:00:00.092000…\n\n\n\"50%\"\n\"0:00:00.092000…\n\n\n\"75%\"\n\"0:00:00.092000…\n\n\n\"max\"\n\"0:04:36.092000…\n\n\n\n\n\n\n\n\n\nCode\nplasma_vars = Variables(\n    provider = provider,\n    dataset=plasma_dataset,\n    parameters=plasma_parameters,\n    timerange=timerange,\n).retrieve_data()\n\n\n\n\nCode\ndef get_and_process_data(\n    mag_dataset, mag_parameters, plasma_dataset, plasma_parameters, timerange, tau, ts, provider\n):\n    # define variables\n    mag_vars = Variables(\n        provider = provider,\n        dataset=mag_dataset,\n        parameters=mag_parameters,\n        timerange=timerange,\n    ).retrieve_data()\n\n    plasma_vars = Variables(\n        provider = provider,\n        dataset=plasma_dataset,\n        parameters=plasma_parameters,\n        timerange=timerange,\n    ).retrieve_data()\n\n    # get column names\n    bcols = mag_vars.data[0].columns\n    density_col = plasma_vars.data[0].columns[0]\n    vec_cols = plasma_vars.data[1].columns\n    temperature_col = plasma_vars.data[2].columns[0]\n\n    # get data\n    mag_data = mag_vars.to_polars()\n    plasma_data = (\n        plasma_vars.to_polars()\n        .with_columns(plasma_speed=pl_norm(vec_cols))\n        .rename({density_col: \"plasma_density\"})\n    )\n    # process temperature data\n    if plasma_vars.data[2].unit == \"km/s\":\n        plasma_data = plasma_data.pipe(df_thermal_spd2temp, temperature_col)\n    else:\n        plasma_data = plasma_data.rename({temperature_col: \"plasma_temperature\"})\n\n    return IDsDataset(\n        mag_data=mag_data.pipe(resample, every=ts),\n        plasma_data=plasma_data,\n        tau=tau,\n        ts=ts,\n        bcols=bcols,\n        vec_cols=vec_cols,\n        density_col=\"plasma_density\",\n        speed_col=\"plasma_speed\",\n        temperature_col=\"plasma_temperature\",\n    ).find_events(return_best_fit=False).update_candidates_with_plasma_data()\n\n\n\n\nCode\nids : list[pl.DataFrame] = []\nfor _tr in tqdm(TimeRange(timerange).split(8)):\n    \n    _timerange = [_tr.start.value, _tr.end.value]\n    _id = get_and_process_data(\n        mag_dataset, mag_parameters, plasma_dataset, plasma_parameters, _timerange, tau, ts,\n        provider = provider\n    )\n    ids.append(_id.events)\n\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s]15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:40: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n15-Feb-24 13:44:41: Non compliant ISTP file: No data variable found, this is suspicious\n  0%|          | 0/8 [00:00&lt;?, ?it/s]\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[10], line 5\n      2 for _tr in tqdm(TimeRange(timerange).split(8)):\n      4     _timerange = [_tr.start.value, _tr.end.value]\n----&gt; 5     _id = get_and_process_data(\n      6         mag_dataset, mag_parameters, plasma_dataset, plasma_parameters, _timerange, tau, ts\n      7     )\n      8     ids.append(_id.events)\n\nCell In[9], line 21, in get_and_process_data(mag_dataset, mag_parameters, plasma_dataset, plasma_parameters, timerange, tau, ts, provider)\n     13 plasma_vars = Variables(\n     14     provider = provider,\n     15     dataset=plasma_dataset,\n     16     parameters=plasma_parameters,\n     17     timerange=timerange,\n     18 ).retrieve_data()\n     20 # get column names\n---&gt; 21 bcols = mag_vars.data[0].columns\n     22 density_col = plasma_vars.data[0].columns[0]\n     23 vec_cols = plasma_vars.data[1].columns\n\nAttributeError: 'NoneType' object has no attribute 'columns'",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from ARTHEMIS"
    ]
  },
  {
    "objectID": "notebooks/missions/themis/index.html#solar-wind-pipeline",
    "href": "notebooks/missions/themis/index.html#solar-wind-pipeline",
    "title": "IDs from ARTHEMIS",
    "section": "Solar wind pipeline",
    "text": "Solar wind pipeline\n::: {#cell-11 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\nfrom discontinuitypy.utils.basic import filter_tranges_df\n\ndef filter_sw_events(events: pl.LazyFrame, sw_state: pl.LazyFrame) -&gt; pl.LazyFrame:\n    \n    start, end = sw_state.select(['start', 'end']).collect()\n    sw_events = filter_tranges_df(events.collect(), (start, end))\n    \n    return sw_events\n\ndef create_sw_events_pipeline(\n    sat_id,\n    tau: int = 60,\n    ts_mag: int = 1,\n    \n):\n  \n    ts_mag_str = f\"ts_{ts_mag}s\"\n    tau_str = f\"tau_{tau}s\"\n    \n    node_filter_sw_events = node(\n        filter_sw_events,\n        inputs=[\n            f\"events.{sat_id}_{ts_mag_str}_{tau_str}\",\n            f\"{sat_id}.STATE.inter_data_sw\",\n        ],\n        outputs=f\"events.{sat_id}_sw_{ts_mag_str}_{tau_str}\"\n        \n    )\n\n    nodes = [node_filter_sw_events]\n    return pipeline(nodes)\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from ARTHEMIS"
    ]
  },
  {
    "objectID": "notebooks/missions/themis/index.html#pipelines",
    "href": "notebooks/missions/themis/index.html#pipelines",
    "title": "IDs from ARTHEMIS",
    "section": "Pipelines",
    "text": "Pipelines\n::: {#cell-13 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef create_pipeline(\n    sat_id=\"THB\",\n    params: Optional[dict] = None,\n):\n    \n    if params is None:\n        params = PARAMS\n    tau = params[\"tau\"]\n    ts_state = params[sat_id][\"STATE\"].get(\"time_resolution\", 0)\n    ts_mag = params[sat_id][\"MAG\"].get(\"time_resolution\", 0)\n    ts_state_str = f\"ts_{ts_state}s\"\n\n\n    input_combined_data = {\n        f\"{sat_id}.STATE.primary_data_{ts_state_str}\": f\"OMNI.LowRes.primary_data_{ts_state_str}\"\n    }\n    \n    node_combined_data = pipeline(\n        create_combined_data_pipeline(sat_id),\n        inputs=input_combined_data,\n    )\n\n    return (\n        create_mag_data_pipeline(sat_id)\n        + create_state_data_pipeline(sat_id)\n        + node_combined_data\n        + create_sw_events_pipeline(sat_id, tau=tau, ts_mag=ts_mag)\n    )\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from ARTHEMIS"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/state.html",
    "href": "notebooks/missions/juno/state.html",
    "title": "JUNO State data pipeline",
    "section": "",
    "text": "JUNO state data is composed of two parts: the plasma data from MHD model and 1h-average magnetic field data from FGM, which providing background interplanetary magnetic field (IMF) information.\n::: {#cell-1 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n:::\n::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ 4=’\n’ 5=‘d’ 6=‘e’ 7=‘f’ 8=‘a’ 9=‘u’ 10=‘l’ 11=‘t’ 12=’_’ 13=‘e’ 14=‘x’ 15=‘p’ 16=’ ’ 17=‘p’ 18=‘i’ 19=‘p’ 20=‘e’ 21=‘l’ 22=‘i’ 23=‘n’ 24=‘e’ 25=‘s’ 26=‘/’ 27=‘j’ 28=‘u’ 29=‘n’ 30=‘o’ 31=‘/’ 32=‘s’ 33=‘t’ 34=‘a’ 35=‘t’ 36=‘e’}\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/state.html#getting-background-magnetic-field",
    "href": "notebooks/missions/juno/state.html#getting-background-magnetic-field",
    "title": "JUNO State data pipeline",
    "section": "Getting background magnetic field",
    "text": "Getting background magnetic field\n::: {#cell-4 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef process_IMF_data(\n    raw_data: Dict[str, Callable[..., pl.LazyFrame]],\n    ts: str = 3600,  # time resolution\n) -&gt; pl.DataFrame | dict[str, pl.DataFrame]:\n    \"\"\"\n    Resampling data to provide background magnetic field\n    \"\"\"\n\n    every = timedelta(seconds=ts)\n    period = every\n    offset = every / 2\n\n    data = pl.concat(\n        resample(func(), every=every, period=period, offset=offset)\n        for func in raw_data.values()\n    )\n\n    name_mapping = {\n        \"BX SE\": \"B_background_x\",\n        \"BY SE\": \"B_background_y\",\n        \"BZ SE\": \"B_background_z\",\n    }\n\n    return data.unique(\"time\").sort(\"time\").rename(name_mapping)\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/state.html#loading-data",
    "href": "notebooks/missions/juno/state.html#loading-data",
    "title": "JUNO State data pipeline",
    "section": "Loading data",
    "text": "Loading data\nFor interpolated solar wind at JUNO’s location, see model output file.\n::: {#cell-6 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef load_data(\n    raw_data: pd.DataFrame,\n    start: str,\n    end: str,\n) -&gt; pl.LazyFrame:\n    return pl.from_pandas(raw_data).lazy()\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/state.html#preprocessing-data",
    "href": "notebooks/missions/juno/state.html#preprocessing-data",
    "title": "JUNO State data pipeline",
    "section": "Preprocessing data",
    "text": "Preprocessing data\nCoordinate System:  HGI\nVariables:\n  Date_Time: date and time in ISO format [UT]\n  hour: elapsed time since trajectory start [hr]\n  r: radial coordinate in HGI [AU]\n  phi: longitude coordinate in HGI [deg]\n  Rho: density [amu/cm^3]\n  Ux, Uy, Uz: bulk velocity components in HGI [km/s]\n  Bx, By, Bz: magnetic field components in HGI [nT]\n  Ti: ion temperature [K]\n::: {#cell-8 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef preprocess_data(\n    raw_data: pl.LazyFrame,\n):\n    \"\"\"\n    Preprocess the raw dataset (only minor transformations)\n\n    - Parsing and typing data (like from string to datetime for time columns)\n    - Changing storing format (like from `csv` to `parquet`)\n    \"\"\"\n    df = (\n        raw_data\n        .with_columns(\n            time=pl.col(\"Date_Time\").str.to_datetime(),\n        )\n        .sort(\"time\")\n        .drop([\"Date_Time\", \"hour\"])\n    )\n    return df\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/state.html#processing-data",
    "href": "notebooks/missions/juno/state.html#processing-data",
    "title": "JUNO State data pipeline",
    "section": "Processing data",
    "text": "Processing data\nCombining plasma data and background magnetic field\n::: {#cell-10 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\nbcols_hgi = [\"bx\", \"by\", \"bz\"]\nbcols_rtn = [\"b_r\", \"b_t\", \"b_n\"]\nvcols_hgi = [\"ux\", \"uy\", \"uz\"]\nvcols_rtn = [\"v_r\", \"v_t\", \"v_n\"]\n\n\ndef hgi2rtn(df: pl.LazyFrame | pl.DataFrame):\n    \"\"\"Transform coordinates from HGI to RTN\"\"\"\n\n    phi_rad = pl.col(\"phi_rad\")\n    ux = pl.col(\"ux\")\n    uy = pl.col(\"uy\")\n    uz = pl.col(\"uz\")\n    result = (\n        df.with_columns(\n            phi_rad=pl.col(\"phi\").radians(),\n        )\n        .with_columns(\n            b_r=pl.col(\"bx\") * phi_rad.cos() + pl.col(\"by\") * phi_rad.sin(),\n            b_t=-pl.col(\"bx\") * phi_rad.sin() + pl.col(\"by\") * phi_rad.cos(),\n            b_n=pl.col(\"bz\"),\n            v_r=ux * phi_rad.cos() + uy * phi_rad.sin(),\n            v_t=-ux * phi_rad.sin() + uy * phi_rad.cos(),\n            v_n=uz,\n            plasma_speed=(ux**2 + uy**2 + uz**2).sqrt(),\n        )\n        .drop([\"phi\", \"phi_rad\"] + bcols_hgi + vcols_hgi)\n    )\n    return result\n\n:::\n::: {#cell-11 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef process_data(\n    model_data: pl.LazyFrame,\n    imf_data: pl.LazyFrame,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Corresponding to primary data layer, where source data models are transformed into domain data models\n\n    - Transforming data to RTN (Radial-Tangential-Normal) coordinate system\n    - Applying naming conventions for columns\n    \"\"\"\n\n    columns_name_mapping = {\n        \"r\": \"radial_distance\",\n        \"v_r\": \"v_x\",\n        \"v_t\": \"v_y\",\n        \"v_n\": \"v_z\",\n        \"b_r\": \"model_b_r\",\n        \"b_n\": \"model_b_n\",\n        \"b_t\": \"model_b_t\",\n        \"Ti\": \"plasma_temperature\",\n        \"rho\": \"plasma_density\",\n    }\n\n    return model_data.pipe(hgi2rtn).rename(columns_name_mapping).join(imf_data, on=\"time\")\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/juno/state.html#pipeline",
    "href": "notebooks/missions/juno/state.html#pipeline",
    "title": "JUNO State data pipeline",
    "section": "Pipeline",
    "text": "Pipeline\n::: {#cell-13 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef create_IMF_pipeline():\n    node_process_IMF_data = node(\n        process_IMF_data,\n        inputs=\"JNO.MAG.inter_data_1SEC\",\n        outputs=\"JNO.STATE.IMF_data\",\n    )\n\n    return pipeline([node_process_IMF_data])\n\n:::\n::: {#cell-14 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\nload_inputs = dict(\n    raw_data=\"model_data\",\n    start=\"params:start_date\", # necessary for the pipeline to work\n    end=\"params:end_date\",\n)\n\nprocess_inputs = dict(\n    model_data=\"inter_data_hourly\",\n    imf_data=\"IMF_data\",\n)\n\ndef create_pipeline(sat_id=\"JNO\", source=\"STATE\"):\n    return create_IMF_pipeline() + create_pipeline_template(\n        sat_id=sat_id,\n        source=source,\n        load_data_fn=load_data,\n        preprocess_data_fn=preprocess_data,\n        process_data_fn=process_data,\n        load_inputs=load_inputs,\n        process_inputs=process_inputs,\n    )\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "notebooks/missions/omni/index.html",
    "href": "notebooks/missions/omni/index.html",
    "title": "OMNI data",
    "section": "",
    "text": "Reference:\nNotes:\n::: {#cell-2 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ execution_count=1}\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "notebooks/missions/omni/index.html#setup",
    "href": "notebooks/missions/omni/index.html#setup",
    "title": "OMNI data",
    "section": "Setup",
    "text": "Setup\nNeed to run command in shell first as pipeline is project-specific command\nkedro pipeline create omni",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "notebooks/missions/omni/index.html#downloading-data",
    "href": "notebooks/missions/omni/index.html#downloading-data",
    "title": "OMNI data",
    "section": "Downloading data",
    "text": "Downloading data\n::: {#cell-7 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef download_data(\n    start,\n    end,\n    datatype,\n):\n    import pyspedas\n\n    trange = [start, end]\n    files = pyspedas.omni.data(trange=trange, datatype=datatype, downloadonly=True)\n    return files\n\n\ndef load_data(\n    start,\n    end,\n    datatype=\"hourly\",\n    vars: dict = OMNI_VARS,\n) -&gt; pl.LazyFrame:\n    files = download_data(start, end, datatype=datatype)\n    df: pl.LazyFrame = pl.concat(files | pmap(cdf2pl, var_names=list(vars)))\n    return df\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "notebooks/missions/omni/index.html#preprocessing-data",
    "href": "notebooks/missions/omni/index.html#preprocessing-data",
    "title": "OMNI data",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n::: {#cell-9 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef preprocess_data(\n    raw_data: pl.LazyFrame,\n    vars: dict = OMNI_VARS,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Preprocess the raw dataset (only minor transformations)\n\n    - Applying naming conventions for columns\n    - Extracting variables from `CDF` files, and convert them to DataFrame\n    \"\"\"\n\n    columns_name_mapping = {key: value[\"COLNAME\"] for key, value in vars.items()}\n\n    return raw_data.rename(columns_name_mapping)\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "notebooks/missions/omni/index.html#processing-data",
    "href": "notebooks/missions/omni/index.html#processing-data",
    "title": "OMNI data",
    "section": "Processing data",
    "text": "Processing data\n::: {#cell-11 .cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’}\n\nCode\ndef flow2gse(df: pl.LazyFrame) -&gt; pl.LazyFrame:\n    \"\"\"\n    - Transforming solar wind data from `Quasi-GSE` coordinate to GSE coordinate system\n    \"\"\"\n    plasma_speed = pl.col(\"plasma_speed\")\n    sw_theta = pl.col(\"sw_vel_theta\")\n    sw_phi = pl.col(\"sw_vel_phi\")\n\n    return df.with_columns(\n        sw_vel_gse_x=-plasma_speed * sw_theta.cos() * sw_phi.cos(),\n        sw_vel_gse_y=+plasma_speed * sw_theta.cos() * sw_phi.sin(),\n        sw_vel_gse_z=+plasma_speed * sw_theta.sin(),\n    ).drop([\"sw_theta\", \"sw_phi\"])\n\ndef process_data(\n    raw_data: pl.LazyFrame,\n    ts=None,  # time resolution\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    - Transforming data to GSE coordinate system\n    \"\"\"\n\n    return raw_data.pipe(flow2gse).rename(\n        {\n            \"sw_vel_gse_x\": \"v_x\",\n            \"sw_vel_gse_y\": \"v_y\",\n            \"sw_vel_gse_z\": \"v_z\",\n        }\n    )\n\n:::",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "notebooks/missions/omni/index.html#pipelines",
    "href": "notebooks/missions/omni/index.html#pipelines",
    "title": "OMNI data",
    "section": "Pipelines",
    "text": "Pipelines\n\n\nCode\n# # | export\n# def create_pipeline(sat_id=\"OMNI\", source=\"LowRes\"):\n\n#     return create_pipeline_template(\n#         sat_id=sat_id,\n#         source=source,\n#         load_data_fn=load_data,\n#         preprocess_data_fn=preprocess_data,\n#         process_data_fn=process_data,\n#     )",
    "crumbs": [
      "Home",
      "Notebooks",
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "notebooks/01_results_julia.html",
    "href": "notebooks/01_results_julia.html",
    "title": "Results (Julia)",
    "section": "",
    "text": "Code\nusing RCall\ninclude(\"main.jl\")\n\n\nWARNING: replacing module DiscontinuityIO.\n\n\nL\"Radial Distance ($AU$)\"\nCode\nj_events_taus = load_taus(60:-10:20)\n\n# find the numbers of events for each tau\ncombine(groupby(j_events_taus, :tau), nrow)\n\n\nNumber of events: 18088\nNumber of events: 20129\nNumber of events: 22606\nNumber of events: 26333\nNumber of events: 31793\n\n\n\n5×2 DataFrame\n\n\n\nRow\ntau\nnrow\n\n\n\nInt64\nInt64\n\n\n\n\n1\n20\n31793\n\n\n2\n30\n21007\n\n\n3\n40\n14952\n\n\n4\n50\n11904\n\n\n5\n60\n9023\nCode\nw_events = load(\"data/events.Wind.fit.ts_0.09s_tau_60s.arrow\");\n\n\n┌ Warning: automatically converting Arrow.Timestamp with precision = MICROSECOND to `DateTime` which only supports millisecond precision; conversion may be lossy; to avoid converting, pass `Arrow.Table(source; convert=false)\n└ @ Arrow /Users/zijin/.julia/packages/Arrow/5pHqZ/src/eltypes.jl:273",
    "crumbs": [
      "Results (Julia)"
    ]
  },
  {
    "objectID": "notebooks/01_results_julia.html#check-the-discontinuities-properties-with-time-and-radial-distance",
    "href": "notebooks/01_results_julia.html#check-the-discontinuities-properties-with-time-and-radial-distance",
    "title": "Results (Julia)",
    "section": "Check the discontinuities properties with time and radial distance",
    "text": "Check the discontinuities properties with time and radial distance\n\n\nCode\ndatalimits_f = x -&gt; quantile(x, [0.02, 0.98])\n\n\n#32 (generic function with 1 method)\n\n\n\n\nCode\ndf = w_events\nplot_dist(\n    data(df) * mapping(color = :year);\n    datalimits=datalimits_f\n)\neasy_save(\"wind_distribution_time\")\n\n\n┌ Info: Saved /Users/zijin/projects/ids_spatial_evolution_juno/figures/wind_distribution_time.png\n└ @ beforerr /Users/zijin/.julia/dev/beforerr.jl/src/utils/makie.jl:34\n\n\n\n\n\n\n\nCode\nplot_dist(\n    data(df) * mapping(color = :year);\n    maps = (density_log_map, B_log_map, b_log_map,),\n    datalimits=datalimits_f,\n)\neasy_save(\"sw_paramters_time\")\n\n\n┌ Info: Saved /Users/zijin/projects/ids_spatial_evolution_juno/figures/sw_paramters_time.png\n└ @ beforerr /Users/zijin/.julia/dev/beforerr.jl/src/utils/makie.jl:34\n\n\n\n\n\n\n\nCode\ndf = w_events\n\nplot_dist(\n    data(df) * mapping(color = :year);\n    datalimits=datalimits_f,\n)\neasy_save(\"wind_distribution_time\")\n\n\n\n\nCode\nplot_dist(\n    data(j_events_taus) * mapping(color=:r);\n    datalimits=datalimits_f,\n)\neasy_save(\"juno_distribution_r\")\n\n\n┌ Info: Saved /Users/zijin/projects/ids_spatial_evolution_juno/figures/juno_distribution_r.png\n└ @ beforerr /Users/zijin/.julia/dev/beforerr.jl/src/utils/makie.jl:34\n\n\n\n\n\n\nThe effect of tau\n\n\nCode\nplot_dist(\n    data(j_events_taus_unique) * mapping(color=:label, row = :r);\n    maps=(l_log_map, l_norm_log_map, j_log_map, j_norm_log_map, b_log_map,),\n    datalimits=datalimits_f,\n    figure_kwargs=(size=(1200, 1000),)\n)\n\neasy_save(\"juno_distribution_r_taus\")\n\n\n┌ Info: Saved /Users/zijin/projects/ids_spatial_evolution_juno/notebooks/figures/juno_distribution_r_taus.png\n└ @ beforerr /Users/zijin/.julia/dev/beforerr.jl/src/utils/makie.jl:34",
    "crumbs": [
      "Results (Julia)"
    ]
  },
  {
    "objectID": "notebooks/01_results_julia.html#archived-following-codes-may-not-work",
    "href": "notebooks/01_results_julia.html#archived-following-codes-may-not-work",
    "title": "Results (Julia)",
    "section": "Archived (Following codes may not work)",
    "text": "Archived (Following codes may not work)\n\n\nCode\nj_events_low_fit = load()\nj_events_tau_20_fit = load(tau = 20)\nj_events_high_fit = load(ts = 0.12)\n\n# add a label column to the dataframes\nj_events_low_fit.label .= \"1 Hz (fit)\"\nj_events_high_fit.label .= \"8 Hz (fit)\"\nj_events_tau_20_fit.label .= \"1 Hz, 20s (fit)\"\nj_events = reduce(vcat, [j_events_low_fit, j_events_high_fit, j_events_tau_20_fit]) |&gt; process;\n\n\n\n\nCode\n# NOTE: log axis for density is not working now\nfunction plot_l_j_r(df)\n    data_layer = data(df) * mapping(row=:r, color=:label)\n    plot_dist(data_layer; datalimits=datalimits_f, figure_kwargs=(size=(1200, 1200),))\nend\n\n\nplot_l_j_r (generic function with 1 method)\n\n\n\n\nCode\nplot_l_j_r(j_events)\n\n\n\n\n\n\n\nCode\nfunction plot_l_j_r_hist(\n    df;\n    maps=(l_map, l_norm_map, j_map, j_norm_map),\n    f_maps=(row=:r, color=:label),\n    normalization=:pdf,\n    fig_options = (size = (1200, 1000),)\n)\n    fig = Figure(;fig_options)\n\n    plt = data(df) * mapping(; f_maps...) * (visual(Lines) + visual(Scatter))\n    plt *= histogram(datalimits=datalimits_f, normalization=normalization)\n\n    plts = [plt * mapping(m) for m in maps]\n    axis = (; yscale=log10)\n\n    grids = [draw!(fig[1:5, i], p, axis=axis) for (i, p) in enumerate(plts)]\n    legend!(fig[0, 1:end], grids[1], titleposition=:left, orientation=:horizontal)\n\n    fig\nend\n\n\nplot_l_j_r_hist (generic function with 1 method)\n\n\n\n\nCode\nplot_l_j_r_hist(j_events)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nCode\nmaps = (l_log_map, l_norm_log_map, j_log_map, j_norm_log_map)\nplot_l_j_r_hist(j_events; maps = maps, normalization = :none)\n\n\n\n\n\n\n\nCode\nfunction plot_l_j_r_hist(\n    df;\n    maps=(l_map, l_norm_map, j_map, j_norm_map),\n    f_maps = (color=:r,),\n    size = (1200, 800),\n)\n    fig = Figure(size=size)\n    axis = (; yscale=log10)\n\n    plt = data(df) * mapping(; f_maps...) * (visual(Lines) + visual(Scatter))\n    plt_h = plt * histogram(datalimits=datalimits_f, normalization=:pdf)\n    plts = [plt_h * mapping(m) for m in maps]\n    grids = [draw!(fig[1, i], p, axis=axis) for (i, p) in enumerate(plts)]\n\n\n    plt_h = plt * histogram(datalimits=datalimits_f)\n    plts = [plt_h * mapping(m) for m in maps]\n    grids = [draw!(fig[2, i], p, axis=axis) for (i, p) in enumerate(plts)]\n    legend!(fig[0, 1:end], grids[1], titleposition=:left, orientation=:horizontal)\n\n    fig\nend\n\nplot_l_j_r_hist(j_events_low_fit; maps=maps, f_maps=f_maps)\n\n\n\n\n\n\n\nCode\n@rput j_events\n@rput j_events_taus\n@rput j_events_taus_u\n@rput j_events_low_fit\n@rput j_events_tau_20_fit\n\n\nR\"\"\"\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(patchwork)\nsource('utils.R')\n\"\"\"\n\n\nRObject{VecSxp}\n$value\nfunction (df1, df2, p1title = p1title, y_lim_1 = c(500, 40000), \n    y_lim_2 = c(1, 200)) \n{\n    x_col &lt;- \"radial_distance\"\n    x_bins &lt;- 5\n    y_bins &lt;- 12\n    add_mode &lt;- TRUE\n    y_col &lt;- \"L_k\"\n    y_lim &lt;- y_lim_1\n    ylab &lt;- lab_l_log\n    p &lt;- plot_binned_data(df1, x_col = x_col, y_col = y_col, \n        x_bins = x_bins, y_bins = y_bins, y_lim = y_lim, y_log = TRUE, \n        add_mode = add_mode)\n    p1 &lt;- p + labs(x = NULL, y = ylab) + ggtitle(p1title)\n    y_col &lt;- \"L_k_norm\"\n    y_lim &lt;- y_lim_2\n    ylab &lt;- lab_l_norm_log\n    p &lt;- plot_binned_data(df1, x_col = x_col, y_col = y_col, \n        x_bins = x_bins, y_bins = y_bins, y_lim = y_lim, y_log = TRUE, \n        add_mode = add_mode)\n    p2 &lt;- p + labs(x = x_lab_r, y = ylab)\n    x_col &lt;- \"time\"\n    y_col &lt;- \"L_k\"\n    y_lim &lt;- y_lim_1\n    p &lt;- plot_binned_data(df2, x_col = x_col, y_col = y_col, \n        x_bins = x_bins, y_bins = y_bins, y_lim = y_lim, y_log = TRUE, \n        add_mode = add_mode)\n    p3 &lt;- p + labs(x = NULL, y = NULL) + ggtitle(p2title)\n    y_col &lt;- \"L_k_norm\"\n    y_lim &lt;- y_lim_2\n    p &lt;- plot_binned_data(df2, x_col = x_col, y_col = y_col, \n        x_bins = x_bins, y_bins = y_bins, y_lim = y_lim, y_log = TRUE, \n        add_mode = add_mode)\n    p4 &lt;- p + labs(x = x_lab_t, y = NULL)\n    (p1 + p2 + p3 + p4) + layout & scale_fill_viridis_c(limits = c(0.01, \n        0.25), trans = \"log10\", name = \"pdf\")\n}\n\n$visible\n[1] FALSE\n\n\n\n\n\nCode\nR\"\"\"\ndensity_plot &lt;- function(df,\n  x,\n  density.y = \"density\",\n  facet.by = \"r\",\n  color = \"label\",\n  strip.position = \"top\")\n{\n  p &lt;- ggdensity(\n    df, \n    x = x, y = density.y,\n    color = color,\n    facet.by = facet.by,\n    add = \"median\",\n    alpha = 0\n  ) %&gt;%\n  facet(facet.by, ncol = 1, strip.position = strip.position)\n}\n\nplot &lt;- function(\n  df,\n  d1_ulim = 8000,\n  d2_ulim = 40,\n  d3_ulim = 8,\n  d4_ulim = 1.2,\n  xscale = \"none\",\n  yscale = \"log10\",\n  p1_xlim = NULL,\n  p1_ylim = NULL,\n  p2_xlim = NULL,\n  p2_ylim = NULL, # c(10^-3, 1)\n  p3_xlim = NULL, p3_ylim = NULL,\n  p4_xlim = NULL, p4_ylim = NULL,\n  plot_func = density_plot,\n  ...\n) {\n\n  p1 &lt;- plot_func(\n    filter(df, L_k &lt; d1_ulim), \"L_k\", ...\n  )\n  \n  p2 &lt;- plot_func(\n    filter(df, L_k_norm &lt; d2_ulim), \"L_k_norm\", ...\n  )\n  \n  p3 &lt;- plot_func(\n    filter(df, j0_k &lt; d3_ulim), \"j0_k\", ...\n  )\n  \n  p4 &lt;- plot_func(\n    filter(df, j0_k_norm &lt; d4_ulim), \"j0_k_norm\", strip.position = \"right\", ...\n  )\n\n  p1 &lt;- ggpar(p1,\n      xscale=xscale, xlim = p1_xlim,\n      yscale=yscale, ylim = p1_ylim\n    ) + labs(x = lab_l)\n  \n  p2 &lt;- ggpar(p2, \n      xscale=xscale, xlim = p2_xlim,\n      yscale=yscale, ylim = p2_ylim, ylab=FALSE\n    ) + labs(x = lab_l_norm)\n  \n  p3 &lt;- ggpar(p3,\n      xscale=xscale, xlim = p3_xlim,\n      yscale=yscale, ylim = p3_ylim, ylab=FALSE\n    ) + labs(x = lab_j)\n  \n  p4 &lt;- ggpar(p4,\n      xscale=xscale, xlim = p4_xlim,\n      yscale=yscale, ylim = p4_ylim, ylab=FALSE\n    ) + labs(x = lab_j_norm)\n  \n  tag_pool &lt;- unique(df$r)\n  \n  p1 &lt;- tag_facet(p1, open = \"(a.\", tag_pool = tag_pool)\n  p2 &lt;- tag_facet(p2, open = \"(b.\", tag_pool = tag_pool)\n  p3 &lt;- tag_facet(p3, open = \"(c.\", tag_pool = tag_pool)\n  p4 &lt;- tag_facet(p4, open = \"(d.\", tag_pool = tag_pool) + theme(\n    strip.background = element_blank(),\n    strip.text.x = element_blank()\n  )\n\n  p &lt;- p1 + p2 + p3 + p4\n  p &lt;- p + plot_layout(nrow = 1, guides = \"collect\") & theme(legend.position='top')  \n}\n\"\"\"\n\n\n\nProperties over radial distance with different tau values\n\n\nCode\nR\"\"\"\nplot(\n    j_events_taus,\n    d1_ulim = Inf, p1_xlim = c(0,1e4), p1_ylim = c(1e-5, 1e-3),\n    d2_ulim = 25, p2_xlim = c(0,25), p2_ylim = c(1e-2, 1e0),\n    d3_ulim = 20, p3_xlim = c(0,8), p3_ylim = c(1e-2, 1e0),\n    d4_ulim = 2, p4_xlim = c(0, 1), p4_ylim =  c(1e-1, 1e1),\n)\n\nsave_plot(\"l_j_fit_taus\", width = 15, height = 5)\n\n\nplot(\n    j_events_taus,\n    xscale = \"log10\",\n    d1_ulim = Inf, p1_xlim = c(1e2, 1e5), p1_ylim = c(1e-2, 1e0),\n    d2_ulim = Inf, p2_xlim = c(1e-1, 1e2), p2_ylim = c(1e-2, 1e0),\n    d3_ulim = Inf, p3_xlim = c(1e-2, 5e1), p3_ylim = c(1e-2, 1e0),\n    d4_ulim = Inf, p4_xlim = c(1e-3, 5e0), p4_ylim = c(1e-2, 1e0),\n)\n\nsave_plot(\"l_j_fit_taus_xscale_log\", width = 15, height = 5)\n\"\"\"\n\n\n\n\nCode\nR\"\"\"\nplot(\n    j_events_taus_u,\n    xscale = \"log10\",\n    d1_ulim = Inf, p1_xlim = c(1e2, 1e5), p1_ylim = c(1e-2, 1e0),\n    d2_ulim = Inf, p2_xlim = c(1e-1, 1e2), p2_ylim = c(1e-2, 1e0),\n    d3_ulim = Inf, p3_xlim = c(1e-2, 5e1), p3_ylim = c(1e-2, 1e0),\n    d4_ulim = Inf, p4_xlim = c(1e-3, 5e0), p4_ylim = c(1e-2, 1e0),\n    facet.by = NULL, color = \"r\"\n)\n\nsave_plot(\"l_j_fit_xscale_log\", width = 15, height = 5)\n\nplot(\n    j_events_taus_u,\n    d1_ulim = Inf, p1_xlim = c(0, 1e4), p1_ylim = c(1e-5, 1e-3),\n    d2_ulim = Inf, p2_xlim = c(0, 25), p2_ylim = c(1e-2, 1e0),\n    d3_ulim = 20, p3_xlim = c(0,8), p3_ylim = c(1e-2, 1e0),\n    d4_ulim = 2, p4_xlim = c(0, 1), p4_ylim =  c(1e-1, 1e1),\n    facet.by = NULL, color = \"r\",\n)\n\nsave_plot(\"l_j_fit\", width = 15, height = 5)\n\"\"\"\n\n\nl_j_fit\nl_j_fit_xscale_log\nl_j_fit_xscale_log_count\nl_j_fit_taus\nl_j_fit_taus_xscale_log\n\nCheck the effect of time resolution and time window\n\n\nCode\nR\"\"\"\n# plot(j_events_der)\n# save_plot(\"l_j_r_der\", width = 15, height = 10)\n# plot(\n#     j_events,\n#     p1_ylim = c(1e-5, 1e-3),\n# )\n# save_plot(\"l_j_r_fit\", width = 15, height = 10)\n\nplot(\n    j_events,\n    xscale = \"log10\",\n    d1_ulim = Inf, p1_xlim = c(1e2, 1e5), p1_ylim = c(1e-2, 1e0),\n    d2_ulim = Inf, p2_xlim = c(1e-1, 1e2), p2_ylim = c(1e-2, 1e0),\n    d3_ulim = Inf, p3_xlim = c(1e-2, 5e1), p3_ylim = c(1e-2, 1e0),\n    p4_xlim = c(1e-3, 5e0), p4_ylim = c(1e-2, 1e0),\n)\nsave_plot(\"l_j_r_fit_xscale_log\", width = 15, height = 10)\n\"\"\"\n\n\nl_j_r_fit\nl_j_r_der\nl_j_r_fit_xscale_log\n\n\n\nPlot the median of discontinuity properties with radial distance\n\n\nCode\nfunction q25(x)\n    quantile(x, 0.25)\nend\n\nfunction q75(x)\n    quantile(x, 0.75)\nend\n\nfunction stat_info(df; group_cols = [:r, :label])\n    cols = [:L_k, :L_k_norm, :j0_k, :j0_k_norm]\n    funcs = [median mean std q25 q75]\n    \n    @chain df begin\n        groupby(group_cols)\n        combine(cols .=&gt; funcs)\n    end\nend\n\n\nstat_info (generic function with 1 method)\n\n\n\n\nCode\n### Plot the median of discontinuity properties with radial distance\nfunction plot_median_r(df; add_error=false)\n    df_m = stat_info(df)\n\n    fig = Figure()\n    plt = data(df_m) * mapping(:r =&gt; r_lab, color=:label)\n\n    mappings = [\n        (\"L_k\", l_lab),\n        (\"L_k_norm\", l_norm_lab),\n        (\"j0_k\", j_lab),\n        (\"j0_k_norm\", j_norm_lab)\n    ]\n\n    function plt_map(plt, col, label)\n        plt *= mapping(\"$(col)_median\" =&gt; label)\n        lines_map = (visual(Lines) + visual(Scatter))\n        errorbars_map = mapping(\"$(col)_q25\", \"$(col)_q75\") * visual(Errorbars, alpha=0.1, whiskerwidth=10)\n\n        maps = [lines_map]\n        add_error && push!(maps, errorbars_map)\n        \n        plt * reduce(+, maps)\n    end\n\n    plts = [plt_map(plt, col, label) for (col, label) in mappings]\n    popositions = [(1, 1), (1, 2), (2, 1), (2, 2)] # Define grid positions\n    grids = [draw!(fig[pos...], plt) for (pos, plt) in zip(popositions, plts)]\n    pretty_legend!(fig, grids[1])\n\n    fig\nend\n\n\nplot_median_r (generic function with 3 methods)\n\n\n\n\nCode\nplot_median_r(j_events)\n\n\n\n\n\n\n\nCode\nplot_median_r(j_events_taus)\n\n\n\n\n\n\n\nCode\nplot_median_r(j_events)\n\n\n\n\n\n\n\nCode\nplot_median_r(j_events_taus)\n\n\n\n\n\n\n\nCode\nplot_median_r(j_events; add_error=true)\n\n\n\n\n\n\n\nPlot different radial distances in the same plot\n\n\nCode\nfunction plot_l_r_one(df)\n    fig = Figure(size=(1000, 500))\n\n    plt = data(df) * mapping(col=:label, color=:r)\n    \n    grid1 = draw!( fig[1, 1:2], plt * mapping(:L_k_norm) * density(datalimits=((0, 50),)) )\n    grid2 = draw!( fig[2, 1:2], plt * mapping(:L_k) * density(datalimits=((0, 5000),)) )\n    legend!(fig[0, 1:end], grid1, titleposition=:left, orientation=:horizontal)\n\n    fig\nend\n\nplot_l_r_one(j_events)\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "Results (Julia)"
    ]
  },
  {
    "objectID": "notebooks/01_results_julia.html#plasma-properties-with-radial-distance",
    "href": "notebooks/01_results_julia.html#plasma-properties-with-radial-distance",
    "title": "Results (Julia)",
    "section": "Plasma properties with radial distance",
    "text": "Plasma properties with radial distance\n\n\nCode\nplt = data(j_events) * mapping(:radial_distance, :Alfven_speed, color=:label) * (visual(Scatter) + smooth())\nplt |&gt; draw\n\n\n\n\n\n\n\nCode\nplt = data(j_events) * mapping(:radial_distance, jA_map, color=:label) * (visual(Scatter) + smooth())\nplt |&gt; draw\n\n\n\n\n\n\n\nCode\nusing Statistics\n\n\n\n\nCode\n# groupby r and describe the data for each group \n# j_events |&gt; @groupby(_.r) |&gt; @map({r=key(_), j0_k=describe(_.j0_k), L_k=describe(_.L_k)})\n@chain j_events begin\n    groupby(:r)\n    combine(:plasma_density =&gt;  mean, :ion_inertial_length =&gt; mean, :b_mag =&gt; mean) \nend\n\n\n\n5×4 DataFrame\n\n\n\nRow\nr\nplasma_density_mean\nion_inertial_length_mean\nb_mag_mean\n\n\n\nString\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1.0\n3.10512\n152.217\n3.75957\n\n\n2\n2.0\n1.42267\n242.722\n2.10041\n\n\n3\n3.0\n0.674452\n317.706\n1.45649\n\n\n4\n4.0\n0.417696\n434.665\n1.3097\n\n\n5\n5.0\n0.343857\n569.222\n1.27077\n\n\n\n\n\n\n\n\n\nCode\nplt = data(j_events) * mapping(:radial_distance, :plasma_density, color=:label) * (visual(Scatter) + smooth())\nplt |&gt; draw\n\n\n\n\n\n\n\nCode\nplt = data(j_events) * mapping(:radial_distance, di_map, color=:label) * (visual(Scatter) + smooth())\nplt |&gt; draw",
    "crumbs": [
      "Results (Julia)"
    ]
  },
  {
    "objectID": "notebooks/01_results_julia.html#check-discontinuities-properties-in-relation-to-the-local-plasma-properties",
    "href": "notebooks/01_results_julia.html#check-discontinuities-properties-in-relation-to-the-local-plasma-properties",
    "title": "Results (Julia)",
    "section": "Check discontinuities properties in relation to the local plasma properties",
    "text": "Check discontinuities properties in relation to the local plasma properties\n\n\nCode\nbegin\n    fig = Figure(size=(1000, 800))\n\n    datalayer = mapping() * visual(Scatter, markersize=1, color=:white, alpha=0.1)\n\n    begin\n        layer = histogram(bins=range(1, 5, length=64), normalization=:pdf)\n        plt = layer * mapping(di_log_map, l_log_map)\n        plt1 = data(j_events) * mapping(layout=:r) * plt\n        plt2 = data(w_events) * plt\n    \n        l_log_limit = ((1.5, 3.5), (1.5, 4.5))\n        axis = (;limits=l_log_limit)\n        draw!(fig[1:2,1:3], plt1, axis=axis)\n        draw!(fig[1:2,4:5], plt2, axis=axis) \n    end\n\n    # Current Density Panels\n    begin\n        layer = histogram(bins=range(-2, 3, length=64), normalization=:pdf)\n        plt = layer * mapping(jA_log_map, j_log_map)\n        plt3 = data(j_events) * mapping(layout=:r) * plt\n        plt4 = data(w_events) * plt\n    \n        j_log_limit = ((-1, 3), (-2, 2))\n        axis = (;limits=j_log_limit)\n        draw!(fig[3:4,1:3], plt3, axis=axis)\n        draw!(fig[3:4,4:5], plt4, axis=axis)\n    end\n\n    # Make ablines across different r panels\n    # begin\n        # ablines_data = (; intercepts=[-3,-1,1], slopes=[1,1,1]) \n        # lines = data(ablines_data) * mapping(:intercepts, :slopes) * visual(ABLines, linestyle=:dash)\n        # draw!(fig[3:4,1:3], lines)\n    # end\n\n    Label(fig[0,1:3], \"Juno in Different Radial Distances\", fontsize=20)\n    Label(fig[0,4:5], \"Wind 11 Hz\", fontsize=20)\n\n    fig\nend\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nCode\nbegin\n    fig = Figure(size=(1000, 800))\n\n    pdflayer = density() * visual(Contour)\n    # small scatter points\n    datalayer = mapping() * visual(Scatter, markersize=3)\n\n    # layer = pdflayer + datalayer\n    layer = datalayer\n    plt = layer * mapping(di_map, l_map)\n    plt1 = data(j_events) * mapping(layout=:r) * plt\n    plt2 = data(w_events) * plt\n\n    l_limit = ((10^1, 10^5), (10^1, 10^5))\n    axis = merge(log_axis, (;limits=l_limit))\n    draw!(fig[1:2,1:3], plt1, axis=axis)\n    draw!(fig[1:2,4:5], plt2, axis=axis)\n\n    plt = layer * mapping(jA_map, j_map)\n    plt3 = data(j_events) * mapping(layout=:r) * plt\n    plt4 = data(w_events) * plt\n\n    j_limit = ((10^-1, 10^3), (10^-2, 10^2))\n    axis = merge(log_axis, (;limits=j_limit))\n    draw!(fig[3:4,1:3], plt3, axis=axis)\n    draw!(fig[3:4,4:5], plt4, axis=axis)\n\n\n    Label(fig[0,1:3], \"Juno in Different Radial Distances\", fontsize=20)\n    Label(fig[0,4:5], \"Wind 11 Hz\", fontsize=20)\n\n    fig\nend\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\nCode\nbegin\n    fig = Figure(size=(1000, 800))\n    datalimits_f = x -&gt; quantile(x, [0.05, 0.95])\n\n    begin\n        layer = histogram(bins=24, datalimits=datalimits_f, normalization=:pdf)\n        plt = layer * mapping(l_norm_log_map, j_norm_log_map)\n        plt1 = data(j_events) * mapping(layout=:r) * plt\n        plt2 = data(w_events) * plt\n\n        axis = (;)\n        draw!(fig[1:2, 1:3], plt1, axis=axis)\n        draw!(fig[1:2, 4:5], plt2, axis=axis)\n    end\n\n    Label(fig[0, 1:3], \"Juno in Different Radial Distances\", fontsize=20)\n    Label(fig[0, 4:5], \"Wind 11 Hz\", fontsize=20)\n\n    fig\nend\n\nfig",
    "crumbs": [
      "Results (Julia)"
    ]
  },
  {
    "objectID": "notebooks/01_results_julia.html#derivative-method-validation",
    "href": "notebooks/01_results_julia.html#derivative-method-validation",
    "title": "Results (Julia)",
    "section": "Derivative method validation",
    "text": "Derivative method validation\n\n\nCode\nfig = plot_l_j_r(j_events_der)\nfig\n\n\n\n\n\n\n\nFigure 5",
    "crumbs": [
      "Results (Julia)"
    ]
  },
  {
    "objectID": "notebooks/20_model.html",
    "href": "notebooks/20_model.html",
    "title": "Models",
    "section": "",
    "text": "::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=1}\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n:::\n\n\nCode\nfrom loguru import logger\nfrom typing import Dict\n\nimport hvplot.polars\n\nfrom datetime import timedelta\nimport numpy as np\nimport polars as pl\nimport polars.selectors as cs\nfrom discontinuitypy.utils.polars import pl_norm\n\n\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n%load_ext rpy2.ipython\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "notebooks/20_model.html#setup",
    "href": "notebooks/20_model.html#setup",
    "title": "Models",
    "section": "",
    "text": "::: {#cell-2 .cell 0=‘h’ 1=‘i’ 2=‘d’ 3=‘e’ execution_count=1}\n\nCode\n%load_ext autoreload\n%autoreload 2\n\n:::\n\n\nCode\nfrom loguru import logger\nfrom typing import Dict\n\nimport hvplot.polars\n\nfrom datetime import timedelta\nimport numpy as np\nimport polars as pl\nimport polars.selectors as cs\nfrom discontinuitypy.utils.polars import pl_norm\n\n\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n%load_ext rpy2.ipython\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "notebooks/20_model.html#process-model-data",
    "href": "notebooks/20_model.html#process-model-data",
    "title": "Models",
    "section": "Process model data",
    "text": "Process model data\n\n\nCode\ndef overview(df: pl.DataFrame, bcols, vcols):\n    \"\"\"Overview of the data\"\"\"\n    df_pd = df.to_pandas()\n    df_pd.hvplot(x=\"time\", y=bcols)\n    \n    b_fig = df_pd.hvplot.line(x=\"time\", y=bcols)\n    v_fig = df_pd.hvplot.line(x=\"time\", y=vcols)\n    rho_fig = df_pd.hvplot.line(x=\"time\", y=\"rho\", logy=True)\n    Ti_fig = df_pd.hvplot.line(x=\"time\", y=\"Ti\", logy=True)\n    return (b_fig + v_fig + rho_fig + Ti_fig).cols(1).opts(shared_axes=False)\n\n\n\n\nCode\njno_state_data : pl.DataFrame = pl.read_parquet('../data/03_primary/JNO_STATE_ts_3600s.parquet')\njno_state_data.columns\n\n\n['radial_distance',\n 'plasma_density',\n 'plasma_temperature',\n 'time',\n 'model_b_r',\n 'model_b_t',\n 'model_b_n',\n 'v_x',\n 'v_y',\n 'v_z',\n 'plasma_speed',\n 'B_background_x',\n 'B_background_y',\n 'B_background_z']",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "notebooks/20_model.html#hvplot",
    "href": "notebooks/20_model.html#hvplot",
    "title": "Models",
    "section": "Hvplot",
    "text": "Hvplot\n\n\nCode\n\ntime_column = \"time\"\nts = timedelta(seconds=3600)\nevery = timedelta(days=3)\n###\nwidth = 1600\nheight = 600\n\nlogy = True\nalpha = 0.618\n###\n\nmodel_b_col = \"MSWIM2D\"\njuno_b_col = \"Juno\"\n\ndf = (\n    jno_state_data.with_columns(\n        model_b=pl_norm(\"model_b_r\", \"model_b_t\", \"model_b_n\"),\n        B_background=pl_norm(\"B_background_x\", \"B_background_y\", \"B_background_z\"),\n    )\n    .sort(time_column)\n    .upsample(time_column, every=ts)\n    .rename({\"model_b\": model_b_col, \"B_background\": juno_b_col})\n)\n\ndf_avg = df.group_by_dynamic(time_column, every=every).agg(cs.numeric().mean())\n\npanel01 = df_avg.hvplot(\n    x=time_column,\n    y=[model_b_col, juno_b_col],\n    logy=logy,\n    color=[\"red\", \"black\"],\n    ylabel=\"Magnetic Field (nT)\",\n    width=width,\n    height=height,\n    xaxis=None,\n) * df.hvplot(\n    x=time_column,\n    y=juno_b_col,\n    color=\"gray\",\n    alpha=alpha,\n    label=\"Juno (high res)\",\n    xaxis=None,\n)\n\npanel02 = df.hvplot(\n    x=time_column, y=\"radial_distance\", width=width, ylabel=\"Radial Distance (AU)\"\n)\n\n\n(panel01.opts(legend_position=\"top_right\") + panel02).cols(1)",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "notebooks/20_model.html#r-plot-publication-quality",
    "href": "notebooks/20_model.html#r-plot-publication-quality",
    "title": "Models",
    "section": "R plot (Publication quality)",
    "text": "R plot (Publication quality)\n\n\nCode\n%R -i df_avg -c conv_pl\n%R -i df -c conv_pl\n\n\n\n\nCode\n%%R -w 1200\ntime_column &lt;- \"time\"\nmodel_b_col &lt;- \"MSWIM2D\"\njuno_b_col &lt;- \"Juno\"\n\nalpha &lt;- 0.3\n\nfilename &lt;- \"model/juno_model_validation\"\n\n\nmagnetic_field_plot &lt;- ggplot(data = df_avg, aes(x = .data[[time_column]])) +\n  geom_line(aes(y = .data[[model_b_col]], color = \"MSWIM2D\")) +\n  geom_line(aes(y = .data[[juno_b_col]], color = \"Juno\")) +\n  geom_line(data = df, aes(y = .data[[juno_b_col]], color = \"Juno (high res)\"), alpha = alpha) +\n  scale_y_log10() +\n  labs(y = \"Magnetic Field (nT)\", x=\"Time\", color=NULL) +\n  theme_pubr(base_size = 16, ) +\n  scale_color_okabeito(palette = \"black_first\")\n\nxmin &lt;- as.POSIXct(\"2015-01-01\")\nxmax &lt;- as.POSIXct(\"2015-12-31\")\n\np2 &lt;- magnetic_field_plot + coord_cartesian(xlim = c(xmin, xmax)) + theme_transparent() + theme(legend.position = \"none\")\nmagnetic_field_plot + inset_element(p2, 0.2, 0, 0.8, 0.4) \n\n\n\n\n\n\n\n\n\n\n\nCode\n%%R -w 1200 -h 1000\nlibrary(ggmagnify)\n\nfilename &lt;- \"model/juno_model_validation_full\"\n\nhide_x = theme(\n  axis.title.x = element_blank(),\n  axis.text.x = element_blank(),\n  axis.ticks.x = element_blank(),\n)\n\nmagnetic_field_plot &lt;- ggplot(data = df_avg, aes(x = as.numeric(.data[[time_column]]))) +\n  geom_line(aes(y = .data[[model_b_col]], color = \"MSWIM2D\")) +\n  geom_line(aes(y = .data[[juno_b_col]], color = \"Juno\")) +\n  geom_line(data = df, aes(y = .data[[juno_b_col]], color = \"Juno (high res)\"), alpha = alpha) +\n  scale_y_log10() +\n  labs(y = \"Magnetic Field (nT)\", color=NULL) +\n  theme_pubr(base_size = 16) +\n  scale_color_okabeito(palette = \"black_first\")\n\n# Computation failed in `stat_magnify()`                                              \n# ! no applicable method for 'find_bounds' applied to an object of class \"c('POSIXct', 'POSIXt')\"\n\nfrom &lt;- c(xmin = as.numeric(as.POSIXct(\"2013-10-20\")), xmax = as.numeric(as.POSIXct(\"2014-03-15\")), ymin = 0.3, ymax = 10)\nto &lt;- c(xmin = as.numeric(as.POSIXct(\"2013-01-01\")), xmax = as.numeric(as.POSIXct(\"2015-01-01\")), ymin = 0.015, ymax = 0.1)\n\np1 &lt;- magnetic_field_plot + geom_magnify(from = from, to = to, recompute = TRUE, axes=\"y\")  + \n  guides(color = guide_legend(nrow = 1)) +\n  hide_x + \n  theme(\n    legend.position = c(0.5, 1),   # Center top inside the plot\n    legend.text = element_text(size = 16)\n  )\n\n# Create the radial distance plot (similar to panel02)\np2 &lt;- ggplot(df, aes(x = .data[[time_column]], y = radial_distance)) +\n  geom_line(linewidth=1) +\n  labs(x=\"Time\", y = \"Radial Distance (AU)\") +\n  theme_pubr(base_size = 16)\n\np_n &lt;- ggplot(df, aes(x = .data[[time_column]], y = plasma_density)) +\n  geom_line() +\n  scale_y_log10() +\n  labs(y = \"Ion Density (per cc)\") +\n  theme_pubr(base_size = 16) +\n  hide_x\n  \np_speed &lt;- ggplot(df, aes(x = .data[[time_column]], y = plasma_speed)) +\n  geom_line() +\n  labs(y = \"Ion Speed (km/s)\") +\n  theme_pubr(base_size = 16) +\n  hide_x\n\n# p &lt;- p1 + p_n + p2 \np &lt;- p1 + p_speed + p_n + p2 + plot_layout(heights = c(2, 1, 1, 1)) + plot_annotation(tag_levels = \"a\")\n\n\nsave_plot(filename)\nprint(p)\n\n\nSaving 16.7 x 13.9 in image\nSaving 16.7 x 13.9 in image\n\n\nWarning: stack imbalance in 'lapply', 112 then 111\nIn addition: There were 33 warnings (use warnings() to see them)",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "article.html",
    "href": "article.html",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "",
    "text": "What I did: I studied the solar wind discontinuities in the outer heliosphere using data from the Juno spacecraft during its cruise phase. I identified and analyzed the properties of the discontinuities at different radial distances from the Sun. I differentiated the temporal effect (correlated with solar activity) and spatial variations (correlated with radial distance).\nWhat I found: The normalized occurrence rate of IDs drops with the radial distance from the Sun, following a \\(1/r\\) law. The thickness of IDs increases with the radial distance from the Sun, but after normalization to the ion inertial length, the thickness of IDs decreases. The current intensity of IDs decreases with the radial distance from the Sun, but after normalization to the Alfven current, the current intensity of IDs increases.\nWhat it means: The results of this study provide a better understanding of the solar wind discontinuities in the outer heliosphere and their spatial evolution. This information is important for understanding the dynamics of the solar wind and testing the local generation mechanism of discontinuities."
  },
  {
    "objectID": "article.html#dataset",
    "href": "article.html#dataset",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Dataset",
    "text": "Dataset\nIn this study we use datasets of four missions measuring solar wind magnetic field and plasma. Synergistic observations of these missions should advance our understanding of the solar wind discontinuities, their radial distribution and evolution (Velli et al. 2020).\n\nThe solar wind plasma state, as evidenced by the sunspot number (referenced in Figure 1), plays a crucial role in understanding the dynamics of discontinuities. During the initial phase of the JUNO mission, the sunspot number reached its peak, indicating a period of heightened solar activity. However, by the end of the mission’s cruise phase, there was a significant decline in solar activity. This variation underscores the importance of calibrating the discontinuity properties in relation to solar activity levels to account for temporal fluctuations.\nFurthermore, an analysis of Figure 1 reveals the significance of the heliographic longitudinal difference between the Juno mission and Near-Earth missions (such as Wind and ARTEMIS) as well as STEREO-A missions. This longitudinal discrepancy ensures comprehensive coverage of the plasma en route to Juno. Specifically, when there is a substantial longitudinal difference between Juno and Earth, the difference between Juno and STEREO-A tends to be minimal, and vice versa. This relationship facilitates an extensive observation network for the plasma traveling towards Juno.\nThe time resolution of the magnetic field and plasma data for Juno, Wind, ARTEMIS, STEREO is summarized in the table below."
  },
  {
    "objectID": "article.html#model",
    "href": "article.html#model",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Model",
    "text": "Model\nJune measurements do not include plasma characteristics, and to estimate discontinuity spatial scale (thicknesses) we will use solar wind speed obtained from the model of solar wind propagation. The hourly solar wind model data from the Two-Dimensional Outer Heliosphere Solar Wind Modeling (MSWIM2D) (Keebler et al. 2022) will be employed to determine the ion bulk velocity \\(v\\) and plasma density \\(n\\) at the location of the Juno mission. Utilizing the BATSRUS MHD solver, this model is capable of simulating the propagation of the solar wind from 1 to 75 astronomical units (AU) in the ecliptic plane, effectively encompassing the region of interest for our study. Figure 2 shows comparison of magnetic field magnitudes obtained from MSWIM2D and measured by Juno.\nThe model time resolution is hourly. After averaging Juno data to the same time resolution, we see that the model data is consistent with the Juno magnetic field data.\n\n\n\n\n\n\nFigure 2: a, Magnetic field magnitude from MSWIM2D and Juno. b-c, Plasma speed and density from MSWIM2D model. d, Juno radial distance from the Sun."
  },
  {
    "objectID": "article.html#methods",
    "href": "article.html#methods",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Methods",
    "text": "Methods\nWe use Liu’s (Liu, Fu, Cao, Wang, et al. 2022) method to identify discontinuities in the solar wind. This method has better compatibility for the discontinuities with minor field changes, and is more robust to the situation encountered in the outer heliosphere. For each sampling instant \\(t\\), we define three intervals: the pre-interval \\([-1,-1/2]\\cdot T+t\\), the middle interval \\([-1/,1/2]\\cdot T+t\\), and the post-interval \\([1/2,1]\\cdot T+t\\), in which \\(T\\) are time lags. Let time series of the magnetic field data in these three intervals are labeled \\({\\mathbf B}_-\\), \\({\\mathbf B}_0\\), \\({\\mathbf B}_+\\), respectively. Then for an discontinuity, the following three conditions should be satisfied: (1) \\(\\sigma({\\mathbf B}_0) &gt; 2\\max\\left(\\sigma({\\mathbf B}_-), \\sigma({\\mathbf B}_+)\\right)\\), (2) \\(\\sigma\\left({\\mathbf B}_-+{\\mathbf B}_+\\right)&gt;\\sigma({\\mathbf B}_-)+\\sigma({\\mathbf B}_+)\\), and (3) \\(|\\Delta {\\mathbf B}|&gt;|{\\mathbf B}_{bg}|/10\\), in which \\(\\sigma\\) and \\({\\mathbf B}_{bg}\\) represent the standard deviation and the background magnetic field magnitude, and \\(\\Delta {\\mathbf B}={\\mathbf B}(t+T/2)-{\\mathbf B}(t-T/2)\\). The first two conditions guarantee that the field changes of the discontinuity identified are large enough to be distinguished from the stochastic fluctuations or Alfven waves on magnetic fields, while the third is a supplementary condition to reduce the uncertainty of recognition.\nBecause the discontinuities may have different spatial scale at different radial distances from the Sun, we repeat the search procedure with time lags \\(T = 10, 20, 30, 40, 50, 60\\) s, and combine the identified ID events together to establish the event list.\nDiscontinuities with spatial scales larger than \\(60\\) s are not considered in this study, however it could be argued that they are not significant in the outer heliosphere if they follow the trend with decreasing number of IDs with increasing spatial scale. Figure 5 also indicates that the spatial scale of IDs does not vary significantly with the radial distance from the Sun.\nTODO:\nMore than ? IDs are identified in the solar wind data from Juno, with ? IDs identified in 5 AU. Then for each discontinuity identified, we calculate the distance matrix of the time series sequence (distance between each pair of magnetic field vectors) to determine the leading edge and trailing edge of the discontinuity. After that, we use the minimum or maximum variance analysis (MVA) analysis (Bengt U. Ö. Sonnerup and Scheible 1998; B. U. Ö. Sonnerup and Cahill Jr. 1967) to get the main (most varying) magnetic field component, \\(B_l\\), and medium variation component, \\(B_m\\). The maxium variance direction is then fitted by a step-like functions to extract the parameters to properly describe the discontinuity (we used logistic function here). Then we combine the magnetic field data and plasma data to obtain the thickness and the current density of the discontinuity.\n\\[\nB(t; A, \\mu, \\sigma, {\\mathrm{form=logistic}}) = A \\left[1 - \\frac{1}{1 + e^{\\alpha}} \\right]\n\\tag{1}\\]\nwhere \\(\\alpha = (t - \\mu)/{\\sigma}\\).\nFigure 3 shows several examples of solar wind discontinuities detected by different spacecraft.\n\n\n\n\n\n\n\nFigure 3: Examples of IDs from ARTEMIS, STEREO, and Wind"
  },
  {
    "objectID": "article.html#occurrence-rate",
    "href": "article.html#occurrence-rate",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Occurrence rate",
    "text": "Occurrence rate\nThe occurrence rate of IDs is investigated with regards of the radial distance from the Sun for Juno, ARTEMIS, STEREO, and Wind. Number of discontinuities measured per day by different spacecraft missions (for the same temporal resolution of magnetic field data and the same criteria of discontinuity determination) is shown in Figure 4. Because data is not continuously available and ARTEMIS spacecrafts will only be exposed in the solar wind for a relative short time during its orbits around the Moon (the Moon would be in the magnetosphere for a long time as it orbits the earth), the occurrence rate (numbder of IDs per day) is corrected with respect to data gap and the time the mission spent in the pristine solar wind.\nThe occurrence rate of IDs observed by Juno is consistent with the occurrence rate observed by 1AU missions, when Juno is around \\(1\\) AU. This number (occurrence rate) decreases with distance as Juno moves from \\(1\\) AU to \\(5\\) AU (corresponding to time after \\(\\sim 2013\\), the radial distance of Juno for 2011-2015 is shown in Figure 2)\nThe rate of occurrence depends linearly on the solar wind velocity (Söding et al. 2001), however the plasma velocity data in Juno mission is not available during its cruise phase. Therefore, we do not normalize the occurrence rate with respect to the solar wind speed. This influence of the solar wind speed on the occurrence rate might be mitigated by the fact all the spacecrafts are in the ecliptic plane, and the solar wind speed is not expected to change significantly with the radial distance from the Sun beyond \\(1\\) AU. We normalize the occurrence rate from Juno to the occurrence rate from 1AU missions to account for the effect of solar wind structures on the occurrence rate.\nThe normalized occurrence rate is shown in panel (b) of Figure 4. The occurrence rate of IDs drops with the radial distance from the Sun, following \\(1/r\\) law. So the decrease of the occurrence rate with increasing radial distance from the Sun is expected to be a real effect of the radial distance from the Sun and this confirms the former results (Söding et al. 2001).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: The number of discontinuities measured by Juno per day coincides with the discontinuity number measured by STEREO, WIND, and ARTEMIS, when Juno is around \\(1\\) AU. This number (occurrence rate) decreases with distance (with time after \\(\\sim 2013\\)), as Juno moves from \\(1\\) AU to \\(5\\) AU. We will use the similar comparison for discontinuity characteristics and occurrence rate derived for PSP and Juno. The radial distance of Juno for 2011-2016 is shown in Figure 2."
  },
  {
    "objectID": "article.html#current-density-and-thickness",
    "href": "article.html#current-density-and-thickness",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Current density and thickness",
    "text": "Current density and thickness\nThe peak current density \\(J_m\\) of a discontinuity is determined by\n\\[\nJ_m = - \\frac{1}{\\mu_0 V_n} \\frac{d B_m}{d t},\n\\tag{2}\\]\nwhere \\(\\mu_0\\) is the vacuum permeability and \\(V_n\\) is the normal component of the proton flow velocity. For fitting method, the derivative of the magnetic field could be estimated as \\(d B_m / d t = A / 4 σ\\). This estimate is more robust than the direct calculation of the derivative of the magnetic field, because the fitting method is less sensitive to the noise in the data and the time resolution of the data (see discussion in the appendix).\nThe thickness \\(d\\) of a discontinuity is determined by\n\\[\nd = (v \\cdot n) \\Delta T\n\\]\nwith \\(v = v_{sw} - v_{s/c} + U\\) , where \\(\\Delta T\\) is the transition time (a fixed factor of the \\(\\sigma\\) used in fitting) , \\(v_{sw}\\) is the solar wind velocity. The velocity of the spacecraft \\(v_{s/c}\\) and the propagation speed of the discontinuities \\(U\\) are negligible in our case.\nTo understand the relationship between the discontinuity and the local plasma, we normalize the thickness of the discontinuity by the initial ion length and the current density by the Alfven current density \\(J_A = e N_p V_A\\) (corresponding to the drift between protons and electrons of local Alfén speed \\(V_A\\)). Figure (junoDistribution?) presents the statistical distribution of the (un)normalized thickness and current density of IDs grouped by the radial distance from the Sun collected by Juno.\nPanel (a,b) shows that the thickness increases with radial distance. However, after normalization, the thickness slightly decreases, with the most probable value around 4-2 \\(d_i\\). With increasing radial distance, larger fraction of IDs have thickness less than 1 \\(d_i\\), becombing sub-proton scale structures. Panel (c,d) shows that the current density decreases with increasing radial distance. However, after normalization, it slightly increases, with the most probable value around 0.05-0.15 \\(J_A\\). It should be noted that for the normalized thickness and current density, the change is not significant compared to the spread of the distribution.\nThe distribution of the thickness and current density of IDs observed by 1AU satellites (Wind, ARTEMIS and STEREO-A) during the five year of Juno cruise phase are shown in Figure 6. The distribution are grouped by the year of observation. Panel (a,b) shows that both the thickness and normalized thickness distribution of IDs remains almost constant with the year of observation. Most of the discontinuities have thickness around 350-3500 km, with the most probable value around 1000 km. Panel (c,d) shows that the current density slightly increases with the year of observation, with the most probable value around 5 \\(nA m^{-2}\\), in consistence with previous studies (Vasko et al. 2022). However, after normalization, the current density distribution remains identical across the five years. The change in the current density could be explained by the change in the solar wind properties with the solar cycle, as shown in Figure 7. Density and magnetic field asscoiated with the discontinuities increases with the solar cycle approaching the solar minimum, which could lead to the increase of the current density of the discontinuities with fixed normalized current density.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Distribution of various properties of IDs observed by Juno grouped by the radial distance from the Sun. Panel (a) thickness, (b) normalized thickness, (c) current density, (d) normalized current density.\n\n\n\n\n\n\n\n\n\nFigure 6: Distribution of various properties of IDs observed by 1AU satellites (Wind, ARTEMIS and STEREO-A) grouped by the year of observation. Panel (a) thickness, (b) normalized thickness, (c) current density, (d) normalized current density.\n\n\n\n\n\n\n\n\n\nFigure 7: Solar wind parameters associated with the IDs observed by 1AU satellites (Wind, ARTEMIS and STEREO-A) grouped by the year of observation. Panel (a) solar wind density, (b) magnetic field, (c) fitted magnetic field \\(A\\) in Equation 1."
  },
  {
    "objectID": "article.html#dependences-of-occurrence-rate",
    "href": "article.html#dependences-of-occurrence-rate",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Dependences of occurrence rate",
    "text": "Dependences of occurrence rate\nMariani, Bavassano, and Villante (1983)\nMariani et al. (1973)\nTODO:"
  },
  {
    "objectID": "article.html#effect-of-the-time-resolution-on-the-discontinuity-properties",
    "href": "article.html#effect-of-the-time-resolution-on-the-discontinuity-properties",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Effect of the time resolution on the discontinuity properties",
    "text": "Effect of the time resolution on the discontinuity properties\nThe transition time \\(\\Delta T\\) is determined in one second resolution data for all our missions to reduce the system error. (Note that highest time resolution of the data is not needed for the transition time determination by fitting method).\n\n\n\n\n\n\nFigure 8: Effect of the time resolution on the discontinuity properties"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "",
    "text": "manuscript please see here\nAGU poster here"
  },
  {
    "objectID": "index.html#motivations",
    "href": "index.html#motivations",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Motivations",
    "text": "Motivations\nStudying the radial distribution of occurrence rate, as well as the properties of solar wind discontinuities may help answer the following questions:\n\nHow does the discontinuities change with the radial distance from the Sun?\nHow is solar wind discontinuities formed? What is the physical mechanisms?\n\nGenerated at or near the sun?\nLocally generated in the interplanetary space by turbulence?\n\n\nJoint observations of JUNO & ARTEMIS & Other missions really provides a unique opportunity!!!\n\n\nTo eliminate the effect of the solar wind structure, we use data from other missions (mainly at 1AU) to provide a way of normalization."
  },
  {
    "objectID": "index.html#occurrence-rate",
    "href": "index.html#occurrence-rate",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Occurrence rate",
    "text": "Occurrence rate"
  },
  {
    "objectID": "index.html#properties",
    "href": "index.html#properties",
    "title": "Solar wind discontinuities spatial evolution in the outer heliosphere",
    "section": "Properties",
    "text": "Properties"
  }
]