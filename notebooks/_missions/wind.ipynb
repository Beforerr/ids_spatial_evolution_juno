{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: IDs from Wind\n",
    "order: 10\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speasy as spz\n",
    "\n",
    "from speasy import SpeasyIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WI_H4-RTN_MFI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spz.cda.flat_inventory.datasets['WI_H4-RTN_MFI'].spz_uid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_mag : SpeasyIndex = spz.inventories.data_tree.archive.local.WI_H4_RTN_MFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spz.archive.update_inventory()\n",
    "\n",
    "# spz.archive.build_inventory(spz.inventories.data_tree.archive.local.WI_H4_RTN_MFI)\n",
    "\n",
    "# spz.archive.update_inventory(spz.inventories.data_tree.archive.local.WI_H4_RTN_MFI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"list\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minventories\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWI_H4_RTN_MFI\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2016-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2016-01-03\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/juno/lib/python3.10/site-packages/speasy/core/requests_scheduling/request_dispatch.py:310\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MaybeAnyProduct:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve requested product(s).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    Speasy gives access to two kind of products, time-dependent products such as physical measurements or trajectories\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03m    and products such as timetables or event catalogs. So depending on which product you want to retrieve :func:`speasy.get_data`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_compile_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must at least provide a product to retrieve\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/juno/lib/python3.10/site-packages/speasy/core/requests_scheduling/request_dispatch.py:179\u001b[0m, in \u001b[0;36m_compile_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop_time\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m--> 179\u001b[0m         args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m), kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstop_time\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_range\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    181\u001b[0m         args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_range\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"list\") to tuple"
     ]
    }
   ],
   "source": [
    "spz.get_data(\n",
    "    spz.inventories.data_tree.archive.local.WI_H4_RTN_MFI,\n",
    "    start_time='2016-01-01',\n",
    "    stop_time='2016-01-03',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amda', 'csa', 'cda', 'cdaweb', 'ssc', 'sscweb', 'archive', 'generic_archive']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spz.list_providers()\n",
    "# spz.inventories.data_tree.archive.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discontinuitypy.datasets import IDsDataset\n",
    "import polars as pl\n",
    "from fastcore.utils import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = 1 # unit: seconds\n",
    "tau = 60 # unit: seconds\n",
    "\n",
    "mission = \"Wind\"\n",
    "data_dir = '../../data'\n",
    "dir_path = f'{data_dir}/03_primary/{mission}_MAG_ts_{ts}s'\n",
    "state_data_path = f'{data_dir}/03_primary/OMNI_LowRes_ts_3600s.parquet'\n",
    "vec_cols = ['v_x', 'v_y', 'v_z']\n",
    "\n",
    "format = 'arrow'\n",
    "fname = f'events.{mission}.ts_{ts}s_tau_{tau}s.{format}'\n",
    "output_path = f'{data_dir}/05_reporting/{fname}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/03_primary/Wind_MAG_ts_1s/2016.parquet']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = list(walk(dir_path))\n",
    "files.sort()\n",
    "files[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-Feb-24 16:08:39: UserWarning: Converting non-nanosecond precision datetime values to nanosecond precision. This behavior can eventually be relaxed in xarray, as it is an artifact from pandas which is now beginning to support non-nanosecond precision values. This warning is caused by passing non-nanosecond np.datetime64 or np.timedelta64 values to the DataArray or Variable constructor; it can be silenced by converting the values to nanosecond precision ahead of time.\n",
      "\n",
      "07-Feb-24 16:08:40: UserWarning: Ray execution environment not yet initialized. Initializing...\n",
      "To remove this warning, run the following python code before doing dataframe operations:\n",
      "\n",
      "    import ray\n",
      "    ray.init()\n",
      "\n",
      "\n",
      "2024-02-07 16:08:42,624\tINFO worker.py:1724 -- Started a local Ray instance.\n",
      "07-Feb-24 16:08:43: UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e528c9af04c46f39c3e8ce08541bc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distributing Dataframe:   0%           Elapsed time: 00:00, estimated remaining time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-Feb-24 16:08:43: Using sequential splitting in '.from_pandas()' because of some of the conditions are False: enough_elements=False; all_numeric_types=False; async_mode_on=False\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m UserWarning: Traceback (most recent call last):\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m   File \"/Users/zijin/micromamba/envs/juno/lib/python3.10/site-packages/pdpipe/__init__.py\", line 85, in <module>\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m     from . import skintegrate\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m   File \"/Users/zijin/micromamba/envs/juno/lib/python3.10/site-packages/pdpipe/skintegrate.py\", line 20, in <module>\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m     from sklearn.base import BaseEstimator\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m ModuleNotFoundError: No module named 'sklearn'\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m \n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m UserWarning: pdpipe: Scikit-learn or skutil import failed. Scikit-learn-dependent pipeline stages will not be loaded.\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m UserWarning: Traceback (most recent call last):\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m   File \"/Users/zijin/micromamba/envs/juno/lib/python3.10/site-packages/pdpipe/__init__.py\", line 105, in <module>\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m     from . import nltk_stages\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m   File \"/Users/zijin/micromamba/envs/juno/lib/python3.10/site-packages/pdpipe/nltk_stages.py\", line 19, in <module>\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m     import nltk\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m ModuleNotFoundError: No module named 'nltk'\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m \n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m UserWarning: pdpipe: nltk import failed. nltk-dependent  pipeline stages will not be loaded.\n",
      "\u001b[36m(_deploy_ray_func pid=71808)\u001b[0m UserWarning: Distributing <class 'dict'> object. This may take some time.\n",
      "Distributing Dataframe:   0%           Elapsed time: 00:00, estimated remaining time: ?\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m UserWarning: Traceback (most recent call last):\u001b[32m [repeated 22x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m   File \"/Users/zijin/micromamba/envs/juno/lib/python3.10/site-packages/pdpipe/nltk_stages.py\", line 19, in <module>\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m     from . import skintegrate\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m     from sklearn.base import BaseEstimator\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m ModuleNotFoundError: No module named 'sklearn'\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m \u001b[32m [repeated 28x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m UserWarning: pdpipe: Scikit-learn or skutil import failed. Scikit-learn-dependent pipeline stages will not be loaded.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m     from . import nltk_stages\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m     import nltk\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m ModuleNotFoundError: No module named 'nltk'\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71818)\u001b[0m UserWarning: pdpipe: nltk import failed. nltk-dependent  pipeline stages will not be loaded.\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m UserWarning: Distributing <class 'dict'> object. This may take some time.\u001b[32m [repeated 6664x across cluster]\u001b[0m\n",
      "Distributing Dataframe:   0%           Elapsed time: 00:00, estimated remaining time: ?\u001b[32m [repeated 536x across cluster]\u001b[0m\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\u001b[32m [repeated 6350x across cluster]\u001b[0m\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\u001b[32m [repeated 526x across cluster]\u001b[0m\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71819)\u001b[0m \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71814)\u001b[0m UserWarning: Distributing <class 'dict'> object. This may take some time.\u001b[32m [repeated 8099x across cluster]\u001b[0m\n",
      "Distributing Dataframe:   0%           Elapsed time: 00:00, estimated remaining time: ?\u001b[32m [repeated 515x across cluster]\u001b[0m\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\u001b[32m [repeated 7583x across cluster]\u001b[0m\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\u001b[32m [repeated 514x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f904d06fdb446baf0e12ed8d7ec205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimated completion of line 8:   0%           Elapsed time: 00:00, estimated remaining time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_deploy_ray_func pid=71812)\u001b[0m RuntimeWarning: overflow encountered in exp\n",
      "\u001b[36m(_deploy_ray_func pid=71814)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m UserWarning: Distributing <class 'dict'> object. This may take some time.\u001b[32m [repeated 2145x across cluster]\u001b[0m\n",
      "Distributing Dataframe:   0%           Elapsed time: 00:00, estimated remaining time: ?\u001b[32m [repeated 150x across cluster]\u001b[0m\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\u001b[32m [repeated 1769x across cluster]\u001b[0m\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\u001b[32m [repeated 152x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71813)\u001b[0m RuntimeWarning: overflow encountered in exp\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71808)\u001b[0m RuntimeWarning: overflow encountered in exp\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "\u001b[36m(_deploy_ray_func pid=71810)\u001b[0m RuntimeWarning: overflow encountered in exp\u001b[32m [repeated 28x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51efa109f8f34443923e39b8b9791047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distributing Dataframe:   0%           Elapsed time: 00:00, estimated remaining time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-Feb-24 16:09:20: Using sequential splitting in '.from_pandas()' because of some of the conditions are False: enough_elements=False; all_numeric_types=False; async_mode_on=False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45779e0306a241d4899eb66270f58fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distributing Dataframe:   0%           Elapsed time: 00:00, estimated remaining time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-Feb-24 16:09:20: Using sequential splitting in '.from_pandas()' because of some of the conditions are False: enough_elements=False; all_numeric_types=True; async_mode_on=False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d466dcbcc0e341638502d3a2839f35cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distributing Dataframe:   0%           Elapsed time: 00:00, estimated remaining time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07-Feb-24 16:09:20: Using sequential splitting in '.from_pandas()' because of some of the conditions are False: enough_elements=False; all_numeric_types=False; async_mode_on=False\n"
     ]
    }
   ],
   "source": [
    "events = []\n",
    "for mag_path in files[-1:]:\n",
    "\n",
    "    mag_data = pl.scan_parquet(mag_path).drop('X', 'Y', 'Z').sort('time')\n",
    "    plasma_data = pl.scan_parquet(state_data_path).sort('time')\n",
    "\n",
    "    _events = (\n",
    "        IDsDataset(\n",
    "            mag_data=mag_data,\n",
    "            plasma_data=plasma_data,\n",
    "            tau=tau,\n",
    "            ts=ts,\n",
    "            vec_cols=vec_cols,\n",
    "        )\n",
    "        .find_events(return_best_fit=False)\n",
    "        .update_candidates_with_plasma_data()\n",
    "        .events\n",
    "    )\n",
    "    \n",
    "    events.append(_events)\n",
    "    \n",
    "ids_dataset = IDsDataset(\n",
    "    events=pl.concat(events),\n",
    "    mag_data= pl.scan_parquet(list(walk(dir_path))).drop('X', 'Y', 'Z').sort('time')\n",
    ").export(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 day of data resampled by 1 sec is about 12 MB.\n",
    "\n",
    "So 1 year of data is about 4 GB, and 6 years of JUNO Cruise data is about 24 GB.\n",
    "\n",
    "Downloading rate is about 250 KB/s, so it will take about 3 days to download all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to download: 126.53 hours\n",
      "Disk space required: 113.88 GB\n",
      "Time to process: 36.50 hours\n"
     ]
    }
   ],
   "source": [
    "num_of_files = 6*365\n",
    "jno_file_size = 12e3\n",
    "thm_file_size = 40e3\n",
    "files_size = jno_file_size + thm_file_size\n",
    "downloading_rate = 250\n",
    "processing_rate = 1/60\n",
    "\n",
    "time_to_download = num_of_files * files_size / downloading_rate / 60 / 60\n",
    "space_required = num_of_files * files_size / 1e6\n",
    "time_to_process = num_of_files / processing_rate / 60 / 60\n",
    "\n",
    "print(f\"Time to download: {time_to_download:.2f} hours\")\n",
    "print(f\"Disk space required: {space_required:.2f} GB\")\n",
    "print(f\"Time to process: {time_to_process:.2f} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
