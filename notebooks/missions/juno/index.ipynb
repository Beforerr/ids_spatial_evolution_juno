{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: IDs from Juno\n",
    "---\n",
    "\n",
    "See following notebooks for details:\n",
    "\n",
    "- [State data](./state.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discontinuitypy.datasets import IDsDataset\n",
    "import polars as pl\n",
    "from fastcore.utils import walk\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mission = \"JNO\"\n",
    "ts = timedelta(seconds=1)\n",
    "tau = timedelta(seconds=60)\n",
    "\n",
    "\n",
    "data_dir = '../../../data'\n",
    "dir_path = f'{data_dir}/03_primary/JNO_MAG_ts_{ts.seconds}s'\n",
    "juno_state_path = f'{data_dir}/03_primary/JNO_STATE_ts_3600s.parquet'\n",
    "vec_cols = ['v_x', 'v_y', 'v_z']\n",
    "\n",
    "format = 'arrow'\n",
    "fname = f'events.{mission}.ts_{ts.total_seconds():.2f}s_tau_{tau.seconds}s.{format}'\n",
    "output_path = f'{data_dir}/05_reporting/{fname}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plasma_data = pl.scan_parquet(juno_state_path).sort('time')\n",
    "logger.info(plasma_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juno_events = []\n",
    "for mag_path in walk(dir_path):\n",
    "    mag_data = pl.scan_parquet(mag_path).drop('X', 'Y', 'Z').sort('time')\n",
    "\n",
    "    _juno_events = (\n",
    "        IDsDataset(\n",
    "            mag_data=mag_data,\n",
    "            plasma_data=plasma_data,\n",
    "            tau=tau,\n",
    "            ts=ts,\n",
    "            vec_cols=vec_cols,\n",
    "            density_col=\"plasma_density\",\n",
    "            speed_col=\"plasma_speed\",\n",
    "            temperature_col=\"plasma_temperature\",\n",
    "        )\n",
    "        .find_events(return_best_fit=False)\n",
    "        .update_candidates_with_plasma_data()\n",
    "        .events\n",
    "    )\n",
    "    \n",
    "    juno_events.append(_juno_events)\n",
    "    \n",
    "juno_ids_dataset = IDsDataset(\n",
    "    events=pl.concat(juno_events),\n",
    "    mag_data= pl.scan_parquet(list(walk(dir_path))).drop('X', 'Y', 'Z').sort('time')\n",
    ")\n",
    "\n",
    "juno_ids_dataset.export(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the discontinuity in Juno cruise phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full time resolution data\n",
    "\n",
    "0.03 s - 0.125 s time resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from space_analysis.missions.juno.fgm import download_data\n",
    "from discontinuitypy.utils.basic import resample\n",
    "from toolz import curry\n",
    "from pipe import select\n",
    "from fastcore.utils import mkdir\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    fp,\n",
    "    every = timedelta(seconds = 0.125),\n",
    "    dir_path = \"../../../data/02_intermediate/JNO_MAG_8hz\",\n",
    "    update = False\n",
    "):\n",
    "    fname = fp.split('/')[-1]\n",
    "    \n",
    "    output_path = f\"{dir_path}/{fname}\"\n",
    "    \n",
    "    if not os.path.exists(output_path) or update:\n",
    "        mkdir(dir_path, parents=True, exist_ok = True)\n",
    "        df = pl.scan_ipc(fp).sort('time').pipe(resample, every = every)\n",
    "        df.collect().write_ipc(output_path)\n",
    "    return output_path\n",
    "\n",
    "@curry\n",
    "def process(fp, ids_dataset: IDsDataset, sparse_num = 10, **kwargs):\n",
    "    df = pl.scan_ipc(fp).sort('time').unique('time')\n",
    "\n",
    "    ids_dataset.data = df\n",
    "    \n",
    "    return ids_dataset.find_events(return_best_fit=False, sparse_num = sparse_num, **kwargs).update_candidates_with_plasma_data().events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_paths = list(download_data(datatype=\"FULL\") | select(preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = timedelta(seconds=0.125)\n",
    "tau = timedelta(seconds=20)\n",
    "method = \"derivative\"\n",
    "# method = \"fit\"\n",
    "\n",
    "fname = f'events.{mission}.{method}.ts_{ts.total_seconds():.2f}s_tau_{tau.seconds}s.{format}'\n",
    "output_path = f'{data_dir}/05_reporting/{fname}'\n",
    "logger.info(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_ds = IDsDataset(\n",
    "    plasma_data=plasma_data,\n",
    "    tau=tau,\n",
    "    ts=ts,\n",
    "    vec_cols=vec_cols,\n",
    "    density_col=\"plasma_density\",\n",
    "    speed_col=\"plasma_speed\",\n",
    "    temperature_col=\"plasma_temperature\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasonably splitting the data files may accelerate the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = split_list(mag_paths, n=100)\n",
    "\n",
    "func = process(ids_dataset = ids_ds, sparse_num = 10, method = method)\n",
    "\n",
    "ids_ds.data = pl.scan_ipc(mag_paths)\n",
    "ids_ds.events = pl.concat(fps | select(func)) \n",
    "ids_ds.export(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superposed epoch analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discontinuitypy.utils.basic import df2ts\n",
    "from discontinuitypy.core.pipeline import compress_data_by_interval\n",
    "from xarray_einstats import linalg\n",
    "\n",
    "from sea_norm import sean\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import Figure, Axes\n",
    "\n",
    "from discontinuitypy.integration import J_FACTOR\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def keep_good_fit(df: pl.DataFrame, rsquared = 0.95):\n",
    "    return df.filter(pl.col('fit.stat.rsquared') > rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discontinuitypy.propeties.mva import minvar\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_dBdt_data(data: pl.LazyFrame):\n",
    "    \"\"\"\n",
    "    Calculate the time derivative of the magnetic field\n",
    "    \"\"\"\n",
    "    # TODO: compress data first by events, however, this will decrease the reliability of the derivative at the edges of the events\n",
    "    ts = df2ts(data)\n",
    "\n",
    "    vec_diff = ts.differentiate(\"time\", datetime_unit=\"s\")\n",
    "    vec_diff_mag: xr.DataArray = linalg.norm(vec_diff, dims=\"v_dim\")\n",
    "    return vec_diff_mag.to_dataframe(name=\"dBdt\")\n",
    "\n",
    "\n",
    "def get_mva_data(\n",
    "    data: pl.LazyFrame,\n",
    "    starts: list,\n",
    "    ends: list,\n",
    "    columns=[\"B_l\", \"B_m\", \"B_n\"],\n",
    "    normalize=True,\n",
    "):\n",
    "    for start, end in tqdm(zip(starts, ends)):\n",
    "        event_data = data.filter(\n",
    "            pl.col(\"time\") >= start, pl.col(\"time\") <= end\n",
    "        ).collect()\n",
    "\n",
    "        event_numpy = event_data.drop(\"time\").to_numpy()  # Assuming this is efficient for your use case\n",
    "        time = event_data.get_column(\"time\").to_numpy()\n",
    "        \n",
    "        vrot, _, _ = minvar(event_numpy)\n",
    "        \n",
    "        if True:\n",
    "            vl = vrot[:, 0]\n",
    "            vl = vl * np.sign(vl[-1] - vl[0])\n",
    "            vl_ts = xr.DataArray(vl, dims=\"time\", coords={\"time\": time})\n",
    "            dvl_dt_df = vl_ts.differentiate(\"time\", datetime_unit=\"s\").to_dataframe(name=\"dBl_dt\")\n",
    "\n",
    "        if normalize:\n",
    "            vrot = normalize_mva_data(vrot)\n",
    "            vrot_df = pd.DataFrame(vrot, columns=columns, index=time)\n",
    "            \n",
    "        yield pd.concat([vrot_df, dvl_dt_df], axis=1)\n",
    "\n",
    "\n",
    "def normalize_mva_data(\n",
    "    data: np.ndarray, shift=False  # shift the data in l direction to the origin\n",
    "):\n",
    "    \"\"\"\n",
    "    normalize the MVA data: Bl, Bm, and Bn were respectively normalized to B0 = 0.5Î”Bl, Bm, and <B>\n",
    "    \"\"\"\n",
    "    vl, vm, vn = data.T\n",
    "\n",
    "    vl_norm_q = (vl[-1] - vl[0]) / 2\n",
    "    vm_norm_q = (vm[0] + vm[-1]) / 2\n",
    "    vn_norm_q = (vn[0] + vn[-1]) / 2\n",
    "\n",
    "    return np.array([vl / vl_norm_q, vm / vm_norm_q, vn / vn_norm_q]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sea_ids(\n",
    "    ds: IDsDataset,\n",
    "    event_cols=[\"t.d_start\", \"t.d_time\", \"t.d_end\"],\n",
    "    sea_cols=[\n",
    "        \"B_l\",\n",
    "        \"j_m\",\n",
    "        \"j_m_norm\",\n",
    "        \"j_k\",\n",
    "        \"j_k_norm\",\n",
    "        \"B_m\",\n",
    "    ],\n",
    "    bins=[10, 10],\n",
    "    return_data=True,\n",
    "):\n",
    "    # converting to a list of numpy arrays\n",
    "\n",
    "    events = keep_good_fit(ds.events)\n",
    "\n",
    "    sea_events = [col.to_numpy() for col in events[event_cols]]\n",
    "\n",
    "    mag_data = ds.data\n",
    "    mag_data_c = compress_data_by_interval(\n",
    "        mag_data.collect(), sea_events[0], sea_events[2]\n",
    "    ).lazy()\n",
    "\n",
    "    # dBdt_data = get_dBdt_data(ds.data)\n",
    "    dBdt_data = get_dBdt_data(mag_data_c)\n",
    "    mva_data = pd.concat(get_mva_data(mag_data_c, sea_events[0], sea_events[2]))\n",
    "\n",
    "    b_data = dBdt_data.join(mva_data, on=\"time\")\n",
    "\n",
    "    p_data = (\n",
    "        events[[\"time\", \"v_k\", \"j_Alfven\"]].to_pandas().set_index(\"time\").sort_index()\n",
    "    )\n",
    "\n",
    "    data = pd.merge_asof(\n",
    "        b_data,\n",
    "        p_data,\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        direction=\"nearest\",\n",
    "    )\n",
    "\n",
    "    data = data.assign(\n",
    "        j_m=lambda df: df.dBl_dt / df.v_k * J_FACTOR,\n",
    "        j_m_norm =lambda df: df.j_m / df.j_Alfven,\n",
    "        j_k=lambda df: df.dBdt / df.v_k * J_FACTOR,\n",
    "        j_k_norm=lambda df: df.j_k / df.j_Alfven,\n",
    "    )\n",
    "\n",
    "    return sean(data, sea_events, bins, return_data=return_data, cols=sea_cols)\n",
    "\n",
    "\n",
    "def plot_SEA(SEAarray: pd.DataFrame, meta) -> tuple[Figure, list[Axes]]:\n",
    "    \n",
    "    cols = meta[\"sea_cols\"]\n",
    "    fig, axes = plt.subplots(nrows=len(cols), sharex=True, squeeze=True)\n",
    "\n",
    "    if len(cols) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # loop over columns that were analyzed\n",
    "    for c, ax in zip(cols, axes):\n",
    "        # for each column identify the column titles which\n",
    "        # have 'c' in the title and those that don't have\n",
    "        # 'cnt' in the title\n",
    "        # e.g. for AE columns\n",
    "        # AE_mean, AE_median, AE_lowq, AE_upq, AE_cnt\n",
    "        # fine columns AE_mean, AE_median, AE_lowq, AE_upq\n",
    "        # mask = SEAarray.columns.str.startswith(c) & ~SEAarray.columns.str.endswith(\"cnt\")\n",
    "        mask = [c + \"_mean\", c + \"_median\", c + \"_lowq\", c + \"_upq\"]\n",
    "\n",
    "        # plot the SEA data\n",
    "        SEAarray.loc[:, mask].plot(\n",
    "            ax=ax,\n",
    "            style=[\"r-\", \"b-\", \"b--\", \"b--\"],\n",
    "            xlabel=\"Normalized Time\",\n",
    "            ylabel=c.replace(\"_\", \" \"),\n",
    "            legend=False,\n",
    "        )\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "def plot_ids_sea(SEAarray: pd.DataFrame, meta) -> tuple[Figure, list[Axes]]:\n",
    "    import scienceplots\n",
    "    \n",
    "    SEAarray.index = SEAarray.index.map(lambda x: x / bin/2)\n",
    "\n",
    "    with plt.style.context(['science', 'nature', 'notebook']):\n",
    "\n",
    "        fig, axes = plot_SEA(SEAarray, meta)\n",
    "\n",
    "        axes[0].set_ylabel(r\"$B_x \\ / \\ B_0$\")\n",
    "        axes[1].set_ylabel(r\"$J_y$\")\n",
    "        axes[2].set_ylabel(r\"$J_y \\ / \\ J_A$\")\n",
    "        axes[3].set_ylabel(r\"$J_k$\")\n",
    "        axes[4].set_ylabel(r\"$J_k \\ / \\ J_A$\")\n",
    "        axes[5].set_ylabel(r\"$B_m \\ / \\ B_g$\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.subplots_adjust(hspace=0)\n",
    "        \n",
    "        axes[0].legend(labels=[\"Mean\", \"Median\", \"LowQ\", \"UpQ\"])\n",
    "    return fig, axes\n",
    "    # fig.savefig(f\"../../../figures/sea/sea_juno_first_year_{freq}.png\", dpi = 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_path = sorted(list(walk(dir_path)))[0]\n",
    "tau = timedelta(seconds=60)\n",
    "mag_data = pl.scan_parquet(mag_path).drop('X', 'Y', 'Z').sort('time')\n",
    "\n",
    "ids_ds = (\n",
    "    IDsDataset(\n",
    "        mag_data=mag_data,\n",
    "        plasma_data=plasma_data,\n",
    "        tau=tau,\n",
    "        ts=ts,\n",
    "        vec_cols=vec_cols,\n",
    "        density_col=\"plasma_density\",\n",
    "        speed_col=\"plasma_speed\",\n",
    "        temperature_col=\"plasma_temperature\",\n",
    "    )\n",
    "    .find_events(return_best_fit=True)\n",
    "    .update_candidates_with_plasma_data()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_paths = list(download_data(datatype=\"FULL\") | select(preprocess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-sea-first\n",
    "#| fig-cap: Juno first year\n",
    "freq = 'high'\n",
    "# freq = 'low'\n",
    "if freq == 'high': # use high dimensional data\n",
    "    ids_ds.data = pl.scan_ipc(mag_paths[0:365]).drop('X', 'Y', 'Z').sort('time')\n",
    "    bin = 16\n",
    "else: # use low dimensional data\n",
    "    bin = 8\n",
    "\n",
    "SEAarray, meta, p1data, p2data = sea_ids(ids_ds, bins=[bin, bin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| layout-ncol: 3\n",
    "#| column: page\n",
    "ids_ds.plot_candidates(num=20, plot_fit_data=True, predicates=(pl.col('fit.stat.rsquared') > 0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mag_path = sorted(list(walk(dir_path)))[-1]\n",
    "tau = timedelta(seconds=300)\n",
    "tau = timedelta(seconds=60)\n",
    "mag_path = sorted(list(walk(dir_path)))[-1]\n",
    "mag_data = pl.scan_parquet(mag_path).drop('X', 'Y', 'Z').sort('time')\n",
    "\n",
    "ids_ds = (\n",
    "    IDsDataset(\n",
    "        mag_data=mag_data,\n",
    "        plasma_data=plasma_data,\n",
    "        tau=tau,\n",
    "        ts=ts,\n",
    "        vec_cols=vec_cols,\n",
    "        density_col=\"plasma_density\",\n",
    "        speed_col=\"plasma_speed\",\n",
    "        temperature_col=\"plasma_temperature\",\n",
    "    )\n",
    "    .find_events(return_best_fit=True)\n",
    "    .update_candidates_with_plasma_data()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_paths = list(download_data(datatype=\"FULL\") | select(preprocess))\n",
    "\n",
    "freq = 'high'\n",
    "# freq = 'low'\n",
    "if freq == 'high': # use high dimensional data\n",
    "    ids_ds.data = pl.scan_ipc(mag_paths[-365:]).drop('X', 'Y', 'Z').sort('time')\n",
    "    bin = 16\n",
    "else: # use low dimensional data\n",
    "    bin = 8\n",
    "\n",
    "SEAarray, meta, p1data, p2data = sea_ids(ids_ds, bins=[bin, bin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_ids_sea(SEAarray, meta)\n",
    "fig.savefig(f\"../../../figures/sea/sea_juno_last_year_{freq}.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Superposed epoch analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby = 'index'\n",
    "columns = ['dBdt', 'j_k']\n",
    "\n",
    "# Plot each group\n",
    "fig, axes = plt.subplots(nrows=len(columns))\n",
    "\n",
    "if len(columns) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "p1groups = p1data.groupby(groupby)\n",
    "p2groups = p2data.groupby(groupby)\n",
    "\n",
    "for ax, column in zip(axes, columns):\n",
    "    for name, group in p1groups:\n",
    "        ax.plot(group['t_norm'] -1 , group[column], color='grey', alpha=0.3)\n",
    "        \n",
    "    for name, group in p2groups:\n",
    "        ax.plot(group['t_norm'], group[column], color='grey', alpha=0.3)\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| layout-ncol: 3\n",
    "#| column: page\n",
    "ids_ds.plot_candidates(num=20, plot_fit_data=True, predicates=(pl.col('fit.stat.rsquared') > 0.95))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 day of data resampled by 1 sec is about 12 MB.\n",
    "\n",
    "So 1 year of data is about 4 GB, and 6 years of JUNO Cruise data is about 24 GB.\n",
    "\n",
    "Downloading rate is about 250 KB/s, so it will take about 3 days to download all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_files = 6*365\n",
    "jno_file_size = 12e3\n",
    "thm_file_size = 40e3\n",
    "files_size = jno_file_size + thm_file_size\n",
    "downloading_rate = 250\n",
    "processing_rate = 1/60\n",
    "\n",
    "time_to_download = num_of_files * files_size / downloading_rate / 60 / 60\n",
    "space_required = num_of_files * files_size / 1e6\n",
    "time_to_process = num_of_files / processing_rate / 60 / 60\n",
    "\n",
    "print(f\"Time to download: {time_to_download:.2f} hours\")\n",
    "print(f\"Disk space required: {space_required:.2f} GB\")\n",
    "print(f\"Time to process: {time_to_process:.2f} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
